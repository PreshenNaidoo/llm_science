{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Character Language Model (PyTorch)\n",
    "\n",
    "This notebook teaches you how a **bigram character language model** works: it predicts the next character using only the current character. It is a tiny but complete language model that helps you understand the core ideas behind LLMs?tokenization, next-token prediction, cross-entropy loss, and sampling?without the complexity of attention or deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup + imports\n",
    "We will use PyTorch for modeling and the Hugging Face `datasets` library for loading text. We'll also set global configuration values to keep training reproducible and easy to tweak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ---- Global config ----\n",
    "SEED = 42  # random seed\n",
    "BATCH_SIZE = 32  # batch size per step\n",
    "BLOCK_SIZE = 64  # context length\n",
    "EMBED_DIM = None  # set to e.g. 64 to use embedding->linear, or None for pure bigram\n",
    "LR = 1e-2  # learning rate\n",
    "MAX_STEPS = 800  # max training steps\n",
    "EVAL_EVERY = 200  # steps between evals\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # device string for torch\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "print('Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d742d",
   "metadata": {},
   "source": [
    "## 2) Load dataset\n",
    "We load `wikitext-2-raw-v1` from Hugging Face. This notebook assumes that dataset is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('wikitext', 'wikitext-2-raw-v1')  # dataset object\n",
    "dataset_name = 'wikitext-2-raw-v1'  # dataset name\n",
    "print('Loaded dataset:', dataset_name)\n",
    "\n",
    "# Optional cleanup: keep ASCII only so the character vocab stays simple\n",
    "\n",
    "# Most text datasets provide a single train split with a 'text' column.\n",
    "text = ''  # raw text\n",
    "if 'train' in ds:\n",
    "    text = '\\n'.join(ds['train']['text'])\n",
    "else:\n",
    "    # fallback if dataset has only one split\n",
    "    first_split = list(ds.keys())[0]\n",
    "    text = '\\n'.join(ds[first_split]['text'])\n",
    "\n",
    "# Filter to ASCII to avoid a huge or noisy character set\n",
    "text = ''.join(ch for ch in text if ch.isascii())  # raw text\n",
    "print('Text length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6e0e7",
   "metadata": {},
   "source": [
    "## 3) Build character vocabulary\n",
    "We build a vocabulary of all unique characters. Each character gets an integer ID. This is the simplest possible tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8540527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))  # unique characters\n",
    "vocab_size = len(chars)  # vocabulary size\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}  # string-to-id map\n",
    "itos = {i: ch for ch, i in stoi.items()}  # id-to-string map\n",
    "\n",
    "print('Vocab size:', vocab_size)\n",
    "print('First 10 chars:', chars[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a38b71",
   "metadata": {},
   "source": [
    "## 4) Encode/decode helpers\n",
    "`encode` turns a string into a list of integers. `decode` reverses that mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8493ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ids):\n",
    "    return ''.join(itos[i] for i in ids)\n",
    "\n",
    "# Quick sanity check\n",
    "sample = text[:100]  # sample text\n",
    "print('Sample text:', repr(sample[:80]))\n",
    "encoded = encode(sample)  # encoded token ids\n",
    "print('Encoded length:', len(encoded))\n",
    "print('Round-trip:', decode(encoded[:80]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb310d",
   "metadata": {},
   "source": [
    "## 5) Train/val split\n",
    "We create a 90/10 split for quick evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b25509",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)  # raw text data\n",
    "n = int(0.9 * len(data))  # length or count\n",
    "train_data = data[:n]  # training split data\n",
    "val_data = data[n:]  # validation split data\n",
    "\n",
    "print('Train tokens:', train_data.numel())\n",
    "print('Val tokens:', val_data.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3930553c",
   "metadata": {},
   "source": [
    "## 6) Batch sampler\n",
    "We sample random chunks of length `BLOCK_SIZE`. For each chunk, the input is characters `x`, and the target is the next character `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc143ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    # Random starting indices\n",
    "    ix = torch.randint(len(data_split) - BLOCK_SIZE - 1, (BATCH_SIZE,))\n",
    "    x = torch.stack([data_split[i:i+BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "    return x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('xb shape:', xb.shape)\n",
    "print('yb shape:', yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01807ff",
   "metadata": {},
   "source": [
    "## 7) Bigram model definition\n",
    "A **bigram model** predicts the next character based solely on the current character. The simplest version is just a lookup table of logits of size `(vocab_size, vocab_size)`.\n",
    "\n",
    "Optional: we can use an embedding table followed by a linear layer. This has the same effect if the embedding dimension equals the vocab size, but also lets us use smaller embeddings for a tiny bit of generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469fe03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        if embed_dim is None:\n",
    "            # Direct bigram logits: for each token id, store logits over next token\n",
    "            self.logit_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        else:\n",
    "            # Optional: smaller embedding -> linear to vocab logits\n",
    "            self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "            self.proj = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx: (B, T)\n",
    "        if self.embed_dim is None:\n",
    "            logits = self.logit_table(idx)  # (B, T, vocab_size)\n",
    "        else:\n",
    "            emb = self.embed(idx)\n",
    "            logits = self.proj(emb)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Flatten the batch/time for cross-entropy\n",
    "            B, T, C = logits.shape\n",
    "            logits_flat = logits.view(B*T, C)\n",
    "            targets_flat = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        # idx: (B, T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            # Focus on last time step\n",
    "            logits_last = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits_last, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size, EMBED_DIM).to(DEVICE)  # model instance\n",
    "print(model)\n",
    "\n",
    "# Sanity check: forward pass shapes and loss\n",
    "xb, yb = get_batch('train')\n",
    "logits, loss = model(xb, yb)\n",
    "print('logits shape:', logits.shape)\n",
    "print('loss:', loss.item())\n",
    "print('logits[0,0,:5]:', logits[0, 0, :5].detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558487c",
   "metadata": {},
   "source": [
    "## 8) Loss function (cross entropy)\n",
    "Cross-entropy compares the model's predicted logits against the true next character. It is the standard loss for classification and next-token prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e254e1a",
   "metadata": {},
   "source": [
    "## 9) Training loop with periodic evaluation\n",
    "We'll train for a small number of steps and print periodic updates. We'll also generate short samples to see qualitative progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f295dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    losses = {}\n",
    "    for split in ['train', 'val']:\n",
    "        loss_vals = []\n",
    "        for _ in range(10):\n",
    "            xb, yb = get_batch(split)\n",
    "            _, loss = model(xb, yb)\n",
    "            loss_vals.append(loss.item())\n",
    "        losses[split] = sum(loss_vals) / len(loss_vals)\n",
    "    model.train()\n",
    "    return losses\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)  # optimizer instance\n",
    "\n",
    "# optimization steps\n",
    "for step in range(1, MAX_STEPS + 1):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % EVAL_EVERY == 0 or step == 1:\n",
    "        losses = estimate_loss()\n",
    "        print('Step {:4d} | train loss {:.4f} | val loss {:.4f}'.format(step, losses['train'], losses['val']))\n",
    "        # Generate a tiny sample to see progress\n",
    "        context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
    "        sample_ids = model.generate(context, max_new_tokens=200, temperature=1.0)[0].tolist()\n",
    "        print('Sample:', decode(sample_ids))\n",
    "        print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Inference: generate text with sampling\n",
    "We can control creativity with a **temperature**. Lower values make the model more confident; higher values make it more random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)  # current context window\n",
    "for temp in [0.8, 1.0, 1.2]:\n",
    "    out_ids = model.generate(context, max_new_tokens=400, temperature=temp)[0].tolist()\n",
    "    print(f'\\nTemperature={temp}')\n",
    "    print(decode(out_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Notes\n",
    "Real LLMs use **many layers**, **attention heads**, and **large embedding dimensions**, trained on huge datasets (billions of tokens). Typical scale knobs include:\n",
    "- Number of layers\n",
    "- Number of attention heads\n",
    "- Embedding dimension\n",
    "- Context length\n",
    "- Dataset size and diversity\n",
    "\n",
    "Here we deliberately keep the model tiny to make the mechanics easy to understand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}