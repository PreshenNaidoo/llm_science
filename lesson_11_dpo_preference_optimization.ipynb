{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 11: Direct Preference Optimization (DPO)\n",
        "\n",
        "In many real-world applications, we care about *preferences*, not just next-token prediction.\\n",
        "Preference optimization teaches a model to produce responses that humans prefer by comparing a **chosen**\n",
        "answer against a **rejected** answer for the same prompt.\\n",
        "\n",
        "DPO (Direct Preference Optimization) is a simple, stable way to do this without explicit RL loops:\\n",
        "it pushes the model to assign higher likelihood to preferred answers than to dispreferred ones.\n",
        "\n",
        "Goals in this notebook:\\n",
        "- Understand preference optimization at a high level.\n",
        "- Fine-tune a small model (GPT-2) with DPO using TRL if available.\n",
        "- Fall back to a tiny PyTorch-only pairwise loss if TRL is missing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup\n",
        "We will use `transformers` for the model and tokenizer, `datasets` for data, and optionally `trl` for DPO.\n",
        "If TRL is missing, we will still run a simplified pairwise objective.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import random\nimport math\nimport os\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset, Dataset\n\n# Optional TRL for real DPO training\ntry:\n    from trl import DPOTrainer, DPOConfig\n    HAS_TRL = True  # TRL availability flag\nexcept Exception:\n    HAS_TRL = False  # TRL availability flag\n    print(\"TRL not installed. You can install it with: pip install trl\")\n\ntorch.manual_seed(0)\nrandom.seed(0)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Preference Dataset (chosen vs rejected)\n",
        "We will try to load a small preference dataset from Hugging Face. If that fails, we will create a tiny\n",
        "synthetic dataset so the notebook still runs end-to-end.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def split_prompt_answer(text):\n    \"\"\"Best-effort split for datasets without explicit prompt column.\"\"\"\n    marker = \"Assistant:\"\n    if marker in text:\n        prompt, answer = text.split(marker, 1)\n        return (prompt + marker).strip() + \"\\n\", answer.strip()\n    # Fallback: no split\n    return \"\", text.strip()\n\ndef try_load_preference_dataset(max_rows=200):\n    candidates = [\n        (\"Dahoas/rm-static\", \"train\"),\n        (\"Anthropic/hh-rlhf\", \"train\"),\n    ]\n\n    for name, split in candidates:\n        try:\n            ds = load_dataset(name, split=f\"{split}[:1%]\")\n            # Normalize columns into prompt/chosen/rejected\n            if \"prompt\" in ds.column_names:\n                def mapper(ex):\n                    return {\n                        \"prompt\": ex[\"prompt\"],\n                        \"chosen\": ex[\"chosen\"],\n                        \"rejected\": ex[\"rejected\"],\n                    }\n            else:\n                def mapper(ex):\n                    c_prompt, c_ans = split_prompt_answer(ex[\"chosen\"])\n                    r_prompt, r_ans = split_prompt_answer(ex[\"rejected\"])\n                    prompt = c_prompt if c_prompt else r_prompt\n                    return {\n                        \"prompt\": prompt,\n                        \"chosen\": c_ans,\n                        \"rejected\": r_ans,\n                    }\n\n            ds = ds.map(mapper, remove_columns=ds.column_names)\n            ds = ds.filter(lambda x: len(x[\"chosen\"]) > 0 and len(x[\"rejected\"]) > 0)\n            ds = ds.shuffle(seed=0)\n            ds = ds.select(range(min(max_rows, len(ds))))\n            print(f\"Loaded dataset: {name} with {len(ds)} pairs\")\n            return ds\n        except Exception as e:\n            print(f\"Failed to load {name}: {e}\")\n    return None\n\ndef build_synthetic_pairs(num_pairs=200):\n    questions = [\n        \"What is the capital of France?\",\n        \"Explain photosynthesis in one sentence.\",\n        \"What is 2 + 2?\",\n        \"Name one benefit of regular exercise.\",\n        \"What does CPU stand for?\",\n        \"How do you boil an egg?\",\n        \"What is the largest planet in our solar system?\",\n        \"Give a short definition of gravity.\",\n        \"What is a triangle?\",\n        \"What is the boiling point of water in Celsius?\",\n    ]\n\n    good_answers = [\n        \"The capital of France is Paris.\",\n        \"Photosynthesis is the process by which plants use sunlight to make food from carbon dioxide and water.\",\n        \"2 + 2 equals 4.\",\n        \"Regular exercise improves health and fitness, such as stronger muscles and a healthier heart.\",\n        \"CPU stands for Central Processing Unit.\",\n        \"Place eggs in boiling water for about 8 to 10 minutes, then cool them in cold water.\",\n        \"Jupiter is the largest planet in our solar system.\",\n        \"Gravity is the force that pulls objects toward each other.\",\n        \"A triangle is a three-sided polygon.\",\n        \"Water boils at 100 degrees Celsius at sea level.\",\n    ]\n\n    bad_answers = [\n        \"I do not know.\",\n        \"Maybe.\",\n        \"No idea.\",\n        \"Because it just is.\",\n        \"It is not important.\",\n    ]\n\n    rows = []\n    for i in range(num_pairs):\n        idx = i % len(questions)\n        prompt = questions[idx]\n        chosen = good_answers[idx]\n        # Make a bad answer by picking something short and unhelpful\n        rejected = random.choice(bad_answers)\n        rows.append({\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected})\n    return Dataset.from_list(rows)\n\ndataset = try_load_preference_dataset(max_rows=200)\nif dataset is None:\n    print(\"Falling back to a synthetic preference dataset.\")\n    dataset = build_synthetic_pairs(num_pairs=200)\n\n# Simple train/test split\ndataset = dataset.shuffle(seed=0)\nsplit_idx = int(0.9 * len(dataset))\ntrain_ds = dataset.select(range(split_idx))\neval_ds = dataset.select(range(split_idx, len(dataset)))\n\nprint(train_ds[0])\nprint(f\"Train pairs: {len(train_ds)}, Eval pairs: {len(eval_ds)}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Tokenization\n",
        "GPT-2 does not have a padding token, so we reuse the end-of-sequence token as padding.\n",
        "We will build prompt + answer sequences and mark prompt tokens so we do not score them when computing\n",
        "the pairwise preference loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "model_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmax_prompt_length = 128\nmax_length = 256\n\ndef encode_prompt_answer(prompt, answer):\n    # Tokenize prompt alone so we can mask its tokens in the loss\n    prompt_ids = tokenizer(prompt, truncation=True, max_length=max_prompt_length)[\"input_ids\"]\n    full = tokenizer(prompt + answer, truncation=True, max_length=max_length)\n    input_ids = full[\"input_ids\"]\n    labels = input_ids.copy()\n    # Mask prompt tokens so we only score the answer\n    labels[: len(prompt_ids)] = [-100] * len(prompt_ids)\n    attention_mask = [1] * len(input_ids)\n    return input_ids, labels, attention_mask"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) DPO Training\n",
        "If TRL is installed, we will use `DPOTrainer`, which implements the DPO loss directly.\n",
        "Otherwise, we fall back to a tiny PyTorch loop that maximizes the log-probability gap between\n",
        "chosen and rejected answers. This fallback is a simplified version of the DPO idea.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Load the model for training\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n\nif HAS_TRL:\n    # TRL DPO training\n    ref_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n    config = DPOConfig(\n        output_dir=\"dpo_outputs\",\n        per_device_train_batch_size=2,\n        num_train_epochs=1,\n        gradient_accumulation_steps=4,\n        learning_rate=1e-5,\n        logging_steps=5,\n        max_length=max_length,\n        max_prompt_length=max_prompt_length,\n        report_to=[],\n    )\n    trainer = DPOTrainer(\n        model=model,\n        ref_model=ref_model,\n        args=config,\n        train_dataset=train_ds,\n        tokenizer=tokenizer,\n    )\n    train_result = trainer.train()\n    print(train_result)\nelse:\n    # PyTorch fallback: pairwise preference loss (DPO-like)\n    beta = 0.1\n    batch_size = 4\n    lr = 1e-5\n    num_steps = 50\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    model.train()\n\n    def collate_fn(batch):\n        def encode_pair(prompt, answer):\n            input_ids, labels, attention_mask = encode_prompt_answer(prompt, answer)\n            return input_ids, labels, attention_mask\n\n        chosen_inputs = []\n        rejected_inputs = []\n\n        for ex in batch:\n            c = encode_pair(ex[\"prompt\"], ex[\"chosen\"])\n            r = encode_pair(ex[\"prompt\"], ex[\"rejected\"])\n            chosen_inputs.append(c)\n            rejected_inputs.append(r)\n\n        def pad_batch(seqs, pad_token_id, label_pad_id=-100):\n            max_len = max(len(s[0]) for s in seqs)\n            input_ids = []\n            labels = []\n            attention_mask = []\n\n            for ids, lab, mask in seqs:\n                pad_len = max_len - len(ids)\n                input_ids.append(ids + [pad_token_id] * pad_len)\n                labels.append(lab + [label_pad_id] * pad_len)\n                attention_mask.append(mask + [0] * pad_len)\n\n            return (\n                torch.tensor(input_ids, dtype=torch.long),\n                torch.tensor(labels, dtype=torch.long),\n                torch.tensor(attention_mask, dtype=torch.long),\n            )\n\n        c_input_ids, c_labels, c_attention = pad_batch(chosen_inputs, tokenizer.pad_token_id)\n        r_input_ids, r_labels, r_attention = pad_batch(rejected_inputs, tokenizer.pad_token_id)\n\n        return {\n            \"c_input_ids\": c_input_ids,\n            \"c_labels\": c_labels,\n            \"c_attention\": c_attention,\n            \"r_input_ids\": r_input_ids,\n            \"r_labels\": r_labels,\n            \"r_attention\": r_attention,\n        }\n\n    loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n\n    def sequence_logp(input_ids, labels, attention_mask):\n        # Compute log-probability of only the answer tokens (labels != -100)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits[:, :-1, :]\n        target = labels[:, 1:]\n        mask = (target != -100).float()\n\n        log_probs = torch.log_softmax(logits, dim=-1)\n        token_logp = log_probs.gather(-1, target.unsqueeze(-1)).squeeze(-1)\n        # Sum over answer tokens\n        return (token_logp * mask).sum(dim=-1)\n\n    step = 0\n    for epoch in range(1):\n        for batch in loader:\n            step += 1\n            if step > num_steps:\n                break\n\n            c_input_ids = batch[\"c_input_ids\"].to(device)\n            c_labels = batch[\"c_labels\"].to(device)\n            c_attention = batch[\"c_attention\"].to(device)\n            r_input_ids = batch[\"r_input_ids\"].to(device)\n            r_labels = batch[\"r_labels\"].to(device)\n            r_attention = batch[\"r_attention\"].to(device)\n\n            chosen_logp = sequence_logp(c_input_ids, c_labels, c_attention)\n            rejected_logp = sequence_logp(r_input_ids, r_labels, r_attention)\n\n            # DPO-like loss: encourage chosen > rejected\n            loss = -torch.nn.functional.logsigmoid(beta * (chosen_logp - rejected_logp)).mean()\n            acc = (chosen_logp > rejected_logp).float().mean().item()\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            if step % 10 == 0:\n                print(f\"step {step:03d} | loss {loss.item():.4f} | pairwise_acc {acc:.2f}\")\n\n        if step > num_steps:\n            break"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Inference: before vs after\n",
        "We compare a few prompts before and after training. The outputs will be noisy (GPT-2 is tiny), but\n",
        "you should see a slight preference shift toward more helpful answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def generate_text(model, prompt, max_new_tokens=60):\n    model.eval()\n    with torch.no_grad():\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=0.8,\n            top_p=0.9,\n        )\n    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\ntest_prompts = [train_ds[i][\"prompt\"] for i in range(3)]\n\n# Load a fresh base model for comparison\nbase_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n\nfor p in test_prompts:\n    print(\"Prompt:\", p)\n    print(\"Base:\", generate_text(base_model, p))\n    print(\"Tuned:\", generate_text(model, p))\n    print(\"-\" * 60)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Scaling Notes\n",
        "- Larger models improve noticeably with richer preference data.\n",
        "- Better preference pairs (more diverse prompts, higher-quality judgments) matter more than raw count.\n",
        "- Safety: preference data can encode bias; always review and filter sources.\n",
        "- In production, consider evaluation benchmarks and human review loops.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Exercises\n",
        "1) Increase `num_steps` and observe if the pairwise accuracy changes.\n",
        "2) Replace GPT-2 with a slightly larger model (e.g., `gpt2-medium`) and compare outputs.\n",
        "3) Try a different dataset and inspect how prompt formatting changes outcomes.\n",
        "4) Add your own prompt/answer pairs and see whether the model shifts.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}