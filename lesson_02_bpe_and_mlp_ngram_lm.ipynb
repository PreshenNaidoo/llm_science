{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE Tokenization + N-gram MLP Language Model\n",
    "\n",
    "This notebook teaches two ideas that sit at the core of modern language modeling:\n",
    "\n",
    "- **Why tokenization exists** and how **Byte Pair Encoding (BPE)** creates a compact vocabulary.\n",
    "- **How a tiny neural n-gram language model works**: use the previous *N* tokens to predict the next token with an embedding table and a small MLP.\n",
    "\n",
    "You will:\n",
    "1. Load a text dataset (Wikitext-2 if available, otherwise tiny Shakespeare).\n",
    "2. Train a tiny BPE tokenizer locally in the notebook.\n",
    "3. Build an n-gram dataset (context, target).\n",
    "4. Train a small neural language model and inspect predictions.\n",
    "5. Generate text with temperature sampling.\n",
    "\n",
    "This is a teaching notebook: it prioritizes clarity and explicit prints over speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and config\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Configuration (toy settings)\n",
    "CONTEXT_SIZE = 16  # context length (tokens)\n",
    "EMBED_DIM = 128  # token embedding dimension\n",
    "HIDDEN_DIM = 256  # hidden layer size\n",
    "BATCH_SIZE = 128  # batch size\n",
    "LR = 3e-4  # learning rate\n",
    "MAX_STEPS = 600  # training steps\n",
    "EVAL_EVERY = 100  # eval interval (steps)\n",
    "\n",
    "# Tokenizer settings\n",
    "TOKENIZER_VOCAB_SIZE = 2000  # target tokenizer vocab size\n",
    "BPE_TRAIN_CHARS = 200_000  # limit for quick demo training\n",
    "\n",
    "# Production-ish example values (much more compute required):\n",
    "# CONTEXT_SIZE = 256\n",
    "# EMBED_DIM = 512\n",
    "# HIDDEN_DIM = 2048\n",
    "# BATCH_SIZE = 512\n",
    "# MAX_STEPS = 20_000\n",
    "\n",
    "seed = 42  # random seed\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # device for tensors\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88834b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (load wikitext-2-raw-v1)\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")  # dataset object\n",
    "dataset_name = \"wikitext-2-raw-v1\"  # dataset name\n",
    "\n",
    "print(\"Dataset:\", dataset_name)\n",
    "print(ds)\n",
    "\n",
    "# Concatenate text with explicit newlines to preserve some structure\n",
    "train_text = \"\\n\".join(ds[\"train\"][\"text\"])  # training text\n",
    "val_text = \"\\n\".join(ds[\"validation\"][\"text\"]) if \"validation\" in ds else \"\"  # validation text\n",
    "\n",
    "# Keep a single large text for tokenizer training and LM training\n",
    "full_text = train_text + \"\\n\" + val_text  # concatenated dataset text\n",
    "print(\"Characters in full_text:\", len(full_text))\n",
    "print(\"Sample snippet:\\n\", full_text[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ef1ab",
   "metadata": {},
   "source": [
    "## Tokenization and BPE (intuition)\n",
    "\n",
    "Why tokenization?\n",
    "- Models operate on **discrete symbols**, but raw text is a stream of characters.\n",
    "- Word-level tokens create huge vocabularies and break on new words.\n",
    "- Character-level tokens are robust but long and slow for models.\n",
    "\n",
    "**BPE** is a compromise:\n",
    "1. Start with a small vocabulary (often characters).\n",
    "2. Count the most common adjacent pairs.\n",
    "3. Merge the most common pair into a new token.\n",
    "4. Repeat to build subword units that capture frequent patterns.\n",
    "\n",
    "Result: a compact vocabulary that can represent **any word** while still compressing frequent word pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e00342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a tiny BPE tokenizer\n",
    "# Uses the `tokenizers` library if available, otherwise a minimal pure-Python BPE fallback.\n",
    "\n",
    "SPECIAL_TOKENS = [\"<pad>\", \"<unk>\"]  # special token strings\n",
    "\n",
    "# --- Simple BPE fallback ---\n",
    "\n",
    "def _get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def _merge_vocab(pair, vocab):\n",
    "    bigram = \" \".join(pair)\n",
    "    replacement = \"\".join(pair)\n",
    "    new_vocab = {}\n",
    "    for word, freq in vocab.items():\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = freq\n",
    "    return new_vocab\n",
    "\n",
    "\n",
    "def _train_simple_bpe(text, vocab_size=2000, max_merges=200):\n",
    "    # Build word frequency table\n",
    "    words = [w for w in text.split() if w]\n",
    "    vocab = defaultdict(int)\n",
    "    for w in words:\n",
    "        vocab[\" \".join(list(w)) + \" </w>\"] += 1\n",
    "\n",
    "    # Estimate how many merges we can do\n",
    "    symbols = set()\n",
    "    for word in vocab:\n",
    "        symbols.update(word.split())\n",
    "    base_vocab = len(symbols) + len(SPECIAL_TOKENS)\n",
    "    num_merges = max(0, min(max_merges, vocab_size - base_vocab))\n",
    "\n",
    "    merges = []\n",
    "    for _ in range(num_merges):\n",
    "        pairs = _get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = _merge_vocab(best, vocab)\n",
    "        merges.append(best)\n",
    "\n",
    "    # Build final symbol set\n",
    "    symbols = set()\n",
    "    for word in vocab:\n",
    "        symbols.update(word.split())\n",
    "\n",
    "    tokens = list(SPECIAL_TOKENS) + sorted(symbols)\n",
    "    token_to_id = {t: i for i, t in enumerate(tokens)}\n",
    "    id_to_token = {i: t for t, i in token_to_id.items()}\n",
    "\n",
    "    def bpe_encode_word(word):\n",
    "        symbols = list(word) + [\"</w>\"]\n",
    "        for a, b in merges:\n",
    "            i = 0\n",
    "            new_symbols = []\n",
    "            while i < len(symbols):\n",
    "                if i < len(symbols) - 1 and symbols[i] == a and symbols[i + 1] == b:\n",
    "                    new_symbols.append(a + b)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "            symbols = new_symbols\n",
    "        return symbols\n",
    "\n",
    "    def encode_fn(text_in):\n",
    "        ids = []\n",
    "        for w in text_in.split():\n",
    "            for sym in bpe_encode_word(w):\n",
    "                ids.append(token_to_id.get(sym, token_to_id[\"<unk>\"]))\n",
    "        return ids\n",
    "\n",
    "    def decode_fn(ids_in):\n",
    "        tokens = [id_to_token[i] for i in ids_in]\n",
    "        text_out = \"\".join([\" \" if t == \"</w>\" else t for t in tokens])\n",
    "        # Normalize whitespace a bit\n",
    "        return \" \".join(text_out.split())\n",
    "\n",
    "    model = {\n",
    "        \"merges\": merges,\n",
    "        \"token_to_id\": token_to_id,\n",
    "    }\n",
    "    return model, encode_fn, decode_fn, len(tokens)\n",
    "\n",
    "\n",
    "def train_tokenizer(text, vocab_size=2000, save_path=\"tokenizer_bpe.json\"):\n",
    "    text = text[:BPE_TRAIN_CHARS]\n",
    "    try:\n",
    "        from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, normalizers\n",
    "\n",
    "        tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "        tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC()])\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "        trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=SPECIAL_TOKENS)\n",
    "\n",
    "        tokenizer.train_from_iterator([text], trainer=trainer)\n",
    "        tokenizer.save(save_path)\n",
    "        tokenizer = Tokenizer.from_file(save_path)\n",
    "\n",
    "        def encode_fn(s):\n",
    "            return tokenizer.encode(s).ids\n",
    "\n",
    "        def decode_fn(ids):\n",
    "            return tokenizer.decode(ids)\n",
    "\n",
    "        vocab_size_out = len(tokenizer.get_vocab())\n",
    "        pad_id = tokenizer.token_to_id(\"<pad>\")\n",
    "        return {\n",
    "            \"type\": \"tokenizers\",\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"encode\": encode_fn,\n",
    "            \"decode\": decode_fn,\n",
    "            \"vocab_size\": vocab_size_out,\n",
    "            \"pad_id\": pad_id,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"tokenizers not available or failed. Using simple BPE fallback.\")\n",
    "        print(\"Reason:\", repr(e))\n",
    "        model, encode_fn, decode_fn, vocab_size_out = _train_simple_bpe(text, vocab_size=vocab_size)\n",
    "        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(model, f)\n",
    "        with open(save_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            _ = json.load(f)\n",
    "\n",
    "        pad_id = model[\"token_to_id\"][\"<pad>\"]\n",
    "        return {\n",
    "            \"type\": \"simple_bpe\",\n",
    "            \"tokenizer\": model,\n",
    "            \"encode\": encode_fn,\n",
    "            \"decode\": decode_fn,\n",
    "            \"vocab_size\": vocab_size_out,\n",
    "            \"pad_id\": pad_id,\n",
    "        }\n",
    "\n",
    "\n",
    "bpe = train_tokenizer(full_text, vocab_size=TOKENIZER_VOCAB_SIZE)  # BPE tokenizer wrapper\n",
    "encode = bpe[\"encode\"]  # text-to-ids function\n",
    "decode = bpe[\"decode\"]  # ids-to-text function\n",
    "\n",
    "print(\"Tokenizer type:\", bpe[\"type\"])\n",
    "print(\"Vocab size:\", bpe[\"vocab_size\"])\n",
    "\n",
    "sample_text = \"Tokenization lets models read text like numbers.\"  # sample text string\n",
    "encoded = encode(sample_text)  # encoded token ids\n",
    "print(\"Sample text:\", sample_text)\n",
    "print(\"Encoded ids:\", encoded[:30])\n",
    "print(\"Decoded text:\", decode(encoded[:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff28de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode full text and build (context, target) pairs\n",
    "\n",
    "all_ids = encode(full_text)  # all token ids\n",
    "print(\"Total tokens:\", len(all_ids))\n",
    "\n",
    "split = int(0.9 * len(all_ids))  # train/val split index\n",
    "train_ids = all_ids[:split]  # token ids for training split\n",
    "val_ids = all_ids[split:]  # token ids for validation split\n",
    "\n",
    "class NGramDataset(Dataset):\n",
    "    def __init__(self, ids, context_size):\n",
    "        self.ids = torch.tensor(ids, dtype=torch.long)\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids) - self.context_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = self.ids[idx:idx + self.context_size]\n",
    "        target = self.ids[idx + self.context_size]\n",
    "        return context, target\n",
    "\n",
    "train_ds = NGramDataset(train_ids, CONTEXT_SIZE)  # training dataset\n",
    "val_ds = NGramDataset(val_ids, CONTEXT_SIZE)  # validation dataset\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)  # training DataLoader\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)  # validation DataLoader\n",
    "\n",
    "# Peek at a batch\n",
    "batch = next(iter(train_loader))  # current batch\n",
    "ctx, tgt = batch\n",
    "print(\"Context batch shape:\", ctx.shape)\n",
    "print(\"Target batch shape:\", tgt.shape)\n",
    "print(\"Decoded context example:\", decode(ctx[0].tolist()))\n",
    "print(\"Decoded target example:\", decode([tgt[0].item()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bdbb72",
   "metadata": {},
   "source": [
    "## Model: Embedding + MLP\n",
    "\n",
    "We embed each token into a vector, **flatten the context**, and run it through a 2-layer MLP to predict the next token. This is a neural version of an n-gram model:\n",
    "\n",
    "- Input: `B x CONTEXT_SIZE` token IDs\n",
    "- Embedding: `B x CONTEXT_SIZE x EMBED_DIM`\n",
    "- Flatten: `B x (CONTEXT_SIZE * EMBED_DIM)`\n",
    "- MLP -> logits over vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e26e890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(context_size * embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C)\n",
    "        emb = self.embed(x)  # (B, C, D)\n",
    "        flat = emb.view(x.size(0), -1)  # (B, C*D)\n",
    "        logits = self.mlp(flat)  # (B, V)\n",
    "        return logits\n",
    "\n",
    "model = NGramMLP(  # model instance\n",
    "    vocab_size=bpe[\"vocab_size\"],\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255ae471",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "We minimize cross-entropy loss between predicted logits and the true next token. We also report **perplexity**, a standard language-model metric:\n",
    "\n",
    "`perplexity = exp(loss)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7427f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for ctx, tgt in loader:\n",
    "            ctx = ctx.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            logits = model(ctx)\n",
    "            loss = F.cross_entropy(logits, tgt)\n",
    "            losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / max(1, len(losses))\n",
    "\n",
    "\n",
    "def show_predictions(model, dataset, num_examples=3):\n",
    "    model.eval()\n",
    "    for _ in range(num_examples):\n",
    "        idx = random.randint(0, len(dataset) - 1)\n",
    "        ctx, tgt = dataset[idx]\n",
    "        ctx = ctx.unsqueeze(0).to(device)\n",
    "        logits = model(ctx)\n",
    "        pred_id = int(torch.argmax(logits, dim=-1).item())\n",
    "\n",
    "        ctx_text = decode(ctx[0].tolist())\n",
    "        pred_text = decode([pred_id])\n",
    "        tgt_text = decode([int(tgt.item())])\n",
    "\n",
    "        print(\"Context:\", ctx_text)\n",
    "        print(\"Pred next:\", pred_text, \"| Target:\", tgt_text)\n",
    "        print(\"-\")\n",
    "    model.train()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)  # optimizer instance\n",
    "\n",
    "train_iter = iter(train_loader)  # training iterator\n",
    "# optimization steps\n",
    "for step in range(1, MAX_STEPS + 1):\n",
    "    try:\n",
    "        ctx, tgt = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_loader)\n",
    "        ctx, tgt = next(train_iter)\n",
    "\n",
    "    ctx = ctx.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "\n",
    "    logits = model(ctx)\n",
    "    loss = F.cross_entropy(logits, tgt)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % EVAL_EVERY == 0 or step == 1:\n",
    "        train_loss = loss.item()\n",
    "        val_loss = evaluate(model, val_loader)\n",
    "        print(f\"Step {step:4d} | train loss {train_loss:.4f} | val loss {val_loss:.4f} | val ppl {math.exp(val_loss):.2f}\")\n",
    "        show_predictions(model, val_ds, num_examples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2911089",
   "metadata": {},
   "source": [
    "## Inference: autoregressive generation\n",
    "\n",
    "We generate one token at a time, feeding the last `CONTEXT_SIZE` tokens back into the model. Temperature controls randomness:\n",
    "\n",
    "- **T < 1.0**: sharper, more deterministic\n",
    "- **T = 1.0**: baseline\n",
    "- **T > 1.0**: more diverse but noisier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f383daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt, max_new_tokens=80, temperature=1.0):\n",
    "    model.eval()\n",
    "    ids = encode(prompt)\n",
    "    pad_id = bpe[\"pad_id\"]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        context = ids[-CONTEXT_SIZE:]\n",
    "        if len(context) < CONTEXT_SIZE:\n",
    "            context = [pad_id] * (CONTEXT_SIZE - len(context)) + context\n",
    "\n",
    "        x = torch.tensor([context], dtype=torch.long).to(device)\n",
    "        logits = model(x)[0] / max(1e-6, temperature)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = int(torch.multinomial(probs, num_samples=1).item())\n",
    "        ids.append(next_id)\n",
    "\n",
    "    model.train()\n",
    "    return decode(ids)\n",
    "\n",
    "prompt = \"The meaning of tokenization\"  # input prompt string\n",
    "print(generate(model, prompt, max_new_tokens=80, temperature=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling notes (toy vs production-ish)\n",
    "\n",
    "Toy settings (this notebook):\n",
    "- Short context (16 tokens)\n",
    "- Small embeddings (128)\n",
    "- 2-layer MLP\n",
    "- Tiny vocab (~2k)\n",
    "\n",
    "Production-ish knobs to turn:\n",
    "- Longer context (128-1024+)\n",
    "- Larger embeddings (512-2048)\n",
    "- Deeper MLP or transformer layers\n",
    "- Larger BPE vocab (30k-100k)\n",
    "\n",
    "Each knob increases memory and compute requirements significantly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Increase `CONTEXT_SIZE` and compare validation perplexity.\n",
    "2. Change the number of BPE merges (or vocab size). How do the tokens change?\n",
    "3. Add dropout to the MLP and observe overfitting behavior.\n",
    "4. Replace the MLP with a single linear layer and compare performance.\n",
    "5. Try different temperatures during generation and evaluate coherence.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}