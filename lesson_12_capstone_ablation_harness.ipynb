{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 12: Capstone Ablation Harness\n",
        "\n",
        "Research engineering is about running controlled experiments with clean, reusable code and careful measurement.\n",
        "\n",
        "In this notebook we:\n",
        "- build a single GPT-like model with toggles (norm, position, FFN)\n",
        "- keep the data split fixed and seeds controlled\n",
        "- run short ablations and collect results in a table\n",
        "- pick the best config and do a quick inference demo\n",
        "\n",
        "The goal is not SOTA accuracy; it is a small, trustworthy experiment loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import math\nimport time\nimport random\nimport re\nfrom dataclasses import dataclass, asdict\nfrom collections import Counter\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\n\n\ndef set_seed(seed):\n    # Reproducibility across Python, NumPy, and PyTorch\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\nprint(\"Torch:\", torch.__version__)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset and tokenizer\n",
        "\n",
        "We prefer `wikitext-2-raw-v1` and fall back to a tiny dataset if the download is blocked.\n",
        "The tokenizer is intentionally simple (word and punctuation based) so you can see every step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def load_text_dataset():\n    from datasets import load_dataset\n    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n    name = \"wikitext-2-raw-v1\"\n    train_text = \"\\n\".join(ds[\"train\"][\"text\"])\n    val_text = \"\\n\".join(ds[\"validation\"][\"text\"]) if \"validation\" in ds else \"\"\n    return train_text, val_text, name\n\ntrain_text, val_text, dataset_name = load_text_dataset()\nprint(\"Dataset:\", dataset_name)\nprint(\"Train chars:\", len(train_text), \"Val chars:\", len(val_text))\nprint(\"Sample:\", train_text[:200].replace(\"\\n\", \" \"))",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizer and vocab\n",
        "\n",
        "We use a basic regex tokenizer and a capped vocabulary. Unknown tokens map to `<unk>`.\n",
        "This is much simpler than BPE, but it keeps the pipeline transparent.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "TOKEN_PATTERN = re.compile(r\"\\w+|[^\\w\\s]\", re.UNICODE)  # tokenization regex pattern\n\n\ndef basic_tokenize(text):\n    return TOKEN_PATTERN.findall(text)\n\n\ndef build_vocab(tokens, vocab_size, min_freq=1):\n    counts = Counter(tokens)\n    specials = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n    vocab_tokens = specials + [tok for tok, freq in counts.most_common() if freq >= min_freq]\n    vocab_tokens = vocab_tokens[:vocab_size]\n    stoi = {tok: i for i, tok in enumerate(vocab_tokens)}\n    itos = {i: tok for tok, i in stoi.items()}\n    return stoi, itos\n\n\ndef encode(text, stoi):\n    tokens = basic_tokenize(text)\n    return [stoi.get(tok, stoi[\"<unk>\"]) for tok in tokens]\n\n\ndef decode(ids, itos):\n    tokens = [itos.get(i, \"<unk>\") for i in ids]\n    text = \" \".join(tokens)\n    for p in [\".\", \",\", \"!\", \"?\", \":\", \";\", \")\", \"]\", \"'\"]:\n        text = text.replace(\" \" + p, p)\n    for p in [\"(\", \"[\"]:\n        text = text.replace(p + \" \", p)\n    return text\n\n\nVOCAB_SIZE = 8000  # vocab size\ntrain_tokens = basic_tokenize(train_text)\nstoi, itos = build_vocab(train_tokens, VOCAB_SIZE)\nVOCAB_SIZE = len(stoi)  # vocab size\n\ntrain_ids = torch.tensor(encode(train_text, stoi), dtype=torch.long)\nval_ids = torch.tensor(encode(val_text, stoi), dtype=torch.long)\n\nprint(\"Vocab size:\", VOCAB_SIZE)\nprint(\"Train tokens:\", train_ids.numel(), \"Val tokens:\", val_ids.numel())\nprint(\"Token sample:\", train_tokens[:20])\nprint(\"Decode sample:\", decode(train_ids[:30].tolist(), itos))",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config system\n",
        "\n",
        "A small dataclass captures hyperparameters and ablation toggles.\n",
        "We keep this separate so it is easy to log, compare, and reuse.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "@dataclass\nclass Config:\n    name: str = \"baseline\"\n    seed: int = 1337\n    vocab_size: int = VOCAB_SIZE\n    block_size: int = 128\n    batch_size: int = 32\n    max_steps: int = 300\n    eval_interval: int = 100\n    eval_iters: int = 50\n    lr: float = 3e-4\n    weight_decay: float = 0.1\n    grad_clip: float = 1.0\n\n    d_model: int = 128\n    n_heads: int = 4\n    n_layers: int = 2\n    ffn_mult: float = 4.0\n    dropout: float = 0.1\n\n    norm_type: str = \"layernorm\"  # or \"rmsnorm\"\n    positional: str = \"learned\"   # or \"rope\"\n    ffn_type: str = \"gelu\"        # or \"swiglu\"\n    rope_base: int = 10000\n\n\ndef print_config(cfg):\n    print(\"Config:\", cfg.name)\n    for k, v in asdict(cfg).items():\n        print(f\"  {k}: {v}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model with toggles\n",
        "\n",
        "We build a small GPT-like decoder-only Transformer.\n",
        "All three toggles are wired into this single implementation.\n",
        "\n",
        "PyTorch note: each `nn.Module` stores parameters, and `forward` describes how tensors flow.\n",
        "We use shapes `(B, T, C)` for batch, time (sequence length), and channels (hidden size).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class RMSNorm(nn.Module):\n    def __init__(self, dim, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x):\n        # x: (B, T, C)\n        norm = x.pow(2).mean(dim=-1, keepdim=True)\n        x = x * torch.rsqrt(norm + self.eps)\n        return self.weight * x\n\n\ndef make_norm(norm_type, dim):\n    if norm_type == \"layernorm\":\n        return nn.LayerNorm(dim)\n    if norm_type == \"rmsnorm\":\n        return RMSNorm(dim)\n    raise ValueError(f\"Unknown norm_type: {norm_type}\")\n\n\ndef build_rope_cache(seq_len, head_dim, base=10000, device=None):\n    # RoPE uses pairs of dimensions as complex numbers\n    theta = 1.0 / (base ** (torch.arange(0, head_dim, 2, device=device).float() / head_dim))\n    positions = torch.arange(seq_len, device=device).float()\n    freqs = torch.einsum(\"i,j->ij\", positions, theta)\n    cos = torch.cos(freqs)\n    sin = torch.sin(freqs)\n    return cos, sin\n\n\ndef apply_rope(x, cos, sin):\n    # x: (B, n_heads, T, head_dim)\n    x1 = x[..., 0::2]\n    x2 = x[..., 1::2]\n    cos = cos[: x.size(-2), :].unsqueeze(0).unsqueeze(0)\n    sin = sin[: x.size(-2), :].unsqueeze(0).unsqueeze(0)\n    rotated = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n    return rotated\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, d_model, n_heads, block_size, dropout, use_rope, rope_base=10000):\n        super().__init__()\n        if d_model % n_heads != 0:\n            raise ValueError(\"d_model must be divisible by n_heads\")\n        head_dim = d_model // n_heads\n        if use_rope and head_dim % 2 != 0:\n            raise ValueError(\"head_dim must be even for RoPE\")\n\n        self.n_heads = n_heads\n        self.head_dim = head_dim\n        self.use_rope = use_rope\n        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n        self.proj = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n        # Causal mask prevents attention from seeing future tokens\n        mask = torch.tril(torch.ones(block_size, block_size))\n        self.register_buffer(\"mask\", mask.view(1, 1, block_size, block_size))\n\n        if use_rope:\n            cos, sin = build_rope_cache(block_size, head_dim, base=rope_base, device=torch.device(\"cpu\"))\n            self.register_buffer(\"cos_cached\", cos)\n            self.register_buffer(\"sin_cached\", sin)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        qkv = self.qkv(x)\n        qkv = qkv.view(B, T, 3, self.n_heads, self.head_dim)\n        q, k, v = qkv.unbind(dim=2)\n        q = q.transpose(1, 2)  # (B, n_heads, T, head_dim)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        if self.use_rope:\n            q = apply_rope(q, self.cos_cached, self.sin_cached)\n            k = apply_rope(k, self.cos_cached, self.sin_cached)\n\n        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n        att = F.softmax(att, dim=-1)\n        att = self.dropout(att)\n\n        y = att @ v\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.proj(y)\n        y = self.dropout(y)\n        return y\n\n\nclass MLP(nn.Module):\n    def __init__(self, d_model, ffn_mult, dropout, ffn_type):\n        super().__init__()\n        hidden_dim = int(d_model * ffn_mult)\n        self.ffn_type = ffn_type\n        if ffn_type == \"gelu\":\n            self.fc1 = nn.Linear(d_model, hidden_dim)\n            self.fc2 = nn.Linear(hidden_dim, d_model)\n        elif ffn_type == \"swiglu\":\n            self.fc1 = nn.Linear(d_model, 2 * hidden_dim)\n            self.fc2 = nn.Linear(hidden_dim, d_model)\n        else:\n            raise ValueError(f\"Unknown ffn_type: {ffn_type}\")\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        if self.ffn_type == \"gelu\":\n            x = self.fc1(x)\n            x = F.gelu(x)\n            x = self.fc2(x)\n        else:\n            x = self.fc1(x)\n            x1, x2 = x.chunk(2, dim=-1)\n            x = F.silu(x1) * x2\n            x = self.fc2(x)\n        return self.dropout(x)\n\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.norm1 = make_norm(config.norm_type, config.d_model)\n        self.attn = CausalSelfAttention(\n            config.d_model,\n            config.n_heads,\n            config.block_size,\n            config.dropout,\n            use_rope=config.positional == \"rope\",\n            rope_base=config.rope_base,\n        )\n        self.norm2 = make_norm(config.norm_type, config.d_model)\n        self.mlp = MLP(config.d_model, config.ffn_mult, config.dropout, config.ffn_type)\n\n    def forward(self, x):\n        # Pre-norm Transformer block\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.tok_emb = nn.Embedding(config.vocab_size, config.d_model)\n        self.pos_emb = None\n        if config.positional == \"learned\":\n            self.pos_emb = nn.Embedding(config.block_size, config.d_model)\n        self.drop = nn.Dropout(config.dropout)\n        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layers)])\n        self.norm_f = make_norm(config.norm_type, config.d_model)\n        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n        self._printed_shapes = False\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        if isinstance(module, nn.Embedding):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        if T > self.config.block_size:\n            raise ValueError(\"Sequence length exceeds block_size\")\n\n        tok = self.tok_emb(idx)\n        if self.pos_emb is not None:\n            pos = torch.arange(T, device=idx.device)\n            pos = self.pos_emb(pos)[None, :, :]\n            x = tok + pos\n        else:\n            x = tok\n        x = self.drop(x)\n\n        for block in self.blocks:\n            x = block(x)\n        x = self.norm_f(x)\n        logits = self.lm_head(x)\n\n        loss = None\n        if targets is not None:\n            # Flatten for cross-entropy: (B*T, vocab)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n\n        if not self._printed_shapes:\n            print(\"Input idx:\", idx.shape)\n            print(\"Token emb:\", tok.shape)\n            print(\"Logits:\", logits.shape)\n            self._printed_shapes = True\n\n        return logits, loss",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and evaluation\n",
        "\n",
        "We use short runs with a fixed random seed and a consistent train/val split.\n",
        "`get_batch` creates random subsequences from the token stream.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def get_batch(data, config, generator):\n    # data is a 1D tensor of token ids\n    max_start = data.numel() - config.block_size - 1\n    ix = torch.randint(0, max_start, (config.batch_size,), generator=generator)\n    x = torch.stack([data[i : i + config.block_size] for i in ix])\n    y = torch.stack([data[i + 1 : i + 1 + config.block_size] for i in ix])\n    return x.to(device), y.to(device)\n\n\ndef estimate_loss(model, data, config):\n    # Run a few batches in eval mode and average the loss\n    model.eval()\n    losses = []\n    eval_gen = torch.Generator().manual_seed(config.seed + 1234)\n    with torch.no_grad():\n        for _ in range(config.eval_iters):\n            xb, yb = get_batch(data, config, eval_gen)\n            _, loss = model(xb, yb)\n            losses.append(loss.item())\n    model.train()\n    return sum(losses) / len(losses)\n\n\ndef train_model(config, train_data, val_data):\n    print_config(config)\n    set_seed(config.seed)\n    model = GPT(config).to(device)\n    num_params = sum(p.numel() for p in model.parameters())\n    print(\"Parameters:\", num_params)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n    train_gen = torch.Generator().manual_seed(config.seed)\n\n    start = time.perf_counter()\n    for step in range(1, config.max_steps + 1):\n        xb, yb = get_batch(train_data, config, train_gen)\n        _, loss = model(xb, yb)\n\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()  # compute gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n        optimizer.step()  # update weights\n\n        if step == 1 or step % config.eval_interval == 0:\n            val_loss = estimate_loss(model, val_data, config)\n            print(f\"step {step:4d} train_loss {loss.item():.4f} val_loss {val_loss:.4f}\")\n\n    runtime = time.perf_counter() - start\n    final_val_loss = estimate_loss(model, val_data, config)\n    return model, final_val_loss, runtime",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ablation runs\n",
        "\n",
        "We run three short experiments to see how the toggles behave.\n",
        "Feel free to add more configs or increase `max_steps` later.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "configs = [\n    Config(\n        name=\"LN + learned + GELU\",\n        norm_type=\"layernorm\",\n        positional=\"learned\",\n        ffn_type=\"gelu\",\n    ),\n    Config(\n        name=\"RMS + learned + SwiGLU\",\n        norm_type=\"rmsnorm\",\n        positional=\"learned\",\n        ffn_type=\"swiglu\",\n    ),\n    Config(\n        name=\"RMS + RoPE + SwiGLU\",\n        norm_type=\"rmsnorm\",\n        positional=\"rope\",\n        ffn_type=\"swiglu\",\n    ),\n]\n\nresults = []\nmodels = []\nfor i, cfg in enumerate(configs):\n    print(\"\\n--- Running\", cfg.name, \"---\")\n    model, val_loss, runtime = train_model(cfg, train_ids, val_ids)\n    ppl = math.exp(val_loss)\n    params = sum(p.numel() for p in model.parameters())\n    results.append({\n        \"config_id\": i,\n        \"name\": cfg.name,\n        \"norm\": cfg.norm_type,\n        \"positional\": cfg.positional,\n        \"ffn\": cfg.ffn_type,\n        \"val_loss\": val_loss,\n        \"perplexity\": ppl,\n        \"runtime_sec\": runtime,\n        \"params\": params,\n    })\n    models.append(model)\n\nresults_df = pd.DataFrame(results).sort_values(\"val_loss\")\nprint(results_df.to_string(index=False))\nresults_df",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference test\n",
        "\n",
        "We pick the best config by validation loss and generate a short sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "@torch.no_grad()\ndef generate(model, idx, max_new_tokens, temperature=1.0, top_k=50):\n    model.eval()\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -model.config.block_size :]\n        logits, _ = model(idx_cond)\n        logits = logits[:, -1, :] / temperature\n        if top_k is not None:\n            k = min(top_k, logits.size(-1))\n            v, _ = torch.topk(logits, k)\n            logits[logits < v[:, [-1]]] = -float(\"inf\")\n        probs = F.softmax(logits, dim=-1)\n        next_id = torch.multinomial(probs, num_samples=1)\n        idx = torch.cat([idx, next_id], dim=1)\n    return idx\n\n\nbest_id = int(results_df.iloc[0][\"config_id\"])\nbest_config = configs[best_id]\nbest_model = models[best_id]\nprint(\"Best config:\", best_config.name)\n\nset_seed(best_config.seed)\nprompt = \"The meaning of life is\"\nprompt_ids = torch.tensor([encode(prompt, stoi)], dtype=torch.long).to(device)\noutput_ids = generate(best_model, prompt_ids, max_new_tokens=40, temperature=1.0, top_k=50)\nprint(decode(output_ids[0].tolist(), itos))",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scaling notes\n",
        "\n",
        "- Larger sweeps: wrap the config list in a grid search and log to a CSV or parquet file.\n",
        "- Better logging: integrate Weights & Biases or TensorBoard for metrics and plots.\n",
        "- More compute: move to multi-GPU with `torch.distributed` or use gradient accumulation.\n",
        "- Reliability: add checkpoints, save model states, and log the dataset hash.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1) Add a fourth run that changes only the positional encoding and compare.\n",
        "2) Increase `max_steps` to 1000 and see if the ranking changes.\n",
        "3) Try a character-level tokenizer and compare perplexity.\n",
        "4) Replace the attention with PyTorch's `scaled_dot_product_attention` and time it.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}