{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 10: Mixture-of-Experts Transformer (Toy)\n",
        "\n",
        "Mixture-of-Experts (MoE) layers let a model have **more parameters without proportional compute**.\n",
        "A router decides which expert MLP(s) should process each token, so only a small subset of\n",
        "experts run per token. This notebook builds a tiny decoder-only Transformer with a toy\n",
        "MoE feedforward layer to make the idea concrete.\n",
        "\n",
        "We keep the code small and readable rather than highly optimized.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup\n",
        "We use PyTorch and keep defaults small so it runs quickly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import math\nimport random\nimport re\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(42)\nrandom.seed(42)\n\n# Small defaults for a toy run\nD_MODEL = 256  # model width\nN_HEADS = 4  # attention heads\nN_LAYERS = 4  # transformer layers\nN_EXPERTS = 4  # number of experts\nTOP_K = 1  # top-k routing\nSEQ_LEN = 128  # sequence length (tokens)\nBATCH_SIZE = 32  # batch size\nLR = 3e-4  # learning rate\nMAX_STEPS = 200  # training steps\nEVAL_EVERY = 50  # eval interval (steps)\nAUX_WEIGHT = 0.01  # aux loss weight\n\n# Bigger knobs (commented)\n# D_MODEL = 512\n# N_LAYERS = 8\n# N_EXPERTS = 8\n# TOP_K = 2\n# MAX_STEPS = 1000"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Dataset and tokenizer\n",
        "We try `wikitext-2-raw-v1`. If it is not available locally, we fall back to a tiny\n",
        "built-in text. The tokenizer is a simple regex word splitter to keep things transparent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def load_wikitext_or_fallback():\n    try:\n        from datasets import load_dataset\n\n        ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n        train_text = \"\\n\\n\".join(ds[\"train\"][\"text\"])\n        valid_text = \"\\n\\n\".join(ds[\"validation\"][\"text\"])\n        return train_text, valid_text\n    except Exception as e:\n        print(\"Falling back to a tiny built-in corpus. Reason:\", e)\n        fallback = (\n            \"In the beginning the universe was created. This has made a lot of people very angry. \"\n            \"It is a truth universally acknowledged, that a single man in possession of a good fortune, \"\n            \"must be in want of a wife. The quick brown fox jumps over the lazy dog. \"\n        )\n        return fallback, fallback\n\ndef tokenize(text):\n    # Split into words and punctuation. This is intentionally simple.\n    return re.findall(r\"\\w+|[^\\w\\s]\", text.lower())\n\ndef build_vocab(tokens, min_freq=2):\n    counter = Counter(tokens)\n    vocab = {\"<pad>\": 0, \"<unk>\": 1, \"<bos>\": 2, \"<eos>\": 3}\n    for tok, freq in counter.items():\n        if freq >= min_freq and tok not in vocab:\n            vocab[tok] = len(vocab)\n    id2tok = {i: t for t, i in vocab.items()}\n    return vocab, id2tok\n\ndef encode(tokens, vocab):\n    return [vocab.get(t, vocab[\"<unk>\"]) for t in tokens]\n\ndef decode(ids, id2tok):\n    return \" \".join(id2tok.get(i, \"<unk>\") for i in ids)\n\ntrain_text, valid_text = load_wikitext_or_fallback()\ntrain_tokens = [\"<bos>\"] + tokenize(train_text) + [\"<eos>\"]\nvalid_tokens = [\"<bos>\"] + tokenize(valid_text) + [\"<eos>\"]\n\nvocab, id2tok = build_vocab(train_tokens, min_freq=2)\ntrain_ids = encode(train_tokens, vocab)\nvalid_ids = encode(valid_tokens, vocab)\n\n# Truncate for a fast demo if the dataset is large\nmax_tokens = 200_000\nif len(train_ids) > max_tokens:\n    train_ids = train_ids[:max_tokens]\nif len(valid_ids) > max_tokens:\n    valid_ids = valid_ids[:max_tokens]\n\nprint(f\"Vocab size: {len(vocab)}\")\nprint(f\"Train tokens: {len(train_ids)}\")\n\nclass LMDataset(Dataset):\n    def __init__(self, ids, seq_len):\n        self.ids = ids\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return max(0, len(self.ids) - self.seq_len)\n\n    def __getitem__(self, idx):\n        x = torch.tensor(self.ids[idx : idx + self.seq_len], dtype=torch.long)\n        y = torch.tensor(self.ids[idx + 1 : idx + self.seq_len + 1], dtype=torch.long)\n        return x, y\n\ntrain_loader = DataLoader(LMDataset(train_ids, SEQ_LEN), batch_size=BATCH_SIZE, shuffle=True)\nvalid_loader = DataLoader(LMDataset(valid_ids, SEQ_LEN), batch_size=BATCH_SIZE)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Baseline Transformer decoder (minimal)\n",
        "We build a tiny decoder-only Transformer using PyTorch modules.\n",
        "The feedforward block is a standard 2-layer MLP.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=None, dropout=0.1):\n        super().__init__()\n        d_ff = d_ff or 4 * d_model\n        self.net = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.GELU(),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, n_heads, ffn):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(d_model)\n        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=0.1, batch_first=True)\n        self.ln2 = nn.LayerNorm(d_model)\n        self.ffn = ffn\n\n    def forward(self, x, attn_mask):\n        # Self-attention (causal mask prevents attending to future tokens)\n        h = self.ln1(x)\n        attn_out, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n        x = x + attn_out\n\n        # Feedforward; if the module returns (out, aux, stats), unpack it\n        h = self.ln2(x)\n        ffn_out = self.ffn(h)\n        aux_loss = torch.tensor(0.0, device=x.device)\n        stats = None\n        if isinstance(ffn_out, tuple):\n            ffn_out, aux_loss, stats = ffn_out\n        x = x + ffn_out\n        return x, aux_loss, stats\n\nclass TransformerLM(nn.Module):\n    def __init__(self, vocab_size, d_model, n_heads, n_layers, ffn_factory):\n        super().__init__()\n        self.tok_emb = nn.Embedding(vocab_size, d_model)\n        self.pos_emb = nn.Embedding(SEQ_LEN, d_model)\n        self.blocks = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, ffn_factory())\n            for _ in range(n_layers)\n        ])\n        self.ln_f = nn.LayerNorm(d_model)\n        self.head = nn.Linear(d_model, vocab_size, bias=False)\n\n    def forward(self, idx):\n        bsz, seq_len = idx.shape\n        pos = torch.arange(seq_len, device=idx.device).unsqueeze(0).expand(bsz, seq_len)\n        x = self.tok_emb(idx) + self.pos_emb(pos)\n\n        # Causal mask: True values are blocked\n        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=idx.device), diagonal=1).bool()\n\n        total_aux = torch.tensor(0.0, device=idx.device)\n        last_stats = None\n        for block in self.blocks:\n            x, aux_loss, stats = block(x, attn_mask)\n            total_aux = total_aux + aux_loss\n            if stats is not None:\n                last_stats = stats\n\n        x = self.ln_f(x)\n        logits = self.head(x)\n        return logits, total_aux, last_stats\n\ndef count_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nbaseline = TransformerLM(\n    vocab_size=len(vocab),\n    d_model=D_MODEL,\n    n_heads=N_HEADS,\n    n_layers=N_LAYERS,\n    ffn_factory=lambda: FeedForward(D_MODEL),\n)\nprint(f\"Baseline params: {count_params(baseline):,}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) MoE Feedforward\n",
        "The router is a linear layer that produces a score for each expert per token.\n",
        "We keep **top-k** experts per token, normalize their scores into gates, and\n",
        "combine the experts' outputs with those weights.\n",
        "\n",
        "We also compute a tiny load-balancing loss that encourages experts to be used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class MoEFeedForward(nn.Module):\n    def __init__(self, d_model, n_experts, top_k=1, d_ff=None, dropout=0.1):\n        super().__init__()\n        self.n_experts = n_experts\n        self.top_k = top_k\n        self.router = nn.Linear(d_model, n_experts)\n        self.experts = nn.ModuleList([\n            FeedForward(d_model, d_ff=d_ff, dropout=dropout)\n            for _ in range(n_experts)\n        ])\n\n    def forward(self, x):\n        # x: (batch, seq, d_model)\n        logits = self.router(x)  # (batch, seq, n_experts)\n        topk_vals, topk_idx = logits.topk(self.top_k, dim=-1)\n        topk_gates = F.softmax(topk_vals, dim=-1)\n\n        # Scatter normalized gates back into full expert dimension\n        gates = torch.zeros_like(logits)\n        gates.scatter_(-1, topk_idx, topk_gates)\n\n        # Toy implementation: evaluate all experts, then mix with gates\n        expert_outs = torch.stack([expert(x) for expert in self.experts], dim=2)\n        out = torch.sum(expert_outs * gates.unsqueeze(-1), dim=2)\n\n        # Routing statistics for the top-1 expert per token\n        top1 = topk_idx[..., 0]\n        counts = torch.bincount(top1.reshape(-1), minlength=self.n_experts)\n\n        # Simple load-balancing loss: encourage importance and load to align\n        importance = gates.sum(dim=(0, 1))\n        load = counts.float()\n        importance = importance / (importance.sum() + 1e-9)\n        load = load / (load.sum() + 1e-9)\n        aux_loss = (self.n_experts * (importance * load).sum())\n\n        stats = {\"counts\": counts.detach().cpu()}\n        return out, aux_loss, stats\n\nmoe_model = TransformerLM(\n    vocab_size=len(vocab),\n    d_model=D_MODEL,\n    n_heads=N_HEADS,\n    n_layers=N_LAYERS,\n    ffn_factory=lambda: MoEFeedForward(D_MODEL, N_EXPERTS, TOP_K),\n)\nprint(f\"MoE params: {count_params(moe_model):,}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Training loop\n",
        "We train briefly and print both the language modeling loss and routing counts.\n",
        "The auxiliary loss is tiny and only nudges the router to balance expert usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "model = moe_model.to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\nloss_fn = nn.CrossEntropyLoss()\n\nmodel.train()\nstep = 0\nfor x, y in train_loader:\n    x = x.to(device)\n    y = y.to(device)\n\n    logits, aux_loss, stats = model(x)\n    lm_loss = loss_fn(logits.view(-1, len(vocab)), y.view(-1))\n    total_loss = lm_loss + AUX_WEIGHT * aux_loss\n\n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n\n    if step % EVAL_EVERY == 0:\n        counts = stats[\"counts\"] if stats is not None else None\n        print(f\"step {step:4d} | lm_loss {lm_loss.item():.4f} | aux {aux_loss.item():.4f}\")\n        if counts is not None:\n            print(\"routing counts:\", counts.tolist())\n\n    step += 1\n    if step >= MAX_STEPS:\n        break"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Inference: generate text\n",
        "We generate a short sample with greedy decoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def generate(model, prompt, max_new_tokens=50):\n    model.eval()\n    ids = encode(tokenize(prompt), vocab)\n    ids = ids if ids else [vocab[\"<bos>\"]]\n    for _ in range(max_new_tokens):\n        x = torch.tensor(ids[-SEQ_LEN:], dtype=torch.long, device=device).unsqueeze(0)\n        with torch.no_grad():\n            logits, _, _ = model(x)\n        next_id = int(torch.argmax(logits[0, -1], dim=-1))\n        ids.append(next_id)\n        if next_id == vocab[\"<eos>\"]:\n            break\n    return decode(ids, id2tok)\n\nprint(generate(model, \"in the\", max_new_tokens=40))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Scaling notes\n",
        "Real MoE systems add several engineering ideas:\n",
        "- Distributed experts: experts live on different devices and only selected experts run.\n",
        "- Capacity factors: limit how many tokens each expert processes to avoid overload.\n",
        "- Auxiliary losses: encourage balanced routing and stable training.\n",
        "- Token dropping or overflow buffers when experts are full.\n",
        "\n",
        "Our toy version computes all experts (slow) and ignores capacity limits. It is\n",
        "meant for clarity, not performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Exercises\n",
        "1) Change TOP_K to 2 and compare routing counts and loss.\n",
        "2) Increase N_EXPERTS and see how the parameter count grows.\n",
        "3) Add a temperature to the router logits before top-k selection.\n",
        "4) Replace greedy decoding with top-k sampling in `generate`.\n",
        "5) Try turning off the auxiliary loss and compare routing balance.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}