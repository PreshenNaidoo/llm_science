{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Language Model over BPE Tokens\n",
    "\n",
    "This notebook teaches the historical importance of **RNNs** for language modeling and implements a small **GRU-based** autoregressive model on BPE tokens.\n",
    "\n",
    "You will:\n",
    "1. Load a text dataset (Wikitext-2 if available, otherwise tiny Shakespeare).\n",
    "2. Load a BPE tokenizer from Lesson 02 if it exists, otherwise train a small one here.\n",
    "3. Build fixed-length training sequences with **teacher forcing**.\n",
    "4. Train a GRU language model with gradient clipping.\n",
    "5. Generate text with temperature and top-k sampling.\n",
    "\n",
    "The focus is clarity and interpretability rather than speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and config\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Configuration (toy settings)\n",
    "SEQ_LEN = 64  # sequence length (tokens)\n",
    "BATCH_SIZE = 32  # batch size\n",
    "EMBED_DIM = 256  # token embedding dimension\n",
    "HIDDEN_DIM = 256  # hidden layer size\n",
    "LR = 3e-4  # learning rate\n",
    "MAX_STEPS = 800  # training steps\n",
    "EVAL_EVERY = 100  # eval interval (steps)\n",
    "\n",
    "# Production-ish example values (much more compute required):\n",
    "# SEQ_LEN = 256\n",
    "# BATCH_SIZE = 128\n",
    "# EMBED_DIM = 512\n",
    "# HIDDEN_DIM = 512\n",
    "# MAX_STEPS = 20_000\n",
    "\n",
    "seed = 42  # random seed\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # device for tensors\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (load wikitext-2-raw-v1)\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")  # dataset object\n",
    "dataset_name = \"wikitext-2-raw-v1\"  # dataset name\n",
    "\n",
    "print(\"Dataset:\", dataset_name)\n",
    "print(ds)\n",
    "\n",
    "# Concatenate text with explicit newlines to preserve some structure\n",
    "train_text = \"\\n\".join(ds[\"train\"][\"text\"])  # training text\n",
    "val_text = \"\\n\".join(ds[\"validation\"][\"text\"]) if \"validation\" in ds else \"\"  # validation text\n",
    "\n",
    "full_text = train_text + \"\\n\" + val_text  # concatenated dataset text\n",
    "print(\"Characters in full_text:\", len(full_text))\n",
    "print(\"Sample snippet:\\n\", full_text[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization: load or train BPE\n",
    "\n",
    "If `./tokenizer_bpe.json` exists (from Lesson 02), we load it. Otherwise we train a small BPE tokenizer in this notebook.\n",
    "\n",
    "Why BPE? It balances word-level and character-level tradeoffs by learning frequent subword units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"<pad>\", \"<unk>\"]  # special token strings\n",
    "TOKENIZER_PATH = Path(\"tokenizer_bpe.json\")  # tokenizer file path\n",
    "TOKENIZER_VOCAB_SIZE = 2000  # target tokenizer vocab size\n",
    "BPE_TRAIN_CHARS = 200_000  # max chars for BPE training\n",
    "\n",
    "# --- Simple BPE fallback ---\n",
    "\n",
    "def _get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def _merge_vocab(pair, vocab):\n",
    "    bigram = \" \".join(pair)\n",
    "    replacement = \"\".join(pair)\n",
    "    new_vocab = {}\n",
    "    for word, freq in vocab.items():\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = freq\n",
    "    return new_vocab\n",
    "\n",
    "\n",
    "def _train_simple_bpe(text, vocab_size=2000, max_merges=200):\n",
    "    words = [w for w in text.split() if w]\n",
    "    vocab = defaultdict(int)\n",
    "    for w in words:\n",
    "        vocab[\" \".join(list(w)) + \" </w>\"] += 1\n",
    "\n",
    "    symbols = set()\n",
    "    for word in vocab:\n",
    "        symbols.update(word.split())\n",
    "    base_vocab = len(symbols) + len(SPECIAL_TOKENS)\n",
    "    num_merges = max(0, min(max_merges, vocab_size - base_vocab))\n",
    "\n",
    "    merges = []\n",
    "    for _ in range(num_merges):\n",
    "        pairs = _get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = _merge_vocab(best, vocab)\n",
    "        merges.append(best)\n",
    "\n",
    "    symbols = set()\n",
    "    for word in vocab:\n",
    "        symbols.update(word.split())\n",
    "\n",
    "    tokens = list(SPECIAL_TOKENS) + sorted(symbols)\n",
    "    token_to_id = {t: i for i, t in enumerate(tokens)}\n",
    "    id_to_token = {i: t for t, i in token_to_id.items()}\n",
    "\n",
    "    def bpe_encode_word(word):\n",
    "        symbols = list(word) + [\"</w>\"]\n",
    "        for a, b in merges:\n",
    "            i = 0\n",
    "            new_symbols = []\n",
    "            while i < len(symbols):\n",
    "                if i < len(symbols) - 1 and symbols[i] == a and symbols[i + 1] == b:\n",
    "                    new_symbols.append(a + b)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "            symbols = new_symbols\n",
    "        return symbols\n",
    "\n",
    "    def encode_fn(text_in):\n",
    "        ids = []\n",
    "        for w in text_in.split():\n",
    "            for sym in bpe_encode_word(w):\n",
    "                ids.append(token_to_id.get(sym, token_to_id[\"<unk>\"]))\n",
    "        return ids\n",
    "\n",
    "    def decode_fn(ids_in):\n",
    "        tokens = [id_to_token[i] for i in ids_in]\n",
    "        text_out = \"\".join([\" \" if t == \"</w>\" else t for t in tokens])\n",
    "        return \" \".join(text_out.split())\n",
    "\n",
    "    model = {\n",
    "        \"merges\": merges,\n",
    "        \"token_to_id\": token_to_id,\n",
    "    }\n",
    "    return model, encode_fn, decode_fn, len(tokens)\n",
    "\n",
    "\n",
    "def _load_simple_bpe(path):\n",
    "    model = json.loads(Path(path).read_text())\n",
    "    merges = [tuple(m) for m in model.get(\"merges\", [])]\n",
    "    token_to_id = model.get(\"token_to_id\", {})\n",
    "    id_to_token = {i: t for t, i in token_to_id.items()}\n",
    "\n",
    "    def bpe_encode_word(word):\n",
    "        symbols = list(word) + [\"</w>\"]\n",
    "        for a, b in merges:\n",
    "            i = 0\n",
    "            new_symbols = []\n",
    "            while i < len(symbols):\n",
    "                if i < len(symbols) - 1 and symbols[i] == a and symbols[i + 1] == b:\n",
    "                    new_symbols.append(a + b)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "            symbols = new_symbols\n",
    "        return symbols\n",
    "\n",
    "    def encode_fn(text_in):\n",
    "        ids = []\n",
    "        for w in text_in.split():\n",
    "            for sym in bpe_encode_word(w):\n",
    "                ids.append(token_to_id.get(sym, token_to_id[\"<unk>\"]))\n",
    "        return ids\n",
    "\n",
    "    def decode_fn(ids_in):\n",
    "        tokens = [id_to_token[i] for i in ids_in]\n",
    "        text_out = \"\".join([\" \" if t == \"</w>\" else t for t in tokens])\n",
    "        return \" \".join(text_out.split())\n",
    "\n",
    "    vocab_size_out = len(token_to_id)\n",
    "    pad_id = token_to_id.get(\"<pad>\", 0)\n",
    "    return model, encode_fn, decode_fn, vocab_size_out, pad_id\n",
    "\n",
    "\n",
    "def build_or_load_tokenizer(text, vocab_size=2000):\n",
    "    text = text[:BPE_TRAIN_CHARS]\n",
    "    if TOKENIZER_PATH.exists():\n",
    "        # Try tokenizers JSON first\n",
    "        try:\n",
    "            from tokenizers import Tokenizer\n",
    "            tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "\n",
    "            def encode_fn(s):\n",
    "                return tokenizer.encode(s).ids\n",
    "\n",
    "            def decode_fn(ids):\n",
    "                return tokenizer.decode(ids)\n",
    "\n",
    "            vocab_size_out = len(tokenizer.get_vocab())\n",
    "            pad_id = tokenizer.token_to_id(\"<pad>\")\n",
    "            return {\n",
    "                \"type\": \"tokenizers\",\n",
    "                \"tokenizer\": tokenizer,\n",
    "                \"encode\": encode_fn,\n",
    "                \"decode\": decode_fn,\n",
    "                \"vocab_size\": vocab_size_out,\n",
    "                \"pad_id\": pad_id,\n",
    "                \"path\": str(TOKENIZER_PATH),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(\"Tokenizer file exists but tokenizers load failed. Falling back.\")\n",
    "            print(\"Reason:\", repr(e))\n",
    "            try:\n",
    "                model, encode_fn, decode_fn, vocab_size_out, pad_id = _load_simple_bpe(TOKENIZER_PATH)\n",
    "                return {\n",
    "                    \"type\": \"simple_bpe\",\n",
    "                    \"tokenizer\": model,\n",
    "                    \"encode\": encode_fn,\n",
    "                    \"decode\": decode_fn,\n",
    "                    \"vocab_size\": vocab_size_out,\n",
    "                    \"pad_id\": pad_id,\n",
    "                    \"path\": str(TOKENIZER_PATH),\n",
    "                }\n",
    "            except Exception as e2:\n",
    "                print(\"Simple BPE load failed. Training new tokenizer.\")\n",
    "                print(\"Reason:\", repr(e2))\n",
    "\n",
    "    # Train new tokenizer\n",
    "    try:\n",
    "        from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "\n",
    "        tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "        tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC()])\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "        trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=SPECIAL_TOKENS)\n",
    "\n",
    "        tokenizer.train_from_iterator([text], trainer=trainer)\n",
    "        tokenizer.save(str(TOKENIZER_PATH))\n",
    "\n",
    "        def encode_fn(s):\n",
    "            return tokenizer.encode(s).ids\n",
    "\n",
    "        def decode_fn(ids):\n",
    "            return tokenizer.decode(ids)\n",
    "\n",
    "        vocab_size_out = len(tokenizer.get_vocab())\n",
    "        pad_id = tokenizer.token_to_id(\"<pad>\")\n",
    "        return {\n",
    "            \"type\": \"tokenizers\",\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"encode\": encode_fn,\n",
    "            \"decode\": decode_fn,\n",
    "            \"vocab_size\": vocab_size_out,\n",
    "            \"pad_id\": pad_id,\n",
    "            \"path\": str(TOKENIZER_PATH),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"tokenizers not available or failed. Using simple BPE.\")\n",
    "        print(\"Reason:\", repr(e))\n",
    "        model, encode_fn, decode_fn, vocab_size_out = _train_simple_bpe(text, vocab_size=vocab_size)\n",
    "        TOKENIZER_PATH.write_text(json.dumps(model))\n",
    "        pad_id = model[\"token_to_id\"][\"<pad>\"]\n",
    "        return {\n",
    "            \"type\": \"simple_bpe\",\n",
    "            \"tokenizer\": model,\n",
    "            \"encode\": encode_fn,\n",
    "            \"decode\": decode_fn,\n",
    "            \"vocab_size\": vocab_size_out,\n",
    "            \"pad_id\": pad_id,\n",
    "            \"path\": str(TOKENIZER_PATH),\n",
    "        }\n",
    "\n",
    "\n",
    "bpe = build_or_load_tokenizer(full_text, vocab_size=TOKENIZER_VOCAB_SIZE)  # BPE tokenizer wrapper\n",
    "encode = bpe[\"encode\"]  # text-to-ids function\n",
    "decode = bpe[\"decode\"]  # ids-to-text function\n",
    "\n",
    "print(\"Tokenizer type:\", bpe[\"type\"])\n",
    "print(\"Tokenizer path:\", bpe[\"path\"])\n",
    "print(\"Vocab size:\", bpe[\"vocab_size\"])\n",
    "\n",
    "sample_text = \"RNNs process tokens sequentially.\"  # sample text string\n",
    "encoded = encode(sample_text)  # encoded token ids\n",
    "print(\"Sample text:\", sample_text)\n",
    "print(\"Encoded ids:\", encoded[:20])\n",
    "print(\"Decoded text:\", decode(encoded[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training sequences (teacher forcing)\n",
    "\n",
    "We build fixed-length sequences of token IDs. For each sequence, the model **sees** tokens `x` and is trained to **predict** the next tokens `y`.\n",
    "\n",
    "This is called **teacher forcing**: the correct previous tokens are fed into the model while predicting the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode text and build (input, target) sequences\n",
    "all_ids = encode(full_text)  # all token ids\n",
    "print(\"Total tokens:\", len(all_ids))\n",
    "\n",
    "split = int(0.9 * len(all_ids))  # train/val split index\n",
    "train_ids = all_ids[:split]  # token ids for training split\n",
    "val_ids = all_ids[split:]  # token ids for validation split\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, ids, seq_len):\n",
    "        self.ids = torch.tensor(ids, dtype=torch.long)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids) - self.seq_len - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.ids[idx:idx + self.seq_len + 1]\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        return x, y\n",
    "\n",
    "train_ds = SequenceDataset(train_ids, SEQ_LEN)  # training dataset\n",
    "val_ds = SequenceDataset(val_ids, SEQ_LEN)  # validation dataset\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)  # training DataLoader\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)  # validation DataLoader\n",
    "\n",
    "x0, y0 = next(iter(train_loader))\n",
    "print(\"Input batch shape:\", x0.shape)\n",
    "print(\"Target batch shape:\", y0.shape)\n",
    "print(\"Decoded input sample:\", decode(x0[0][:20].tolist()))\n",
    "print(\"Decoded target sample:\", decode(y0[0][:20].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Embedding + GRU + Linear\n",
    "\n",
    "An RNN processes sequences step-by-step, maintaining a **hidden state** that summarizes the past.\n",
    "A **GRU** is a gated RNN that improves training stability compared to a vanilla RNN.\n",
    "\n",
    "We use:\n",
    "- Embedding layer\n",
    "- GRU layer\n",
    "- Linear projection to vocabulary logits\n",
    "\n",
    "Output shape: `(B, T, V)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: (B, T)\n",
    "        emb = self.embed(x)  # (B, T, D)\n",
    "        out, hidden = self.gru(emb, hidden)  # out: (B, T, H)\n",
    "        logits = self.fc(out)  # (B, T, V)\n",
    "        return logits, hidden\n",
    "\n",
    "model = GRULM(  # model instance\n",
    "    vocab_size=bpe[\"vocab_size\"],\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop with gradient clipping\n",
    "\n",
    "We flatten `(B, T, V)` logits to `(B*T, V)` and compute cross-entropy against `(B*T)` targets.\n",
    "Gradient clipping (e.g., `clip_norm=1.0`) helps prevent exploding gradients in RNNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits, _ = model(x)\n",
    "            vocab_size = logits.size(-1)\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "            losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / max(1, len(losses))\n",
    "\n",
    "\n",
    "def show_predictions(model, batch_x, batch_y, num_tokens=12):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(batch_x.to(device))\n",
    "        pred_ids = torch.argmax(logits, dim=-1).cpu()\n",
    "\n",
    "        print(\"Input:\", decode(batch_x[0][:num_tokens].tolist()))\n",
    "        print(\"Pred :\", decode(pred_ids[0][:num_tokens].tolist()))\n",
    "        print(\"Tgt  :\", decode(batch_y[0][:num_tokens].tolist()))\n",
    "    model.train()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)  # optimizer instance\n",
    "\n",
    "train_iter = iter(train_loader)  # training iterator\n",
    "# optimization steps\n",
    "for step in range(1, MAX_STEPS + 1):\n",
    "    try:\n",
    "        x, y = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_loader)\n",
    "        x, y = next(train_iter)\n",
    "\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    logits, _ = model(x)\n",
    "    vocab_size = logits.size(-1)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % EVAL_EVERY == 0 or step == 1:\n",
    "        val_loss = evaluate(model, val_loader)\n",
    "        print(f\"Step {step:4d} | train loss {loss.item():.4f} | val loss {val_loss:.4f} | val ppl {math.exp(val_loss):.2f}\")\n",
    "        show_predictions(model, x.cpu(), y.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: autoregressive generation\n",
    "\n",
    "We generate tokens one by one while carrying the GRU hidden state forward.\n",
    "We can control randomness with **temperature** and **top-k sampling**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_token(logits, temperature=1.0, top_k=None):\n",
    "    logits = logits / max(1e-6, temperature)\n",
    "    if top_k is not None and top_k > 0:\n",
    "        v, idx = torch.topk(logits, top_k)\n",
    "        mask = torch.full_like(logits, float(\"-inf\"))\n",
    "        mask.scatter_(0, idx, v)\n",
    "        logits = mask\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return int(torch.multinomial(probs, num_samples=1).item())\n",
    "\n",
    "\n",
    "def generate(model, prompt, max_new_tokens=80, temperature=1.0, top_k=50):\n",
    "    model.eval()\n",
    "    ids = encode(prompt)\n",
    "    pad_id = bpe[\"pad_id\"]\n",
    "\n",
    "    hidden = None\n",
    "    if not ids:\n",
    "        ids = [pad_id]\n",
    "\n",
    "    # Warm up the hidden state with the prompt\n",
    "    for token_id in ids[:-1]:\n",
    "        x = torch.tensor([[token_id]], dtype=torch.long).to(device)\n",
    "        _, hidden = model(x, hidden)\n",
    "\n",
    "    cur = ids[-1]\n",
    "    for _ in range(max_new_tokens):\n",
    "        x = torch.tensor([[cur]], dtype=torch.long).to(device)\n",
    "        logits, hidden = model(x, hidden)\n",
    "        next_id = sample_next_token(logits[0, -1], temperature=temperature, top_k=top_k)\n",
    "        ids.append(next_id)\n",
    "        cur = next_id\n",
    "\n",
    "    model.train()\n",
    "    return decode(ids)\n",
    "\n",
    "prompt = \"The meaning of sequence\"  # input prompt string\n",
    "print(generate(model, prompt, max_new_tokens=80, temperature=0.9, top_k=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling notes: GRU vs Transformer\n",
    "\n",
    "RNNs (including GRUs) were historically important because they can model sequences with a hidden state and were efficient on small data.\n",
    "\n",
    "However, **Transformers** dominate today because:\n",
    "- They process sequences in parallel (faster on GPUs).\n",
    "- Self-attention captures long-range dependencies better.\n",
    "- Scaling tends to improve performance more predictably.\n",
    "\n",
    "GRUs are still useful for small models, streaming tasks, or environments where memory is tight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Increase `SEQ_LEN` and observe changes in validation perplexity.\n",
    "2. Compare `temperature=0.7` vs `1.2` during generation.\n",
    "3. Set `top_k=None` and see how generation changes.\n",
    "4. Increase `HIDDEN_DIM` and compare training stability.\n",
    "5. Try a 2-layer GRU and compare perplexity vs runtime.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}