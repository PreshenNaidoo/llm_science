{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# GRU Language Model over BPE Tokens\n\nThis notebook teaches the historical importance of **RNNs** for language modeling and implements a small **GRU-based** autoregressive model on BPE tokens.\n\nYou will:\n1. Load a text dataset (Wikitext-2 if available, otherwise tiny Shakespeare).\n2. Load a BPE tokenizer from Lesson 02 if it exists, otherwise train a small one here.\n3. Build fixed-length training sequences with **teacher forcing**.\n4. Train a GRU language model with gradient clipping.\n5. Generate text with temperature and top-k sampling.\n\nThe focus is clarity and interpretability rather than speed.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Setup and config\nimport math\nimport os\nimport random\nimport json\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# Configuration (toy settings)\nSEQ_LEN = 64  # sequence length (tokens)\nBATCH_SIZE = 32  # batch size\nEMBED_DIM = 256  # token embedding dimension\nHIDDEN_DIM = 256  # hidden layer size\nLR = 3e-4  # learning rate\nMAX_STEPS = 800  # training steps\nEVAL_EVERY = 100  # eval interval (steps)\n\n# Production-ish example values (much more compute required):\n# SEQ_LEN = 256\n# BATCH_SIZE = 128\n# EMBED_DIM = 512\n# HIDDEN_DIM = 512\n# MAX_STEPS = 20_000\n\nseed = 42\nrandom.seed(seed)\ntorch.manual_seed(seed)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Load dataset (load wikitext-2-raw-v1)\nfrom datasets import load_dataset\n\nds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\ndataset_name = \"wikitext-2-raw-v1\"\n\nprint(\"Dataset:\", dataset_name)\nprint(ds)\n\n# Concatenate text with explicit newlines to preserve some structure\ntrain_text = \"\\n\".join(ds[\"train\"][\"text\"])\nval_text = \"\\n\".join(ds[\"validation\"][\"text\"]) if \"validation\" in ds else \"\"\n\nfull_text = train_text + \"\\n\" + val_text\nprint(\"Characters in full_text:\", len(full_text))\nprint(\"Sample snippet:\\n\", full_text[:400])",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Tokenization: load or train BPE\n\nIf `./tokenizer_bpe.json` exists (from Lesson 02), we load it. Otherwise we train a small BPE tokenizer in this notebook.\n\nWhy BPE? It balances word-level and character-level tradeoffs by learning frequent subword units.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "SPECIAL_TOKENS = [\"<pad>\", \"<unk>\"]  # special token strings\nTOKENIZER_PATH = Path(\"tokenizer_bpe.json\")  # tokenizer file path\nTOKENIZER_VOCAB_SIZE = 2000  # target tokenizer vocab size\nBPE_TRAIN_CHARS = 200_000  # max chars for BPE training\n\n# --- Simple BPE fallback ---\n\ndef _get_stats(vocab):\n    pairs = defaultdict(int)\n    for word, freq in vocab.items():\n        symbols = word.split()\n        for i in range(len(symbols) - 1):\n            pairs[(symbols[i], symbols[i + 1])] += freq\n    return pairs\n\n\ndef _merge_vocab(pair, vocab):\n    bigram = \" \".join(pair)\n    replacement = \"\".join(pair)\n    new_vocab = {}\n    for word, freq in vocab.items():\n        new_word = word.replace(bigram, replacement)\n        new_vocab[new_word] = freq\n    return new_vocab\n\n\ndef _train_simple_bpe(text, vocab_size=2000, max_merges=200):\n    words = [w for w in text.split() if w]\n    vocab = defaultdict(int)\n    for w in words:\n        vocab[\" \".join(list(w)) + \" </w>\"] += 1\n\n    symbols = set()\n    for word in vocab:\n        symbols.update(word.split())\n    base_vocab = len(symbols) + len(SPECIAL_TOKENS)\n    num_merges = max(0, min(max_merges, vocab_size - base_vocab))\n\n    merges = []\n    for _ in range(num_merges):\n        pairs = _get_stats(vocab)\n        if not pairs:\n            break\n        best = max(pairs, key=pairs.get)\n        vocab = _merge_vocab(best, vocab)\n        merges.append(best)\n\n    symbols = set()\n    for word in vocab:\n        symbols.update(word.split())\n\n    tokens = list(SPECIAL_TOKENS) + sorted(symbols)\n    token_to_id = {t: i for i, t in enumerate(tokens)}\n    id_to_token = {i: t for t, i in token_to_id.items()}\n\n    def bpe_encode_word(word):\n        symbols = list(word) + [\"</w>\"]\n        for a, b in merges:\n            i = 0\n            new_symbols = []\n            while i < len(symbols):\n                if i < len(symbols) - 1 and symbols[i] == a and symbols[i + 1] == b:\n                    new_symbols.append(a + b)\n                    i += 2\n                else:\n                    new_symbols.append(symbols[i])\n                    i += 1\n            symbols = new_symbols\n        return symbols\n\n    def encode_fn(text_in):\n        ids = []\n        for w in text_in.split():\n            for sym in bpe_encode_word(w):\n                ids.append(token_to_id.get(sym, token_to_id[\"<unk>\"]))\n        return ids\n\n    def decode_fn(ids_in):\n        tokens = [id_to_token[i] for i in ids_in]\n        text_out = \"\".join([\" \" if t == \"</w>\" else t for t in tokens])\n        return \" \".join(text_out.split())\n\n    model = {\n        \"merges\": merges,\n        \"token_to_id\": token_to_id,\n    }\n    return model, encode_fn, decode_fn, len(tokens)\n\n\ndef _load_simple_bpe(path):\n    model = json.loads(Path(path).read_text())\n    merges = [tuple(m) for m in model.get(\"merges\", [])]\n    token_to_id = model.get(\"token_to_id\", {})\n    id_to_token = {i: t for t, i in token_to_id.items()}\n\n    def bpe_encode_word(word):\n        symbols = list(word) + [\"</w>\"]\n        for a, b in merges:\n            i = 0\n            new_symbols = []\n            while i < len(symbols):\n                if i < len(symbols) - 1 and symbols[i] == a and symbols[i + 1] == b:\n                    new_symbols.append(a + b)\n                    i += 2\n                else:\n                    new_symbols.append(symbols[i])\n                    i += 1\n            symbols = new_symbols\n        return symbols\n\n    def encode_fn(text_in):\n        ids = []\n        for w in text_in.split():\n            for sym in bpe_encode_word(w):\n                ids.append(token_to_id.get(sym, token_to_id[\"<unk>\"]))\n        return ids\n\n    def decode_fn(ids_in):\n        tokens = [id_to_token[i] for i in ids_in]\n        text_out = \"\".join([\" \" if t == \"</w>\" else t for t in tokens])\n        return \" \".join(text_out.split())\n\n    vocab_size_out = len(token_to_id)\n    pad_id = token_to_id.get(\"<pad>\", 0)\n    return model, encode_fn, decode_fn, vocab_size_out, pad_id\n\n\ndef build_or_load_tokenizer(text, vocab_size=2000):\n    text = text[:BPE_TRAIN_CHARS]\n    if TOKENIZER_PATH.exists():\n        # Try tokenizers JSON first\n        try:\n            from tokenizers import Tokenizer\n            tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n\n            def encode_fn(s):\n                return tokenizer.encode(s).ids\n\n            def decode_fn(ids):\n                return tokenizer.decode(ids)\n\n            vocab_size_out = len(tokenizer.get_vocab())\n            pad_id = tokenizer.token_to_id(\"<pad>\")\n            return {\n                \"type\": \"tokenizers\",\n                \"tokenizer\": tokenizer,\n                \"encode\": encode_fn,\n                \"decode\": decode_fn,\n                \"vocab_size\": vocab_size_out,\n                \"pad_id\": pad_id,\n                \"path\": str(TOKENIZER_PATH),\n            }\n        except Exception as e:\n            print(\"Tokenizer file exists but tokenizers load failed. Falling back.\")\n            print(\"Reason:\", repr(e))\n            try:\n                model, encode_fn, decode_fn, vocab_size_out, pad_id = _load_simple_bpe(TOKENIZER_PATH)\n                return {\n                    \"type\": \"simple_bpe\",\n                    \"tokenizer\": model,\n                    \"encode\": encode_fn,\n                    \"decode\": decode_fn,\n                    \"vocab_size\": vocab_size_out,\n                    \"pad_id\": pad_id,\n                    \"path\": str(TOKENIZER_PATH),\n                }\n            except Exception as e2:\n                print(\"Simple BPE load failed. Training new tokenizer.\")\n                print(\"Reason:\", repr(e2))\n\n    # Train new tokenizer\n    try:\n        from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n\n        tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n        tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC()])\n        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n        trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=SPECIAL_TOKENS)\n\n        tokenizer.train_from_iterator([text], trainer=trainer)\n        tokenizer.save(str(TOKENIZER_PATH))\n\n        def encode_fn(s):\n            return tokenizer.encode(s).ids\n\n        def decode_fn(ids):\n            return tokenizer.decode(ids)\n\n        vocab_size_out = len(tokenizer.get_vocab())\n        pad_id = tokenizer.token_to_id(\"<pad>\")\n        return {\n            \"type\": \"tokenizers\",\n            \"tokenizer\": tokenizer,\n            \"encode\": encode_fn,\n            \"decode\": decode_fn,\n            \"vocab_size\": vocab_size_out,\n            \"pad_id\": pad_id,\n            \"path\": str(TOKENIZER_PATH),\n        }\n    except Exception as e:\n        print(\"tokenizers not available or failed. Using simple BPE.\")\n        print(\"Reason:\", repr(e))\n        model, encode_fn, decode_fn, vocab_size_out = _train_simple_bpe(text, vocab_size=vocab_size)\n        TOKENIZER_PATH.write_text(json.dumps(model))\n        pad_id = model[\"token_to_id\"][\"<pad>\"]\n        return {\n            \"type\": \"simple_bpe\",\n            \"tokenizer\": model,\n            \"encode\": encode_fn,\n            \"decode\": decode_fn,\n            \"vocab_size\": vocab_size_out,\n            \"pad_id\": pad_id,\n            \"path\": str(TOKENIZER_PATH),\n        }\n\n\nbpe = build_or_load_tokenizer(full_text, vocab_size=TOKENIZER_VOCAB_SIZE)\nencode = bpe[\"encode\"]\ndecode = bpe[\"decode\"]\n\nprint(\"Tokenizer type:\", bpe[\"type\"])\nprint(\"Tokenizer path:\", bpe[\"path\"])\nprint(\"Vocab size:\", bpe[\"vocab_size\"])\n\nsample_text = \"RNNs process tokens sequentially.\"\nencoded = encode(sample_text)\nprint(\"Sample text:\", sample_text)\nprint(\"Encoded ids:\", encoded[:20])\nprint(\"Decoded text:\", decode(encoded[:20]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Prepare training sequences (teacher forcing)\n\nWe build fixed-length sequences of token IDs. For each sequence, the model **sees** tokens `x` and is trained to **predict** the next tokens `y`.\n\nThis is called **teacher forcing**: the correct previous tokens are fed into the model while predicting the next step.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Encode text and build (input, target) sequences\nall_ids = encode(full_text)\nprint(\"Total tokens:\", len(all_ids))\n\nsplit = int(0.9 * len(all_ids))\ntrain_ids = all_ids[:split]\nval_ids = all_ids[split:]\n\nclass SequenceDataset(Dataset):\n    def __init__(self, ids, seq_len):\n        self.ids = torch.tensor(ids, dtype=torch.long)\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return len(self.ids) - self.seq_len - 1\n\n    def __getitem__(self, idx):\n        chunk = self.ids[idx:idx + self.seq_len + 1]\n        x = chunk[:-1]\n        y = chunk[1:]\n        return x, y\n\ntrain_ds = SequenceDataset(train_ids, SEQ_LEN)\nval_ds = SequenceDataset(val_ids, SEQ_LEN)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n\nx0, y0 = next(iter(train_loader))\nprint(\"Input batch shape:\", x0.shape)\nprint(\"Target batch shape:\", y0.shape)\nprint(\"Decoded input sample:\", decode(x0[0][:20].tolist()))\nprint(\"Decoded target sample:\", decode(y0[0][:20].tolist()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Model: Embedding + GRU + Linear\n\nAn RNN processes sequences step-by-step, maintaining a **hidden state** that summarizes the past.\nA **GRU** is a gated RNN that improves training stability compared to a vanilla RNN.\n\nWe use:\n- Embedding layer\n- GRU layer\n- Linear projection to vocabulary logits\n\nOutput shape: `(B, T, V)`\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class GRULM(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, x, hidden=None):\n        # x: (B, T)\n        emb = self.embed(x)  # (B, T, D)\n        out, hidden = self.gru(emb, hidden)  # out: (B, T, H)\n        logits = self.fc(out)  # (B, T, V)\n        return logits, hidden\n\nmodel = GRULM(\n    vocab_size=bpe[\"vocab_size\"],\n    embed_dim=EMBED_DIM,\n    hidden_dim=HIDDEN_DIM,\n).to(device)\n\nprint(model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Training loop with gradient clipping\n\nWe flatten `(B, T, V)` logits to `(B*T, V)` and compute cross-entropy against `(B*T)` targets.\nGradient clipping (e.g., `clip_norm=1.0`) helps prevent exploding gradients in RNNs.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def evaluate(model, loader):\n    model.eval()\n    losses = []\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device)\n            logits, _ = model(x)\n            vocab_size = logits.size(-1)\n            loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n            losses.append(loss.item())\n    model.train()\n    return sum(losses) / max(1, len(losses))\n\n\ndef show_predictions(model, batch_x, batch_y, num_tokens=12):\n    model.eval()\n    with torch.no_grad():\n        logits, _ = model(batch_x.to(device))\n        pred_ids = torch.argmax(logits, dim=-1).cpu()\n\n        print(\"Input:\", decode(batch_x[0][:num_tokens].tolist()))\n        print(\"Pred :\", decode(pred_ids[0][:num_tokens].tolist()))\n        print(\"Tgt  :\", decode(batch_y[0][:num_tokens].tolist()))\n    model.train()\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n\ntrain_iter = iter(train_loader)\nfor step in range(1, MAX_STEPS + 1):\n    try:\n        x, y = next(train_iter)\n    except StopIteration:\n        train_iter = iter(train_loader)\n        x, y = next(train_iter)\n\n    x = x.to(device)\n    y = y.to(device)\n\n    logits, _ = model(x)\n    vocab_size = logits.size(-1)\n    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n\n    optimizer.zero_grad()\n    loss.backward()\n    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    optimizer.step()\n\n    if step % EVAL_EVERY == 0 or step == 1:\n        val_loss = evaluate(model, val_loader)\n        print(f\"Step {step:4d} | train loss {loss.item():.4f} | val loss {val_loss:.4f} | val ppl {math.exp(val_loss):.2f}\")\n        show_predictions(model, x.cpu(), y.cpu())",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Inference: autoregressive generation\n\nWe generate tokens one by one while carrying the GRU hidden state forward.\nWe can control randomness with **temperature** and **top-k sampling**.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def sample_next_token(logits, temperature=1.0, top_k=None):\n    logits = logits / max(1e-6, temperature)\n    if top_k is not None and top_k > 0:\n        v, idx = torch.topk(logits, top_k)\n        mask = torch.full_like(logits, float(\"-inf\"))\n        mask.scatter_(0, idx, v)\n        logits = mask\n    probs = F.softmax(logits, dim=-1)\n    return int(torch.multinomial(probs, num_samples=1).item())\n\n\ndef generate(model, prompt, max_new_tokens=80, temperature=1.0, top_k=50):\n    model.eval()\n    ids = encode(prompt)\n    pad_id = bpe[\"pad_id\"]\n\n    hidden = None\n    if not ids:\n        ids = [pad_id]\n\n    # Warm up the hidden state with the prompt\n    for token_id in ids[:-1]:\n        x = torch.tensor([[token_id]], dtype=torch.long).to(device)\n        _, hidden = model(x, hidden)\n\n    cur = ids[-1]\n    for _ in range(max_new_tokens):\n        x = torch.tensor([[cur]], dtype=torch.long).to(device)\n        logits, hidden = model(x, hidden)\n        next_id = sample_next_token(logits[0, -1], temperature=temperature, top_k=top_k)\n        ids.append(next_id)\n        cur = next_id\n\n    model.train()\n    return decode(ids)\n\nprompt = \"The meaning of sequence\"\nprint(generate(model, prompt, max_new_tokens=80, temperature=0.9, top_k=40))",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Scaling notes: GRU vs Transformer\n\nRNNs (including GRUs) were historically important because they can model sequences with a hidden state and were efficient on small data.\n\nHowever, **Transformers** dominate today because:\n- They process sequences in parallel (faster on GPUs).\n- Self-attention captures long-range dependencies better.\n- Scaling tends to improve performance more predictably.\n\nGRUs are still useful for small models, streaming tasks, or environments where memory is tight.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercises\n\n1. Increase `SEQ_LEN` and observe changes in validation perplexity.\n2. Compare `temperature=0.7` vs `1.2` during generation.\n3. Set `top_k=None` and see how generation changes.\n4. Increase `HIDDEN_DIM` and compare training stability.\n5. Try a 2-layer GRU and compare perplexity vs runtime.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}