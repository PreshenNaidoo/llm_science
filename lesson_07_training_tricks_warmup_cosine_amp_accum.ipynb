{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 07: Training Tricks (Warmup + Cosine + AMP + Accumulation)\n",
    "\n",
    "This notebook builds a tiny GPT-like decoder and uses it to show **training-engineering tricks** that matter in real systems:\n",
    "\n",
    "- **AdamW + weight decay** (why it is different from L2 in Adam)\n",
    "- **learning rate warmup + cosine decay** (stability and smooth convergence)\n",
    "- **gradient accumulation** (simulate larger batches without more GPU memory)\n",
    "- **optional AMP** (mixed precision for speed on GPU)\n",
    "\n",
    "The goal is not state-of-the-art performance. The goal is a *clear, runnable training loop* where you can see how each trick changes the mechanics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup\n",
    "\n",
    "We set seeds for reproducibility, pick a device, and define configuration knobs. The values are intentionally small so the notebook can run on a laptop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce4fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SEED = 42  # random seed\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # compute device\n",
    "\n",
    "BATCH_SIZE = 32  # batch size\n",
    "BLOCK_SIZE = 128  # context length (tokens)\n",
    "EMB_SIZE = 256  # embedding size\n",
    "N_LAYERS = 4  # transformer layers\n",
    "N_HEADS = 4  # attention heads\n",
    "DROPOUT = 0.1  # dropout prob\n",
    "\n",
    "MAX_STEPS = 1000  # training steps\n",
    "EVAL_INTERVAL = 100  # eval interval (steps)\n",
    "EVAL_ITERS = 20  # eval batches\n",
    "\n",
    "LEARNING_RATE = 3e-4  # learning rate\n",
    "WEIGHT_DECAY = 0.1  # weight decay\n",
    "WARMUP_STEPS = 100  # lr warmup steps\n",
    "ACCUM_STEPS = 4  # gradient accumulation steps\n",
    "AMP_ENABLED = torch.cuda.is_available()  # use mixed precision\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print('Device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7add367",
   "metadata": {},
   "source": [
    "## 2) Dataset + tokenizer (WikiText-2)\n",
    "\n",
    "We use the `wikitext-2-raw-v1` dataset from `torchtext`. We build a basic English tokenizer and a vocabulary, then concatenate tokens into a 1D tensor.\n",
    "\n",
    "**Note:** The first run will download the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b90c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (wikitext-2-raw-v1) using Hugging Face datasets\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")  # dataset object\n",
    "train_text = \"\\n\".join(ds[\"train\"][\"text\"])  # training text\n",
    "val_text = \"\\n\".join(ds[\"validation\"][\"text\"]) if \"validation\" in ds else \"\"  # validation text\n",
    "\n",
    "# Build a simple character-level vocabulary\n",
    "text = train_text + \"\\n\" + val_text  # raw text\n",
    "chars = sorted(list(set(text)))  # unique characters\n",
    "VOCAB_SIZE = len(chars)  # vocab size\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}  # string-to-id map\n",
    "itos = {i: ch for i, ch in enumerate(chars)}  # id-to-string map\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ids):\n",
    "    return ''.join([itos[i] for i in ids])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)  # raw text data\n",
    "n = int(0.9 * len(data))  # length or count\n",
    "train_data = data[:n]  # training split data\n",
    "val_data = data[n:]  # validation split data\n",
    "\n",
    "print('Vocab size:', len(chars))\n",
    "print('Train tokens:', len(train_data), 'Val tokens:', len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48068bdc",
   "metadata": {},
   "source": [
    "## 3) GPT-like decoder (simple)\n",
    "\n",
    "We implement a tiny, decoder-only Transformer with:\n",
    "- token + position embeddings\n",
    "- stacked self-attention blocks with causal mask\n",
    "- linear head for next-token prediction\n",
    "\n",
    "This is intentionally small and minimal so the training loop stays readable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd417749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, emb_size, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=emb_size,\n",
    "            num_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_size, 4 * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * emb_size, emb_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Causal mask prevents attending to future tokens\n",
    "        T = x.size(1)  # temperature\n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
    "        x_norm = self.ln1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm, attn_mask=mask, need_weights=False)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, emb_size, n_layers, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.pos_emb = nn.Embedding(block_size, emb_size)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(emb_size, n_heads, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(emb_size)\n",
    "        self.head = nn.Linear(emb_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        if T > self.block_size:\n",
    "            raise ValueError(\"Sequence length exceeds block size\")\n",
    "\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "        x = self.token_emb(idx) + self.pos_emb(pos)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Cross-entropy on next-token prediction\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "model = GPT(  # model instance\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    emb_size=EMB_SIZE,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout=DROPOUT,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model.parameters()) / 1e6, \"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c49eae",
   "metadata": {},
   "source": [
    "## 4) Training utilities\n",
    "\n",
    "We will:\n",
    "- sample random batches from the token stream\n",
    "- compute evaluation loss + perplexity\n",
    "- build a warmup + cosine learning-rate schedule\n",
    "\n",
    "**Why AdamW?** Weight decay should *not* be part of Adam's adaptive moment estimates. `AdamW` decouples decay from the gradient update, which improves generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64cfa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_data, val_data, eval_iters):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split, data in [(\"train\", train_data), (\"val\", val_data)]:\n",
    "        losses = []\n",
    "        for _ in range(eval_iters):\n",
    "            xb, yb = get_batch(data, BATCH_SIZE, BLOCK_SIZE)\n",
    "            _, loss = model(xb, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = sum(losses) / len(losses)\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    # Randomly sample starting positions\n",
    "    max_start = len(data) - block_size - 1\n",
    "    idx = torch.randint(0, max_start, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in idx])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in idx])\n",
    "    return x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "\n",
    "def warmup_cosine_lr(step, warmup_steps, total_steps):\n",
    "    # Linear warmup, then cosine decay to zero\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))\n",
    "    progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f29e9a",
   "metadata": {},
   "source": [
    "## 5) Training loop (accumulation + warmup/cosine + AMP)\n",
    "\n",
    "Key mechanics:\n",
    "- **Gradient accumulation** divides the loss by `ACCUM_STEPS`, backprops multiple micro-batches, then steps once.\n",
    "- **Warmup + cosine** updates LR each step (not each epoch) for smoother optimization.\n",
    "- **AMP** uses mixed precision on GPU via `autocast` and `GradScaler`.\n",
    "\n",
    "We also print the learning rate occasionally so you can see the schedule in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3fb2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)  # optimizer instance\n",
    "\n",
    "# LR scheduler uses a lambda in [0, 1] multiplied by the base LR\n",
    "lr_lambda = lambda step: warmup_cosine_lr(step, WARMUP_STEPS, MAX_STEPS)  # learning-rate schedule fn\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)  # LR scheduler\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=AMP_ENABLED)  # grad scaler for AMP\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# optimization steps\n",
    "for step in range(1, MAX_STEPS + 1):\n",
    "    # Accumulate gradients over multiple micro-batches\n",
    "    for micro_step in range(ACCUM_STEPS):\n",
    "        xb, yb = get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)\n",
    "        with torch.cuda.amp.autocast(enabled=AMP_ENABLED):\n",
    "            _, loss = model(xb, yb)\n",
    "            loss = loss / ACCUM_STEPS  # scale loss for accumulation\n",
    "\n",
    "        if AMP_ENABLED:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "    # One optimizer step after accumulation\n",
    "    if AMP_ENABLED:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    scheduler.step()\n",
    "\n",
    "    # Periodic evaluation\n",
    "    if step % EVAL_INTERVAL == 0 or step == 1:\n",
    "        losses = estimate_loss(model, train_data, val_data, EVAL_ITERS)\n",
    "        train_ppl = math.exp(losses[\"train\"])\n",
    "        val_ppl = math.exp(losses[\"val\"])\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(\n",
    "            f\"step {step:5d} | lr {lr:.2e} | \"\n",
    "            f\"train loss {losses['train']:.3f} (ppl {train_ppl:.1f}) | \"\n",
    "            f\"val loss {losses['val']:.3f} (ppl {val_ppl:.1f})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333ee389",
   "metadata": {},
   "source": [
    "## 6) Inference: tiny text generation\n",
    "\n",
    "We do a simple greedy generation. For more creative text, use temperature and sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58450f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, prompt, max_new_tokens=50, temperature=1.0):\n",
    "    model.eval()\n",
    "    ids = encode(prompt)\n",
    "    if not ids:\n",
    "        ids = [0]\n",
    "    idx = torch.tensor([ids], dtype=torch.long).to(DEVICE)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -BLOCK_SIZE:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / max(1e-8, temperature)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    text = decode(idx[0].tolist())\n",
    "    return text\n",
    "\n",
    "print(generate(model, 'The meaning of life is'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Scaling notes\n",
    "\n",
    "Real training setups go far beyond this notebook:\n",
    "\n",
    "- **Distributed Data Parallel (DDP)** to split large batches across many GPUs.\n",
    "- **Larger batch sizes** using both data-parallelism and gradient accumulation.\n",
    "- **Mixed precision** plus kernel fusions for faster throughput.\n",
    "- **Activation checkpointing** to save memory.\n",
    "- **Longer schedules** and more aggressive regularization.\n",
    "\n",
    "The mechanics you just saw (warmup, cosine decay, AdamW, accumulation, AMP) are still the core building blocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Exercises\n",
    "\n",
    "1. Increase `BLOCK_SIZE` and compare training loss and generated text quality.\n",
    "2. Turn off AMP and measure training speed on GPU.\n",
    "3. Increase `ACCUM_STEPS` and reduce `BATCH_SIZE`. Does training stay stable?\n",
    "4. Replace the cosine schedule with a constant LR. Compare perplexity.\n",
    "5. Add gradient clipping and see if it changes stability.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}