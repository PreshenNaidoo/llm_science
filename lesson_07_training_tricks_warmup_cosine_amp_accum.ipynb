{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Lesson 07: Training Tricks (Warmup + Cosine + AMP + Accumulation)\n\nThis notebook builds a tiny GPT-like decoder and uses it to show **training-engineering tricks** that matter in real systems:\n\n- **AdamW + weight decay** (why it is different from L2 in Adam)\n- **learning rate warmup + cosine decay** (stability and smooth convergence)\n- **gradient accumulation** (simulate larger batches without more GPU memory)\n- **optional AMP** (mixed precision for speed on GPU)\n\nThe goal is not state-of-the-art performance. The goal is a *clear, runnable training loop* where you can see how each trick changes the mechanics.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Setup\n\nWe set seeds for reproducibility, pick a device, and define configuration knobs. The values are intentionally small so the notebook can run on a laptop.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import math\nimport os\nimport random\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# If torchtext is missing, install it (CPU-only) with:\n# pip install torchtext\nfrom torchtext.datasets import WikiText2\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\n# ---- Reproducibility ----\nSEED = 1337  # random seed\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\n\n# ---- Device ----\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # compute device\nprint(\"Device:\", DEVICE)\n\n# ---- Training config (small, educational) ----\nBATCH_SIZE = 16  # batch size\nBLOCK_SIZE = 128  # context length (tokens)\nEMB_SIZE = 256  # embedding size\nN_LAYERS = 4  # transformer layers\nN_HEADS = 4  # attention heads\nDROPOUT = 0.1  # dropout prob\n\nLEARNING_RATE = 3e-4  # learning rate\nWEIGHT_DECAY = 0.1  # weight decay\n\nACCUM_STEPS = 4          # <-- required by prompt\nWARMUP_STEPS = 200       # <-- required by prompt\nMAX_STEPS = 4000         # <-- required by prompt\n\nEVAL_INTERVAL = 200  # eval interval (steps)\nEVAL_ITERS = 50  # eval batches\n\n# AMP is on by default if CUDA is available\nAMP_ENABLED = torch.cuda.is_available()  # use mixed precision\n\n# ---- (Optional) production-ish knobs you might add later ----\n# GRAD_CLIP = 1.0\n# MIN_LR = 1e-5\n# DDP_WORLD_SIZE = 8\n# GLOBAL_BATCH_SIZE = 1024"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Dataset + tokenizer (WikiText-2)\n\nWe use the `wikitext-2-raw-v1` dataset from `torchtext`. We build a basic English tokenizer and a vocabulary, then concatenate tokens into a 1D tensor.\n\n**Note:** The first run will download the dataset.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# --- Load raw text ---\ntrain_iter, val_iter, test_iter = WikiText2(root=\".data\", split=(\"train\", \"valid\", \"test\"))\n\n# --- Tokenizer ---\ntokenizer = get_tokenizer(\"basic_english\")\n\nSPECIALS = [\"<unk>\", \"<eos>\"]  # special token strings\n\n# Build vocab from training data only\n\ndef yield_tokens(data_iter):\n    for text in data_iter:\n        tokens = tokenizer(text)\n        if tokens:\n            yield tokens + [\"<eos>\"]\n\nvocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=SPECIALS)\nvocab.set_default_index(vocab[\"<unk>\"])\n\n# Need to re-create iterators after they were consumed\ntrain_iter, val_iter, test_iter = WikiText2(root=\".data\", split=(\"train\", \"valid\", \"test\"))\n\ndef data_process(data_iter):\n    ids = []\n    for text in data_iter:\n        tokens = tokenizer(text)\n        if tokens:\n            tokens = tokens + [\"<eos>\"]\n            ids.append(torch.tensor(vocab(tokens), dtype=torch.long))\n    if not ids:\n        raise RuntimeError(\"No data found. Check dataset download.\")\n    return torch.cat(ids)\n\ntrain_data = data_process(train_iter)\nval_data = data_process(val_iter)\n\nVOCAB_SIZE = len(vocab)  # vocab size\nprint(\"Vocab size:\", VOCAB_SIZE)\nprint(\"Train tokens:\", len(train_data))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) GPT-like decoder (simple)\n\nWe implement a tiny, decoder-only Transformer with:\n- token + position embeddings\n- stacked self-attention blocks with causal mask\n- linear head for next-token prediction\n\nThis is intentionally small and minimal so the training loop stays readable.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class Block(nn.Module):\n    def __init__(self, emb_size, n_heads, dropout):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(emb_size)\n        self.attn = nn.MultiheadAttention(\n            embed_dim=emb_size,\n            num_heads=n_heads,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.ln2 = nn.LayerNorm(emb_size)\n        self.mlp = nn.Sequential(\n            nn.Linear(emb_size, 4 * emb_size),\n            nn.GELU(),\n            nn.Linear(4 * emb_size, emb_size),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        # Causal mask prevents attending to future tokens\n        T = x.size(1)  # temperature\n        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n        x_norm = self.ln1(x)\n        attn_out, _ = self.attn(x_norm, x_norm, x_norm, attn_mask=mask, need_weights=False)\n        x = x + attn_out\n        x = x + self.mlp(self.ln2(x))\n        return x\n\n\nclass GPT(nn.Module):\n    def __init__(self, vocab_size, block_size, emb_size, n_layers, n_heads, dropout):\n        super().__init__()\n        self.block_size = block_size\n        self.token_emb = nn.Embedding(vocab_size, emb_size)\n        self.pos_emb = nn.Embedding(block_size, emb_size)\n        self.blocks = nn.ModuleList([\n            Block(emb_size, n_heads, dropout) for _ in range(n_layers)\n        ])\n        self.ln_f = nn.LayerNorm(emb_size)\n        self.head = nn.Linear(emb_size, vocab_size, bias=False)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        if T > self.block_size:\n            raise ValueError(\"Sequence length exceeds block size\")\n\n        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n        x = self.token_emb(idx) + self.pos_emb(pos)\n\n        for block in self.blocks:\n            x = block(x)\n\n        x = self.ln_f(x)\n        logits = self.head(x)\n\n        loss = None\n        if targets is not None:\n            # Cross-entropy on next-token prediction\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n        return logits, loss\n\n\nmodel = GPT(\n    vocab_size=VOCAB_SIZE,\n    block_size=BLOCK_SIZE,\n    emb_size=EMB_SIZE,\n    n_layers=N_LAYERS,\n    n_heads=N_HEADS,\n    dropout=DROPOUT,\n).to(DEVICE)\n\nprint(\"Model parameters:\", sum(p.numel() for p in model.parameters()) / 1e6, \"M\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Training utilities\n\nWe will:\n- sample random batches from the token stream\n- compute evaluation loss + perplexity\n- build a warmup + cosine learning-rate schedule\n\n**Why AdamW?** Weight decay should *not* be part of Adam's adaptive moment estimates. `AdamW` decouples decay from the gradient update, which improves generalization.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "@torch.no_grad()\ndef estimate_loss(model, train_data, val_data, eval_iters):\n    model.eval()\n    out = {}\n    for split, data in [(\"train\", train_data), (\"val\", val_data)]:\n        losses = []\n        for _ in range(eval_iters):\n            xb, yb = get_batch(data, BATCH_SIZE, BLOCK_SIZE)\n            _, loss = model(xb, yb)\n            losses.append(loss.item())\n        out[split] = sum(losses) / len(losses)\n    model.train()\n    return out\n\n\ndef get_batch(data, batch_size, block_size):\n    # Randomly sample starting positions\n    max_start = len(data) - block_size - 1\n    idx = torch.randint(0, max_start, (batch_size,))\n    x = torch.stack([data[i : i + block_size] for i in idx])\n    y = torch.stack([data[i + 1 : i + block_size + 1] for i in idx])\n    return x.to(DEVICE), y.to(DEVICE)\n\n\ndef warmup_cosine_lr(step, warmup_steps, total_steps):\n    # Linear warmup, then cosine decay to zero\n    if step < warmup_steps:\n        return float(step) / float(max(1, warmup_steps))\n    progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n    return 0.5 * (1.0 + math.cos(math.pi * progress))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Training loop (accumulation + warmup/cosine + AMP)\n\nKey mechanics:\n- **Gradient accumulation** divides the loss by `ACCUM_STEPS`, backprops multiple micro-batches, then steps once.\n- **Warmup + cosine** updates LR each step (not each epoch) for smoother optimization.\n- **AMP** uses mixed precision on GPU via `autocast` and `GradScaler`.\n\nWe also print the learning rate occasionally so you can see the schedule in action.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\n# LR scheduler uses a lambda in [0, 1] multiplied by the base LR\nlr_lambda = lambda step: warmup_cosine_lr(step, WARMUP_STEPS, MAX_STEPS)\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n\nscaler = torch.cuda.amp.GradScaler(enabled=AMP_ENABLED)\n\nmodel.train()\noptimizer.zero_grad(set_to_none=True)\n\nfor step in range(1, MAX_STEPS + 1):\n    # Accumulate gradients over multiple micro-batches\n    for micro_step in range(ACCUM_STEPS):\n        xb, yb = get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)\n        with torch.cuda.amp.autocast(enabled=AMP_ENABLED):\n            _, loss = model(xb, yb)\n            loss = loss / ACCUM_STEPS  # scale loss for accumulation\n\n        if AMP_ENABLED:\n            scaler.scale(loss).backward()\n        else:\n            loss.backward()\n\n    # One optimizer step after accumulation\n    if AMP_ENABLED:\n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        optimizer.step()\n\n    optimizer.zero_grad(set_to_none=True)\n    scheduler.step()\n\n    # Periodic evaluation\n    if step % EVAL_INTERVAL == 0 or step == 1:\n        losses = estimate_loss(model, train_data, val_data, EVAL_ITERS)\n        train_ppl = math.exp(losses[\"train\"])\n        val_ppl = math.exp(losses[\"val\"])\n        lr = optimizer.param_groups[0][\"lr\"]\n        print(\n            f\"step {step:5d} | lr {lr:.2e} | \"\n            f\"train loss {losses['train']:.3f} (ppl {train_ppl:.1f}) | \"\n            f\"val loss {losses['val']:.3f} (ppl {val_ppl:.1f})\"\n        )"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Inference: tiny text generation\n\nWe do a simple greedy generation. For more creative text, use temperature and sampling.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "@torch.no_grad()\ndef generate(model, prompt, max_new_tokens=50, temperature=1.0):\n    model.eval()\n    tokens = tokenizer(prompt)\n    if not tokens:\n        tokens = [\"<eos>\"]\n    idx = torch.tensor([vocab(tokens)], dtype=torch.long).to(DEVICE)\n\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -BLOCK_SIZE:]\n        logits, _ = model(idx_cond)\n        logits = logits[:, -1, :] / max(1e-8, temperature)\n        probs = F.softmax(logits, dim=-1)\n        next_id = torch.argmax(probs, dim=-1, keepdim=True)\n        idx = torch.cat([idx, next_id], dim=1)\n\n    out_tokens = vocab.lookup_tokens(idx[0].tolist())\n    text = \" \".join(out_tokens)\n    return text\n\nprint(generate(model, \"The meaning of life is\"))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Scaling notes\n\nReal training setups go far beyond this notebook:\n\n- **Distributed Data Parallel (DDP)** to split large batches across many GPUs.\n- **Larger batch sizes** using both data-parallelism and gradient accumulation.\n- **Mixed precision** plus kernel fusions for faster throughput.\n- **Activation checkpointing** to save memory.\n- **Longer schedules** and more aggressive regularization.\n\nThe mechanics you just saw (warmup, cosine decay, AdamW, accumulation, AMP) are still the core building blocks.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Exercises\n\n1. Increase `BLOCK_SIZE` and compare training loss and generated text quality.\n2. Turn off AMP and measure training speed on GPU.\n3. Increase `ACCUM_STEPS` and reduce `BATCH_SIZE`. Does training stay stable?\n4. Replace the cosine schedule with a constant LR. Compare perplexity.\n5. Add gradient clipping and see if it changes stability.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}