{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 04: Transformer Decoder from Scratch (GPT-style)\n",
    "\n",
    "In this notebook you will build a minimal GPT-style decoder Transformer using PyTorch, step by step.\n",
    "\n",
    "You will learn:\n",
    "- token and learned positional embeddings\n",
    "- causal self-attention and why masking matters\n",
    "- multi-head attention and head mixing\n",
    "- feedforward blocks with residual connections and LayerNorm\n",
    "- how to train and sample from an autoregressive language model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and config\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Top-level configuration (toy defaults)\n",
    "BLOCK_SIZE = 128  # context length (tokens)\n",
    "BATCH_SIZE = 32  # batch size\n",
    "D_MODEL = 256  # model width\n",
    "N_HEADS = 4  # attention heads\n",
    "N_LAYERS = 4  # transformer layers\n",
    "FFN_MULT = 4  # FFN expansion multiplier\n",
    "DROPOUT = 0.1  # dropout prob\n",
    "\n",
    "LR = 3e-4  # learning rate\n",
    "MAX_STEPS = 3000  # training steps\n",
    "EVAL_EVERY = 200  # eval interval (steps)\n",
    "EVAL_ITERS = 50  # eval batches\n",
    "\n",
    "TOKENIZER_VOCAB_SIZE = 2000  # target tokenizer vocab size\n",
    "BPE_TRAIN_CHARS = 200_000  # max chars for BPE training\n",
    "\n",
    "# Larger, slower settings (commented)\n",
    "# BLOCK_SIZE = 1024\n",
    "# BATCH_SIZE = 64\n",
    "# D_MODEL = 768\n",
    "# N_HEADS = 12\n",
    "# N_LAYERS = 12\n",
    "# FFN_MULT = 4\n",
    "# LR = 2e-4\n",
    "# MAX_STEPS = 20000\n",
    "\n",
    "seed = 42  # random seed\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # device for tensors\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset (wikitext-2-raw-v1 preferred)\n",
    "\n",
    "We try Wikitext-2 via Hugging Face `datasets`. If that fails (offline or missing),\n",
    "we fall back to wikitext-2-raw-v1, and finally a local fallback so the notebook still runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def load_text_dataset():\n",
    "    from datasets import load_dataset\n",
    "    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "    name = \"wikitext-2-raw-v1\"\n",
    "    train_text = \"\\n\".join(ds[\"train\"][\"text\"])\n",
    "    val_text = \"\\n\".join(ds[\"validation\"][\"text\"]) if \"validation\" in ds else \"\"\n",
    "    return train_text, val_text, name\n",
    "\n",
    "train_text, val_text, dataset_name = load_text_dataset()\n",
    "print(\"Dataset:\", dataset_name)\n",
    "print(\"Train chars:\", len(train_text), \"Val chars:\", len(val_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer: load existing BPE or train a small one\n",
    "\n",
    "If `./tokenizer_bpe.json` exists, we load it. Otherwise we train a small BPE tokenizer and save it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"<pad>\", \"<unk>\"]  # special token strings\n",
    "TOKENIZER_PATH = \"tokenizer_bpe.json\"  # tokenizer file path\n",
    "\n",
    "\n",
    "def _get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def _merge_vocab(pair, vocab):\n",
    "    bigram = \" \".join(pair)\n",
    "    replacement = \"\".join(pair)\n",
    "    new_vocab = {}\n",
    "    for word, freq in vocab.items():\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = freq\n",
    "    return new_vocab\n",
    "\n",
    "\n",
    "def _build_simple_bpe_helpers(model):\n",
    "    merges = [tuple(p) for p in model[\"merges\"]]\n",
    "    token_to_id = model[\"token_to_id\"]\n",
    "    id_to_token = {i: t for t, i in token_to_id.items()}\n",
    "\n",
    "    def bpe_encode_word(word):\n",
    "        # Start from characters, then apply merges greedily\n",
    "        symbols = list(word) + [\"</w>\"]\n",
    "        for a, b in merges:\n",
    "            i = 0\n",
    "            new_symbols = []\n",
    "            while i < len(symbols):\n",
    "                if i < len(symbols) - 1 and symbols[i] == a and symbols[i + 1] == b:\n",
    "                    new_symbols.append(a + b)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "            symbols = new_symbols\n",
    "        return symbols\n",
    "\n",
    "    def encode_fn(text_in):\n",
    "        ids = []\n",
    "        for w in text_in.split():\n",
    "            for sym in bpe_encode_word(w):\n",
    "                ids.append(token_to_id.get(sym, token_to_id[\"<unk>\"]))\n",
    "        return ids\n",
    "\n",
    "    def decode_fn(ids_in):\n",
    "        tokens = [id_to_token[i] for i in ids_in]\n",
    "        text_out = \"\".join([\" \" if t == \"</w>\" else t for t in tokens])\n",
    "        return \" \".join(text_out.split())\n",
    "\n",
    "    return encode_fn, decode_fn\n",
    "\n",
    "\n",
    "def _train_simple_bpe(text, vocab_size=2000, max_merges=200):\n",
    "    words = [w for w in text.split() if w]\n",
    "    vocab = defaultdict(int)\n",
    "    for w in words:\n",
    "        vocab[\" \".join(list(w)) + \" </w>\"] += 1\n",
    "\n",
    "    symbols = set()\n",
    "    for word in vocab:\n",
    "        symbols.update(word.split())\n",
    "    base_vocab = len(symbols) + len(SPECIAL_TOKENS)\n",
    "    num_merges = max(0, min(max_merges, vocab_size - base_vocab))\n",
    "\n",
    "    merges = []\n",
    "    for _ in range(num_merges):\n",
    "        pairs = _get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = _merge_vocab(best, vocab)\n",
    "        merges.append(best)\n",
    "\n",
    "    symbols = set()\n",
    "    for word in vocab:\n",
    "        symbols.update(word.split())\n",
    "\n",
    "    tokens = list(SPECIAL_TOKENS) + sorted(symbols)\n",
    "    token_to_id = {t: i for i, t in enumerate(tokens)}\n",
    "\n",
    "    model = {\n",
    "        \"merges\": merges,\n",
    "        \"token_to_id\": token_to_id,\n",
    "    }\n",
    "\n",
    "    encode_fn, decode_fn = _build_simple_bpe_helpers(model)\n",
    "    pad_id = token_to_id[\"<pad>\"]\n",
    "    return model, encode_fn, decode_fn, len(tokens), pad_id\n",
    "\n",
    "\n",
    "def load_tokenizer(path):\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    # Try Hugging Face tokenizers first\n",
    "    try:\n",
    "        from tokenizers import Tokenizer\n",
    "\n",
    "        tokenizer = Tokenizer.from_file(path)\n",
    "\n",
    "        def encode_fn(s):\n",
    "            return tokenizer.encode(s).ids\n",
    "\n",
    "        def decode_fn(ids):\n",
    "            return tokenizer.decode(ids)\n",
    "\n",
    "        vocab_size = len(tokenizer.get_vocab())\n",
    "        pad_id = tokenizer.token_to_id(\"<pad>\")\n",
    "        if pad_id is None:\n",
    "            pad_id = 0\n",
    "        return {\n",
    "            \"type\": \"tokenizers\",\n",
    "            \"encode\": encode_fn,\n",
    "            \"decode\": decode_fn,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"pad_id\": pad_id,\n",
    "        }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback to simple BPE json\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            model = json.load(f)\n",
    "        if \"merges\" in model and \"token_to_id\" in model:\n",
    "            encode_fn, decode_fn = _build_simple_bpe_helpers(model)\n",
    "            pad_id = model[\"token_to_id\"].get(\"<pad>\", 0)\n",
    "            return {\n",
    "                \"type\": \"simple_bpe\",\n",
    "                \"encode\": encode_fn,\n",
    "                \"decode\": decode_fn,\n",
    "                \"vocab_size\": len(model[\"token_to_id\"]),\n",
    "                \"pad_id\": pad_id,\n",
    "            }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def train_tokenizer(text, vocab_size=2000, save_path=\"tokenizer_bpe.json\"):\n",
    "    text = text[:BPE_TRAIN_CHARS]\n",
    "    try:\n",
    "        from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "\n",
    "        tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "        tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC()])\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "        trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=SPECIAL_TOKENS)\n",
    "\n",
    "        tokenizer.train_from_iterator([text], trainer=trainer)\n",
    "        tokenizer.save(save_path)\n",
    "\n",
    "        def encode_fn(s):\n",
    "            return tokenizer.encode(s).ids\n",
    "\n",
    "        def decode_fn(ids):\n",
    "            return tokenizer.decode(ids)\n",
    "\n",
    "        vocab_size_out = len(tokenizer.get_vocab())\n",
    "        pad_id = tokenizer.token_to_id(\"<pad>\")\n",
    "        if pad_id is None:\n",
    "            pad_id = 0\n",
    "        return {\n",
    "            \"type\": \"tokenizers\",\n",
    "            \"encode\": encode_fn,\n",
    "            \"decode\": decode_fn,\n",
    "            \"vocab_size\": vocab_size_out,\n",
    "            \"pad_id\": pad_id,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"tokenizers not available or failed. Using simple BPE fallback.\")\n",
    "        print(\"Reason:\", repr(e))\n",
    "        model, encode_fn, decode_fn, vocab_size_out, pad_id = _train_simple_bpe(\n",
    "            text,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(model, f)\n",
    "        return {\n",
    "            \"type\": \"simple_bpe\",\n",
    "            \"encode\": encode_fn,\n",
    "            \"decode\": decode_fn,\n",
    "            \"vocab_size\": vocab_size_out,\n",
    "            \"pad_id\": pad_id,\n",
    "        }\n",
    "\n",
    "\n",
    "tok = load_tokenizer(TOKENIZER_PATH)  # tokenizer handle\n",
    "if tok is None:\n",
    "    print(\"No tokenizer found. Training a small BPE tokenizer...\")\n",
    "    tok = train_tokenizer(train_text + \"\\n\" + val_text, vocab_size=TOKENIZER_VOCAB_SIZE)\n",
    "else:\n",
    "    print(\"Loaded tokenizer from\", TOKENIZER_PATH)\n",
    "\n",
    "encode = tok[\"encode\"]  # text-to-ids function\n",
    "decode = tok[\"decode\"]  # ids-to-text function\n",
    "\n",
    "print(\"Tokenizer type:\", tok[\"type\"])\n",
    "print(\"Vocab size:\", tok[\"vocab_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalize the dataset\n",
    "\n",
    "We convert text to token ids once, then sample contiguous blocks for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = torch.tensor(encode(train_text), dtype=torch.long)  # token ids for training split\n",
    "val_ids = torch.tensor(encode(val_text), dtype=torch.long)  # token ids for validation split\n",
    "\n",
    "print(\"Train tokens:\", train_ids.numel())\n",
    "print(\"Val tokens:\", val_ids.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch sampling for autoregressive language modeling\n",
    "\n",
    "We sample random contiguous windows of length `BLOCK_SIZE` and predict the next token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_ids if split == \"train\" else val_ids\n",
    "    # Random starting indices for each sequence in the batch\n",
    "    idx = torch.randint(0, len(data) - BLOCK_SIZE - 1, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i : i + BLOCK_SIZE] for i in idx])\n",
    "    y = torch.stack([data[i + 1 : i + BLOCK_SIZE + 1] for i in idx])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm intuition\n",
    "\n",
    "LayerNorm normalizes each token's feature vector (across channels), which stabilizes training\n",
    "when stacked with residual connections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, n_embd, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(n_embd))\n",
    "        self.bias = nn.Parameter(torch.zeros(n_embd))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.weight * x_hat + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention: Q, K, V, masking, and weighted sum\n",
    "\n",
    "Each token projects to queries, keys, and values. We compute attention scores between tokens,\n",
    "mask future positions (causal mask), then take a weighted sum of values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, d_model, head_dim, block_size, dropout, print_shapes=False):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d_model, head_dim, bias=False)\n",
    "        self.query = nn.Linear(d_model, head_dim, bias=False)\n",
    "        self.value = nn.Linear(d_model, head_dim, bias=False)\n",
    "        # Causal mask keeps attention from looking ahead\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.print_shapes = print_shapes\n",
    "        self._printed = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # Attention scores (B, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n",
    "        att = att.masked_fill(self.mask[:T, :T] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        if self.print_shapes and not self._printed:\n",
    "            print(\"B,T,C:\", x.shape)\n",
    "            print(\"Attention weights shape:\", att.shape)\n",
    "            self._printed = True\n",
    "\n",
    "        # Weighted sum of values\n",
    "        out = att @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention\n",
    "\n",
    "Multiple heads attend to different subspaces. We concatenate their outputs and project back.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, block_size, dropout, print_shapes=False):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        head_dim = d_model // n_heads\n",
    "        heads = []\n",
    "        for i in range(n_heads):\n",
    "            heads.append(\n",
    "                SelfAttentionHead(\n",
    "                    d_model,\n",
    "                    head_dim,\n",
    "                    block_size,\n",
    "                    dropout,\n",
    "                    print_shapes=print_shapes and i == 0,\n",
    "                )\n",
    "            )\n",
    "        self.heads = nn.ModuleList(heads)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward block (MLP)\n",
    "\n",
    "After attention mixes information across tokens, the MLP mixes information within each token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, ffn_mult=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        hidden = ffn_mult * d_model\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer block\n",
    "\n",
    "We use pre-norm: normalize before attention and MLP. Residual connections help gradients flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, block_size, ffn_mult, dropout, print_shapes=False):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(d_model)\n",
    "        self.ln2 = LayerNorm(d_model)\n",
    "        self.attn = MultiHeadSelfAttention(\n",
    "            d_model,\n",
    "            n_heads,\n",
    "            block_size,\n",
    "            dropout,\n",
    "            print_shapes=print_shapes,\n",
    "        )\n",
    "        self.ffn = FeedForward(d_model, ffn_mult, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full GPT-style decoder model\n",
    "\n",
    "We combine token embeddings, learned positional embeddings, stacked blocks, and a final LM head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        block_size,\n",
    "        d_model,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        ffn_mult,\n",
    "        dropout,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(block_size, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        blocks = []\n",
    "        for i in range(n_layers):\n",
    "            blocks.append(\n",
    "                TransformerBlock(\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    block_size,\n",
    "                    ffn_mult,\n",
    "                    dropout,\n",
    "                    print_shapes=(i == 0),\n",
    "                )\n",
    "            )\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.ln_f = LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        if T > self.block_size:\n",
    "            raise ValueError(\"Sequence length exceeds block size\")\n",
    "\n",
    "        tok = self.token_emb(idx)\n",
    "        pos = self.pos_emb(torch.arange(T, device=idx.device))\n",
    "        x = tok + pos  # broadcast over batch\n",
    "        x = self.drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            logits_flat = logits.view(-1, logits.size(-1))\n",
    "            targets_flat = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        was_training = self.training\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size :]  # crop to model context window\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "\n",
    "            if top_k is not None:\n",
    "                k = min(top_k, logits.size(-1))\n",
    "                v, _ = torch.topk(logits, k)\n",
    "                cutoff = v[:, [-1]]\n",
    "                logits = logits.masked_fill(logits < cutoff, float(\"-inf\"))\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "        if was_training:\n",
    "            self.train()\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "We use AdamW, dropout (already in the model), and evaluate perplexity periodically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tok[\"vocab_size\"]  # vocabulary size\n",
    "model = GPT(  # model instance\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    n_layers=N_LAYERS,\n",
    "    ffn_mult=FFN_MULT,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "# Parameter count is a quick sanity check on model size\n",
    "param_count = sum(p.numel() for p in model.parameters())  # parameter count\n",
    "print(f\"Model parameters: {param_count/1e6:.2f}M\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)  # optimizer instance\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(EVAL_ITERS):\n",
    "            xb, yb = get_batch(split)\n",
    "            _, loss = model(xb, yb)\n",
    "            losses.append(loss.item())\n",
    "        avg = sum(losses) / len(losses)\n",
    "        # Perplexity is exp(cross-entropy)\n",
    "        out[split] = {\"loss\": avg, \"ppl\": math.exp(avg)}\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "print(\"Initial eval:\")\n",
    "metrics = estimate_loss()  # evaluation metrics dict\n",
    "print(\n",
    "    f\"train loss {metrics['train']['loss']:.3f}, train ppl {metrics['train']['ppl']:.2f} | \"\n",
    "    f\"val loss {metrics['val']['loss']:.3f}, val ppl {metrics['val']['ppl']:.2f}\"\n",
    ")\n",
    "\n",
    "# optimization steps\n",
    "for step in range(1, MAX_STEPS + 1):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    # Forward pass returns logits and loss\n",
    "    _, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Backprop + parameter update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % EVAL_EVERY == 0:\n",
    "        metrics = estimate_loss()\n",
    "        print(\n",
    "            f\"step {step}: \"\n",
    "            f\"train loss {metrics['train']['loss']:.3f}, train ppl {metrics['train']['ppl']:.2f} | \"\n",
    "            f\"val loss {metrics['val']['loss']:.3f}, val ppl {metrics['val']['ppl']:.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: generate text\n",
    "\n",
    "We sample from the trained model using temperature and top-k sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [  # list of prompts\n",
    "    \"The meaning of life is\",\n",
    "    \"Once upon a time\",\n",
    "    \"In the middle of the night\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    idx = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "    out = model.generate(idx, max_new_tokens=80, temperature=0.9, top_k=40)\n",
    "    print(\"PROMPT:\", prompt)\n",
    "    print(decode(out[0].tolist()))\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling notes\n",
    "\n",
    "Toy settings (this notebook):\n",
    "- D_MODEL=256, N_LAYERS=4, N_HEADS=4, BLOCK_SIZE=128, BATCH_SIZE=32\n",
    "\n",
    "Production-ish example (much larger):\n",
    "- D_MODEL=768 or 1024, N_LAYERS=12 to 24, N_HEADS=12 to 16, BLOCK_SIZE=1024 or 2048\n",
    "\n",
    "Why bigger models need more data and compute:\n",
    "- Attention is O(T^2) in sequence length, so longer context is expensive\n",
    "- More parameters increase capacity, which needs more training tokens to avoid overfitting\n",
    "- Larger models often require longer training and higher batch sizes to converge well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Increase `BLOCK_SIZE` to 256 and compare validation perplexity.\n",
    "2. Try `N_LAYERS=6` and `D_MODEL=384` and see how training speed changes.\n",
    "3. Add gradient clipping and see if it stabilizes training at higher learning rates.\n",
    "4. Compare outputs with `temperature=0.7` vs `temperature=1.2`.\n",
    "5. Implement tied embeddings (share token embedding and LM head weights).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}