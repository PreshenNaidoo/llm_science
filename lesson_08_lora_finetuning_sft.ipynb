{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 08: LoRA Fine-Tuning (SFT)\n",
        "\n",
        "Supervised fine-tuning (SFT) starts from a **pretrained** language model and nudges it toward following instructions. Instead of training from scratch, we adapt an existing model using a dataset of instruction-response pairs.\n",
        "\n",
        "**Why LoRA helps**: LoRA (Low-Rank Adapters) freezes the base model and trains small, low-rank matrices injected into attention and MLP layers. That means faster training, less memory use, and good performance for many tasks.\n",
        "\n",
        "In this notebook we will:\n",
        "- Load a small instruction dataset.\n",
        "- Format it into a prompt template.\n",
        "- Attach LoRA adapters to GPT-2.\n",
        "- Fine-tune with a simple Trainer loop.\n",
        "- Run inference with sampling controls.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Imports and basic setup\nimport math\nimport os\nimport random\n\nimport torch\nfrom datasets import Dataset, load_dataset\nfrom transformers import (\n    DataCollatorForLanguageModeling,\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n    Trainer,\n    TrainingArguments,\n)\n\ntry:\n    from peft import LoraConfig, TaskType, get_peft_model\nexcept ImportError:\n    print(\"peft is required. Install it with: pip install peft\")\n    raise"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Reproducibility and device selection\ndef set_seed(seed: int = 42) -> None:\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load a small instruction dataset\n",
        "\n",
        "We will try to load a lightweight instruction dataset from Hugging Face (Alpaca). If that fails (for example, no internet), we will fall back to a tiny synthetic dataset so the notebook still runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "dataset_name = \"tatsu-lab/alpaca\"\n\ntry:\n    raw = load_dataset(dataset_name)\n    train_data = raw[\"train\"]\n    print(f\"Loaded dataset: {dataset_name} ({len(train_data)} examples)\")\nexcept Exception as exc:\n    print(\"Dataset load failed, using a tiny synthetic dataset instead.\")\n    print(\"Reason:\", exc)\n    synthetic = [\n        {\n            \"instruction\": \"Explain what a variable is in Python.\",\n            \"response\": \"A variable is a named reference to a value. You can assign a value using = and reuse the name later.\",\n        },\n        {\n            \"instruction\": \"Give two tips for learning PyTorch.\",\n            \"response\": \"Start with small tensor exercises and read the official tutorials. Practice writing simple training loops.\",\n        },\n        {\n            \"instruction\": \"Write a short poem about rain.\",\n            \"response\": \"Rain taps soft rhythms on the roof, a silver hush above the street.\",\n        },\n        {\n            \"instruction\": \"Summarize the water cycle in one sentence.\",\n            \"response\": \"Water evaporates, condenses into clouds, then falls as precipitation and collects again.\",\n        },\n    ]\n    train_data = Dataset.from_list(synthetic)\n\n\ndef format_example(example):\n    instruction = example.get(\"instruction\", \"\")\n    input_text = example.get(\"input\", \"\")\n    response = example.get(\"output\", example.get(\"response\", \"\"))\n\n    if input_text:\n        instruction = instruction + \"\\n\\n### Input:\\n\" + input_text\n\n    text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n    return {\"text\": text}\n\n\nsubset_size = min(2000, len(train_data))\ntrain_subset = train_data.select(range(subset_size))\nformatted = train_subset.map(format_example, remove_columns=train_subset.column_names)\n\nprint(\"Using subset size:\", len(formatted))\nprint(\"Sample prompt:\\n\", formatted[0][\"text\"][:500])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Tokenization\n",
        "\n",
        "We use the GPT-2 tokenizer and turn each prompt into token IDs. For causal language modeling, labels are the same as input IDs (the model learns to predict the next token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n\ndef tokenize_function(batch):\n    tokens = tokenizer(\n        batch[\"text\"],\n        truncation=True,\n        max_length=256,\n    )\n    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n    return tokens\n\n\ntokenized = formatted.map(tokenize_function, batched=True, remove_columns=[\"text\"])\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\nprint(\"Tokenized example length:\", len(tokenized[0][\"input_ids\"]))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Load a pretrained model (GPT-2)\n",
        "\n",
        "We start from a pretrained GPT-2 model. This is the core idea of modern practice: we reuse a strong base model and adapt it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\nmodel.config.pad_token_id = tokenizer.pad_token_id\nprint(\"Base model loaded.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Attach LoRA adapters\n",
        "\n",
        "We inject LoRA adapters into key GPT-2 layers. For GPT-2, the attention and MLP use linear projections named:\n",
        "- `c_attn` and `c_proj` in attention\n",
        "- `c_fc` and `c_proj` in the MLP\n",
        "\n",
        "LoRA will train small low-rank matrices for those layers while keeping the original weights frozen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "lora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\nmodel.to(device)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Training\n",
        "\n",
        "We use `Trainer` to handle the PyTorch training loop. It wraps:\n",
        "- Forward pass (compute model outputs)\n",
        "- Loss computation\n",
        "- Backpropagation and optimizer step\n",
        "- Logging of training loss\n",
        "\n",
        "This keeps the code short and readable while still using real PyTorch under the hood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "training_args = TrainingArguments(\n    output_dir=\"lora-gpt2-sft\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    logging_steps=20,\n    save_strategy=\"no\",\n    report_to=[],\n    fp16=torch.cuda.is_available(),\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized,\n    data_collator=data_collator,\n)\n\ntrainer.train()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Inference tests\n",
        "\n",
        "We run a few prompts and adjust sampling controls:\n",
        "- **temperature**: higher = more randomness\n",
        "- **top_p**: nucleus sampling (probability mass cutoff)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def generate_text(prompt, temperature=0.7, top_p=0.9, max_new_tokens=80):\n    model.eval()\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    # no_grad disables gradient tracking during inference\n    with torch.no_grad():\n        output_ids = model.generate(\n            **inputs,\n            do_sample=True,\n            temperature=temperature,\n            top_p=top_p,\n            max_new_tokens=max_new_tokens,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n\ntest_prompts = [\n    \"### Instruction:\\nGive three tips for focusing while studying.\\n\\n### Response:\\n\",\n    \"### Instruction:\\nExplain what backpropagation is in one sentence.\\n\\n### Response:\\n\",\n    \"### Instruction:\\nWrite a friendly one-line greeting.\\n\\n### Response:\\n\",\n]\n\nfor prompt in test_prompts:\n    print(\"-\" * 60)\n    print(generate_text(prompt, temperature=0.8, top_p=0.9))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Scaling notes and practical knobs\n",
        "\n",
        "Real-world SFT uses much larger base models and much more data. LoRA makes this feasible by training a small number of parameters.\n",
        "\n",
        "**Toy knobs (this notebook)**\n",
        "- Base model: `gpt2`\n",
        "- LoRA rank `r`: 4-16\n",
        "- Dataset size: 2k examples\n",
        "- Epochs: 1\n",
        "\n",
        "**Production-ish knobs**\n",
        "- Base model: 7B+ parameters\n",
        "- LoRA rank `r`: 8-64 (task-dependent)\n",
        "- Dataset size: tens or hundreds of thousands of examples\n",
        "- Careful evaluation and safety filtering\n",
        "\n",
        "Larger models and datasets improve capability, but training cost scales quickly. LoRA lets you trade off quality vs. speed and memory by adjusting rank and target modules."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}