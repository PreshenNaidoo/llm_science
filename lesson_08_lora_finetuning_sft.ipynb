{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 08: LoRA Fine-Tuning (SFT)\n",
    "\n",
    "Supervised fine-tuning (SFT) starts from a **pretrained** language model and nudges it toward following instructions. Instead of training from scratch, we adapt an existing model using a dataset of instruction-response pairs.\n",
    "\n",
    "**Why LoRA helps**: LoRA (Low-Rank Adapters) freezes the base model and trains small, low-rank matrices injected into attention and MLP layers. That means faster training, less memory use, and good performance for many tasks.\n",
    "\n",
    "In this notebook we will:\n",
    "- Load a small instruction dataset.\n",
    "- Format it into a prompt template.\n",
    "- Attach LoRA adapters to GPT-2.\n",
    "- Fine-tune with a simple Trainer loop.\n",
    "- Run inference with sampling controls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and basic setup\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    DataCollatorForLanguageModeling,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "try:\n",
    "    from peft import LoraConfig, TaskType, get_peft_model\n",
    "except ImportError:\n",
    "    print(\"peft is required. Install it with: pip install peft\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b575555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility and device selection\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # device for tensors\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21305a12",
   "metadata": {},
   "source": [
    "## 1) Load a small instruction dataset\n",
    "\n",
    "We will try to load a lightweight instruction dataset from Hugging Face (Alpaca). If that fails (for example, no internet), we will fall back to a tiny synthetic dataset so the notebook still runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"tatsu-lab/alpaca\"  # dataset name\n",
    "\n",
    "try:\n",
    "    raw = load_dataset(dataset_name)\n",
    "    train_data = raw[\"train\"]\n",
    "    print(f\"Loaded dataset: {dataset_name} ({len(train_data)} examples)\")\n",
    "except Exception as exc:\n",
    "    print(\"Dataset load failed, using a tiny synthetic dataset instead.\")\n",
    "    print(\"Reason:\", exc)\n",
    "    synthetic = [\n",
    "        {\n",
    "            \"instruction\": \"Explain what a variable is in Python.\",\n",
    "            \"response\": \"A variable is a named reference to a value. You can assign a value using = and reuse the name later.\",\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Give two tips for learning PyTorch.\",\n",
    "            \"response\": \"Start with small tensor exercises and read the official tutorials. Practice writing simple training loops.\",\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Write a short poem about rain.\",\n",
    "            \"response\": \"Rain taps soft rhythms on the roof, a silver hush above the street.\",\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Summarize the water cycle in one sentence.\",\n",
    "            \"response\": \"Water evaporates, condenses into clouds, then falls as precipitation and collects again.\",\n",
    "        },\n",
    "    ]\n",
    "    train_data = Dataset.from_list(synthetic)\n",
    "\n",
    "\n",
    "def format_example(example):\n",
    "    instruction = example.get(\"instruction\", \"\")\n",
    "    input_text = example.get(\"input\", \"\")\n",
    "    response = example.get(\"output\", example.get(\"response\", \"\"))\n",
    "\n",
    "    if input_text:\n",
    "        instruction = instruction + \"\\n\\n### Input:\\n\" + input_text\n",
    "\n",
    "    text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "subset_size = min(2000, len(train_data))  # subset size\n",
    "train_subset = train_data.select(range(subset_size))  # training subset\n",
    "formatted = train_subset.map(format_example, remove_columns=train_subset.column_names)  # formatted training examples\n",
    "\n",
    "print(\"Using subset size:\", len(formatted))\n",
    "print(\"Sample prompt:\\n\", formatted[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbbb3e",
   "metadata": {},
   "source": [
    "## 2) Tokenization\n",
    "\n",
    "We use the GPT-2 tokenizer and turn each prompt into token IDs. For causal language modeling, labels are the same as input IDs (the model learns to predict the next token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696467e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  # tokenizer instance\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokens = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized = formatted.map(tokenize_function, batched=True, remove_columns=[\"text\"])  # tokenized dataset\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)  # HF data collator\n",
    "\n",
    "print(\"Tokenized example length:\", len(tokenized[0][\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b9725",
   "metadata": {},
   "source": [
    "## 3) Load a pretrained model (GPT-2)\n",
    "\n",
    "We start from a pretrained GPT-2 model. This is the core idea of modern practice: we reuse a strong base model and adapt it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc312eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")  # model instance\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "print(\"Base model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07ccaa8",
   "metadata": {},
   "source": [
    "## 4) Attach LoRA adapters\n",
    "\n",
    "We inject LoRA adapters into key GPT-2 layers. For GPT-2, the attention and MLP use linear projections named:\n",
    "- `c_attn` and `c_proj` in attention\n",
    "- `c_fc` and `c_proj` in the MLP\n",
    "\n",
    "LoRA will train small low-rank matrices for those layers while keeping the original weights frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8408a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(  # LoRA config\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)  # model instance\n",
    "model.print_trainable_parameters()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad43a56",
   "metadata": {},
   "source": [
    "## 5) Training\n",
    "\n",
    "We use `Trainer` to handle the PyTorch training loop. It wraps:\n",
    "- Forward pass (compute model outputs)\n",
    "- Loss computation\n",
    "- Backpropagation and optimizer step\n",
    "- Logging of training loss\n",
    "\n",
    "This keeps the code short and readable while still using real PyTorch under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a401524",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(  # HF training args\n",
    "    output_dir=\"lora-gpt2-sft\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[],\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(  # trainer instance\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Inference tests\n",
    "\n",
    "We run a few prompts and adjust sampling controls:\n",
    "- **temperature**: higher = more randomness\n",
    "- **top_p**: nucleus sampling (probability mass cutoff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, temperature=0.7, top_p=0.9, max_new_tokens=80):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # no_grad disables gradient tracking during inference\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "test_prompts = [  # prompts for evaluation\n",
    "    \"### Instruction:\\nGive three tips for focusing while studying.\\n\\n### Response:\\n\",\n",
    "    \"### Instruction:\\nExplain what backpropagation is in one sentence.\\n\\n### Response:\\n\",\n",
    "    \"### Instruction:\\nWrite a friendly one-line greeting.\\n\\n### Response:\\n\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(\"-\" * 60)\n",
    "    print(generate_text(prompt, temperature=0.8, top_p=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Scaling notes and practical knobs\n",
    "\n",
    "Real-world SFT uses much larger base models and much more data. LoRA makes this feasible by training a small number of parameters.\n",
    "\n",
    "**Toy knobs (this notebook)**\n",
    "- Base model: `gpt2`\n",
    "- LoRA rank `r`: 4-16\n",
    "- Dataset size: 2k examples\n",
    "- Epochs: 1\n",
    "\n",
    "**Production-ish knobs**\n",
    "- Base model: 7B+ parameters\n",
    "- LoRA rank `r`: 8-64 (task-dependent)\n",
    "- Dataset size: tens or hundreds of thousands of examples\n",
    "- Careful evaluation and safety filtering\n",
    "\n",
    "Larger models and datasets improve capability, but training cost scales quickly. LoRA lets you trade off quality vs. speed and memory by adjusting rank and target modules."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}