{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 09: Quantization for Inference\n",
        "\n",
        "Quantization is a way to store and run neural networks with fewer bits. The goal is to **trade a small amount of accuracy for much better speed and memory usage**.\n",
        "\n",
        "**Common formats:**\n",
        "- **FP32**: 32-bit floating point (baseline, largest memory).\n",
        "- **FP16 / BF16**: 16-bit floating point (half memory, often faster on GPUs).\n",
        "- **INT8**: 8-bit integers (much smaller, often faster on CPUs).\n",
        "- **INT4**: 4-bit integers (very small, needs specialized kernels, often used on GPUs).\n",
        "\n",
        "In practice, quantization can reduce:\n",
        "- **Model size** (parameters * bytes)\n",
        "- **Memory bandwidth** (less data moved)\n",
        "- **Latency** (especially on CPU)\n",
        "\n",
        "But it can also cause **accuracy drops** or weird output, especially for smaller models or more aggressive quantization.\n",
        "\n",
        "In this notebook, we will use **PyTorch dynamic quantization** for a reliable CPU demo. Dynamic quantization is simple: it converts `nn.Linear` layers to INT8 and does the quantization at runtime (activations are quantized on the fly).\n",
        "\n",
        "We will also briefly mention GPU 4-bit quantization (bitsandbytes), but keep it optional.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Setup + imports\nimport time\nimport os\nfrom typing import Tuple\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Helpers for timing and size estimates\ndef estimate_model_size_bytes(model: torch.nn.Module, dtype_bytes: int) -> int:\n    # Rough estimate: number of parameters * bytes per parameter\n    num_params = sum(p.numel() for p in model.parameters())\n    return num_params * dtype_bytes\n\ndef format_size(num_bytes: int) -> str:\n    # Human-readable size\n    for unit in [\"B\", \"KB\", \"MB\", \"GB\"]:\n        if num_bytes < 1024:\n            return f\"{num_bytes:.2f} {unit}\"\n        num_bytes /= 1024\n    return f\"{num_bytes:.2f} TB\"\n\ndef timed_generate(model, tokenizer, prompt: str, max_new_tokens: int, device: str) -> Tuple[str, float]:\n    # Time text generation. This measures end-to-end latency for a single prompt.\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    start = time.time()\n    with torch.no_grad():\n        output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n    end = time.time()\n    text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return text, end - start\n\n# Use CPU by default, but allow CUDA if available for fp16/fp32 comparisons\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Load tokenizer + model\n# We use a small pretrained model for a quick demo.\nmodel_name = \"gpt2\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Move to GPU if available for baseline fp16/fp32.\nmodel = model.to(device)\nmodel.eval()\n\nprint(\"Model loaded\")"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Baseline inference test (FP32 or FP16)\nprompt = \"Explain quantization in simple terms.\"\nmax_new_tokens = 40\n\n# If on CUDA, we can optionally use fp16 to show speed/memory tradeoff.\nif device == \"cuda\":\n    model_fp16 = model.half()\n    text_fp16, t_fp16 = timed_generate(model_fp16, tokenizer, prompt, max_new_tokens, device)\n    size_fp16 = estimate_model_size_bytes(model_fp16, dtype_bytes=2)\n    print(\"--- FP16 (GPU) ---\")\n    print(text_fp16)\n    print(f\"Latency: {t_fp16:.3f}s\")\n    print(f\"Size (est): {format_size(size_fp16)}\")\n\n# Always run fp32 baseline (CPU or GPU)\nmodel_fp32 = model.float()\ntext_fp32, t_fp32 = timed_generate(model_fp32, tokenizer, prompt, max_new_tokens, device)\nsize_fp32 = estimate_model_size_bytes(model_fp32, dtype_bytes=4)\nprint(\"--- FP32 ---\")\nprint(text_fp32)\nprint(f\"Latency: {t_fp32:.3f}s\")\nprint(f\"Size (est): {format_size(size_fp32)}\")"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Dynamic quantization on CPU\n# This is the most reliable built-in quantization demo in PyTorch.\n# It replaces nn.Linear with an int8-quantized version.\n\n# Ensure we are on CPU for quantized inference\ncpu_device = \"cpu\"\nmodel_cpu = AutoModelForCausalLM.from_pretrained(model_name)\nmodel_cpu.eval()\n\n# Apply dynamic quantization to Linear layers\nquantized_model = torch.quantization.quantize_dynamic(\n    model_cpu,\n    {torch.nn.Linear},\n    dtype=torch.qint8,\n)\n\n# Run inference and time it\ntext_int8, t_int8 = timed_generate(quantized_model, tokenizer, prompt, max_new_tokens, cpu_device)\nsize_int8 = estimate_model_size_bytes(quantized_model, dtype_bytes=1)\n\nprint(\"--- INT8 (dynamic, CPU) ---\")\nprint(text_int8)\nprint(f\"Latency: {t_int8:.3f}s\")\nprint(f\"Size (est): {format_size(size_int8)}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: What changed?\n",
        "\n",
        "Dynamic quantization only changes **Linear layers** and only on **CPU**. This is why it is so reliable for a teaching demo: it uses mature CPU kernels in PyTorch.\n",
        "\n",
        "Typical results (your numbers will vary):\n",
        "- **Model size** decreases roughly 4x (FP32 -> INT8).\n",
        "- **Latency** can improve on CPU due to cheaper math and less memory traffic.\n",
        "- **Output quality** usually stays close to FP32 for small prompts, but can degrade on longer generations.\n",
        "\n",
        "### Why CPU quantization is easier in vanilla PyTorch\n",
        "- PyTorch ships optimized CPU int8 kernels (like FBGEMM).\n",
        "- GPU quantization needs specialized libraries and kernels.\n",
        "- GPU 4-bit often relies on external packages (e.g., bitsandbytes).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: GPU 4-bit notes (bitsandbytes)\n",
        "\n",
        "If you want 4-bit quantization on GPU, you typically use:\n",
        "- **bitsandbytes** with Hugging Face Transformers\n",
        "- `load_in_4bit=True` with a suitable quantization config\n",
        "\n",
        "This is powerful but depends on CUDA versions and specific GPU support.\n",
        "It is optional because it adds install complexity and can be fragile across systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scaling notes (production)\n",
        "\n",
        "Real systems often go beyond simple post-training quantization:\n",
        "- **Quantization-aware training (QAT)** trains the model to tolerate low precision.\n",
        "- **Specialized kernels** (e.g., fused attention / matmul) are required for speed.\n",
        "- **Serving stacks** (like TensorRT, ONNX Runtime, or custom CUDA kernels) often give the biggest gains.\n",
        "\n",
        "The key idea: quantization alone helps, but performance depends heavily on the runtime.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1) Try a different prompt and compare outputs between FP32 and INT8.\n",
        "2) Increase `max_new_tokens` and see if accuracy differences grow.\n",
        "3) Measure average latency over 5 runs (and compute the mean).\n",
        "4) Try a smaller model (e.g., `distilgpt2`) and compare speed/quality.\n",
        "5) Research one production quantization method and summarize it.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}