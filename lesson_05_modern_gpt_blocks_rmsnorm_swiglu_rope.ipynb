{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 05: Modern GPT Blocks (RMSNorm, SwiGLU, RoPE)\n",
    "\n",
    "In this lesson we build a decoder-only Transformer that is closer to modern LLMs:\n",
    "\n",
    "- RMSNorm instead of LayerNorm\n",
    "- SwiGLU feedforward instead of GELU\n",
    "- Rotary positional embeddings (RoPE) instead of learned absolute positions\n",
    "\n",
    "We train on wikitext-2-raw-v1 (with a small fallback), and compare to Lesson 04.\n",
    "\n",
    "Comparison to Lesson 04:\n",
    "- RMSNorm is simpler and can be a bit faster, but it does not center activations.\n",
    "- SwiGLU adds a gated path and often improves quality, but it adds parameters and compute.\n",
    "- RoPE encodes relative positions and often extrapolates better, but it is a bit more complex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and config\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Top-level configuration (toy defaults)\n",
    "BLOCK_SIZE = 128  # context length (tokens)\n",
    "BATCH_SIZE = 32  # batch size\n",
    "D_MODEL = 256  # model width\n",
    "N_HEADS = 4  # attention heads\n",
    "N_LAYERS = 4  # transformer layers\n",
    "FFN_MULT = 4  # FFN expansion multiplier\n",
    "DROPOUT = 0.1  # dropout prob\n",
    "\n",
    "LR = 3e-4  # learning rate\n",
    "MAX_STEPS = 3000  # training steps\n",
    "EVAL_EVERY = 200  # eval interval (steps)\n",
    "EVAL_ITERS = 50  # eval batches\n",
    "SAMPLE_EVERY = 500  # sample interval (steps)\n",
    "\n",
    "TOKENIZER_VOCAB_SIZE = 2000  # target tokenizer vocab size\n",
    "BPE_TRAIN_CHARS = 200_000  # max chars for BPE training\n",
    "\n",
    "ROPE_BASE = 10000  # RoPE base\n",
    "NORM_EPS = 1e-8  # norm epsilon\n",
    "CLIP_GRAD_NORM = 1.0  # set to None to disable\n",
    "\n",
    "# Larger, slower settings (commented)\n",
    "# BLOCK_SIZE = 1024\n",
    "# BATCH_SIZE = 64\n",
    "# D_MODEL = 768\n",
    "# N_HEADS = 12\n",
    "# N_LAYERS = 12\n",
    "# FFN_MULT = 4\n",
    "# LR = 2e-4\n",
    "# MAX_STEPS = 20000\n",
    "\n",
    "seed = 42  # random seed\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # device for tensors\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def load_text_dataset():\n",
    "    from datasets import load_dataset\n",
    "    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "    name = \"wikitext-2-raw-v1\"\n",
    "    train_text = \"\\n\".join(ds[\"train\"][\"text\"])\n",
    "    val_text = \"\\n\".join(ds[\"validation\"][\"text\"]) if \"validation\" in ds else \"\"\n",
    "    return train_text, val_text, name\n",
    "\n",
    "train_text, val_text, dataset_name = load_text_dataset()\n",
    "print(\"Dataset:\", dataset_name)\n",
    "print(\"Train chars:\", len(train_text), \"Val chars:\", len(val_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer: load existing BPE or train a small one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"<pad>\", \"<unk>\"]  # special token strings\n",
    "TOKENIZER_PATH = \"tokenizer_bpe.json\"  # tokenizer file path\n",
    "\n",
    "\n",
    "def _get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def _merge_vocab(pair, vocab):\n",
    "    bigram = \" \".join(pair)\n",
    "    replacement = \"\".join(pair)\n",
    "    new_vocab = {}\n",
    "    for word, freq in vocab.items():\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = freq\n",
    "    return new_vocab\n",
    "\n",
    "\n",
    "def _build_simple_bpe_helpers(model):\n",
    "    merges = [tuple(p) for p in model[\"merges\"]]\n",
    "    token_to_id = model[\"token_to_id\"]\n",
    "    id_to_token = {i: t for t, i in token_to_id.items()}\n",
    "\n",
    "    def bpe_encode_word(word):\n",
    "        # Start from characters, then apply merges greedily\n",
    "        symbols = list(word) + [\"</w>\"]\n",
    "        for a, b in merges:\n",
    "            i = 0\n",
    "            new_symbols = []\n",
    "            while i < len(symbols):\n",
    "                if i < len(symbols) - 1 and symbols[i] == a and symbols[i + 1] == b:\n",
    "                    new_symbols.append(a + b)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "            symbols = new_symbols\n",
    "        return symbols\n",
    "\n",
    "    def encode_fn(text_in):\n",
    "        ids = []\n",
    "        for w in text_in.split():\n",
    "            for sym in bpe_encode_word(w):\n",
    "                ids.append(token_to_id.get(sym, token_to_id[\"<unk>\"]))\n",
    "        return ids\n",
    "\n",
    "    def decode_fn(ids_in):\n",
    "        tokens = [id_to_token[i] for i in ids_in]\n",
    "        text_out = \"\".join([\" \" if t == \"</w>\" else t for t in tokens])\n",
    "        return \" \".join(text_out.split())\n",
    "\n",
    "    return encode_fn, decode_fn\n",
    "\n",
    "\n",
    "def _train_simple_bpe(text, vocab_size=2000, max_merges=200):\n",
    "    words = [w for w in text.split() if w]\n",
    "    vocab = defaultdict(int)\n",
    "    for w in words:\n",
    "        vocab[\" \".join(list(w)) + \" </w>\"] += 1\n",
    "\n",
    "    symbols = set()\n",
    "    for word in vocab:\n",
    "        symbols.update(word.split())\n",
    "    base_vocab = len(symbols) + len(SPECIAL_TOKENS)\n",
    "    num_merges = max(0, min(max_merges, vocab_size - base_vocab))\n",
    "\n",
    "    merges = []\n",
    "    for _ in range(num_merges):\n",
    "        pairs = _get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = _merge_vocab(best, vocab)\n",
    "        merges.append(best)\n",
    "\n",
    "    symbols = set()\n",
    "    for word in vocab:\n",
    "        symbols.update(word.split())\n",
    "\n",
    "    tokens = list(SPECIAL_TOKENS) + sorted(symbols)\n",
    "    token_to_id = {t: i for i, t in enumerate(tokens)}\n",
    "\n",
    "    model = {\n",
    "        \"merges\": merges,\n",
    "        \"token_to_id\": token_to_id,\n",
    "    }\n",
    "\n",
    "    encode_fn, decode_fn = _build_simple_bpe_helpers(model)\n",
    "    pad_id = token_to_id[\"<pad>\"]\n",
    "    return model, encode_fn, decode_fn, len(tokens), pad_id\n",
    "\n",
    "\n",
    "def load_tokenizer(path):\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    # Try Hugging Face tokenizers first\n",
    "    try:\n",
    "        from tokenizers import Tokenizer\n",
    "\n",
    "        tokenizer = Tokenizer.from_file(path)\n",
    "\n",
    "        def encode_fn(s):\n",
    "            return tokenizer.encode(s).ids\n",
    "\n",
    "        def decode_fn(ids):\n",
    "            return tokenizer.decode(ids)\n",
    "\n",
    "        vocab_size = len(tokenizer.get_vocab())\n",
    "        pad_id = tokenizer.token_to_id(\"<pad>\")\n",
    "        if pad_id is None:\n",
    "            pad_id = 0\n",
    "        return {\n",
    "            \"type\": \"tokenizers\",\n",
    "            \"encode\": encode_fn,\n",
    "            \"decode\": decode_fn,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"pad_id\": pad_id,\n",
    "        }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback to simple BPE json\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            model = json.load(f)\n",
    "        if \"merges\" in model and \"token_to_id\" in model:\n",
    "            encode_fn, decode_fn = _build_simple_bpe_helpers(model)\n",
    "            pad_id = model[\"token_to_id\"].get(\"<pad>\", 0)\n",
    "            return {\n",
    "                \"type\": \"simple_bpe\",\n",
    "                \"encode\": encode_fn,\n",
    "                \"decode\": decode_fn,\n",
    "                \"vocab_size\": len(model[\"token_to_id\"]),\n",
    "                \"pad_id\": pad_id,\n",
    "            }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def train_tokenizer(text, vocab_size=2000, save_path=\"tokenizer_bpe.json\"):\n",
    "    text = text[:BPE_TRAIN_CHARS]\n",
    "    try:\n",
    "        from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "\n",
    "        tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "        tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC()])\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "        trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=SPECIAL_TOKENS)\n",
    "\n",
    "        tokenizer.train_from_iterator([text], trainer=trainer)\n",
    "        tokenizer.save(save_path)\n",
    "\n",
    "        def encode_fn(s):\n",
    "            return tokenizer.encode(s).ids\n",
    "\n",
    "        def decode_fn(ids):\n",
    "            return tokenizer.decode(ids)\n",
    "\n",
    "        vocab_size_out = len(tokenizer.get_vocab())\n",
    "        pad_id = tokenizer.token_to_id(\"<pad>\")\n",
    "        if pad_id is None:\n",
    "            pad_id = 0\n",
    "        return {\n",
    "            \"type\": \"tokenizers\",\n",
    "            \"encode\": encode_fn,\n",
    "            \"decode\": decode_fn,\n",
    "            \"vocab_size\": vocab_size_out,\n",
    "            \"pad_id\": pad_id,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"tokenizers not available or failed. Using simple BPE fallback.\")\n",
    "        print(\"Reason:\", repr(e))\n",
    "        model, encode_fn, decode_fn, vocab_size_out, pad_id = _train_simple_bpe(\n",
    "            text,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(model, f)\n",
    "        return {\n",
    "            \"type\": \"simple_bpe\",\n",
    "            \"encode\": encode_fn,\n",
    "            \"decode\": decode_fn,\n",
    "            \"vocab_size\": vocab_size_out,\n",
    "            \"pad_id\": pad_id,\n",
    "        }\n",
    "\n",
    "\n",
    "tok = load_tokenizer(TOKENIZER_PATH)  # tokenizer handle\n",
    "if tok is None:\n",
    "    print(\"No tokenizer found. Training a small BPE tokenizer...\")\n",
    "    tok = train_tokenizer(train_text + \"\\n\" + val_text, vocab_size=TOKENIZER_VOCAB_SIZE)\n",
    "else:\n",
    "    print(\"Loaded tokenizer from\", TOKENIZER_PATH)\n",
    "\n",
    "encode = tok[\"encode\"]  # text-to-ids function\n",
    "decode = tok[\"decode\"]  # ids-to-text function\n",
    "\n",
    "print(\"Tokenizer type:\", tok[\"type\"])\n",
    "print(\"Vocab size:\", tok[\"vocab_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = encode(train_text)  # token ids for training split\n",
    "val_ids = encode(val_text)  # token ids for validation split\n",
    "\n",
    "train_data = torch.tensor(train_ids, dtype=torch.long)  # training split data\n",
    "val_data = torch.tensor(val_ids, dtype=torch.long)  # validation split data\n",
    "\n",
    "print(\"Train tokens:\", len(train_data))\n",
    "print(\"Val tokens:\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch sampling for autoregressive language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    if len(data) <= BLOCK_SIZE + 1:\n",
    "        raise ValueError(\"Dataset too small for the chosen block size\")\n",
    "    # Random starting positions for each batch element\n",
    "    ix = torch.randint(0, len(data) - BLOCK_SIZE - 1, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i : i + BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + BLOCK_SIZE + 1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSNorm\n",
    "\n",
    "RMSNorm normalizes by the root mean square (RMS) of the features. It does not\n",
    "subtract the mean like LayerNorm. The formula is:\n",
    "\n",
    "- rms = sqrt(mean(x^2) + eps)\n",
    "- y = (x / rms) * g\n",
    "\n",
    "where g is a learned scale vector. This is simpler and often faster while\n",
    "keeping stable activations in deep networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, time, channels)\n",
    "        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).sqrt()\n",
    "        x_norm = x / rms\n",
    "        return x_norm * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SwiGLU feedforward\n",
    "\n",
    "SwiGLU uses a gated activation: one linear path provides values, another\n",
    "provides gates, and the gates are squashed with SiLU (Swish). In symbols:\n",
    "\n",
    "- a = W1 x\n",
    "- b = W2 x\n",
    "- y = W3 (silu(a) * b)\n",
    "\n",
    "This often improves quality compared to ReLU/GELU, but adds some compute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.w2 = nn.Linear(d_model, hidden_dim)\n",
    "        self.w3 = nn.Linear(hidden_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Gate and value paths, then project back\n",
    "        gated = F.silu(self.w1(x)) * self.w2(x)\n",
    "        return self.dropout(self.w3(gated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoPE (rotary positional embeddings)\n",
    "\n",
    "RoPE rotates query and key vectors in each attention head by a position-dependent\n",
    "angle. This injects relative position information directly into attention.\n",
    "\n",
    "We build a cosine/sine cache for all positions, then rotate the first half of\n",
    "head_dim against the second half. The shapes below are the common multi-head\n",
    "layout: (batch, heads, time, head_dim).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rope_cache(seq_len, head_dim, device, base=10000):\n",
    "    # head_dim must be even so we can split into two halves for rotation\n",
    "    if head_dim % 2 != 0:\n",
    "        raise ValueError(\"head_dim must be even for RoPE\")\n",
    "\n",
    "    half_dim = head_dim // 2\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, half_dim, device=device).float() / half_dim))\n",
    "    positions = torch.arange(seq_len, device=device).float()\n",
    "    # Outer product -> (seq_len, half_dim)\n",
    "    freqs = torch.einsum(\"i,j->ij\", positions, inv_freq)\n",
    "\n",
    "    # Duplicate for cos/sin to match head_dim\n",
    "    emb = torch.cat([freqs, freqs], dim=-1)\n",
    "    cos = emb.cos()[None, None, :, :]  # (1, 1, T, head_dim)\n",
    "    sin = emb.sin()[None, None, :, :]\n",
    "    return cos, sin\n",
    "\n",
    "\n",
    "def apply_rope(q, k, cos, sin, debug=False):\n",
    "    # q, k: (B, H, T, D) where D is head_dim\n",
    "    if debug:\n",
    "        print(\"q shape:\", q.shape)\n",
    "        print(\"k shape:\", k.shape)\n",
    "        print(\"cos shape:\", cos.shape)\n",
    "        print(\"sin shape:\", sin.shape)\n",
    "\n",
    "    D = q.size(-1)  # model width (alias)\n",
    "    half = D // 2\n",
    "    q1, q2 = q[..., :half], q[..., half:]\n",
    "    k1, k2 = k[..., :half], k[..., half:]\n",
    "\n",
    "    # Rotate: (x1, x2) -> (x1*cos - x2*sin, x1*sin + x2*cos)\n",
    "    q_rot = torch.cat([q1 * cos[..., :half] - q2 * sin[..., :half],\n",
    "                       q1 * sin[..., :half] + q2 * cos[..., :half]], dim=-1)\n",
    "    k_rot = torch.cat([k1 * cos[..., :half] - k2 * sin[..., :half],\n",
    "                       k1 * sin[..., :half] + k2 * cos[..., :half]], dim=-1)\n",
    "\n",
    "    if debug:\n",
    "        print(\"q_rot shape:\", q_rot.shape)\n",
    "        print(\"k_rot shape:\", k_rot.shape)\n",
    "\n",
    "    return q_rot, k_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick shape sanity check for RoPE\n",
    "B, H, T, D = 2, 4, 8, 64\n",
    "dummy_q = torch.randn(B, H, T, D)  # dummy query tensor for shape check\n",
    "dummy_k = torch.randn(B, H, T, D)  # dummy key tensor for shape check\n",
    "cos, sin = build_rope_cache(T, D, device=dummy_q.device, base=ROPE_BASE)\n",
    "_ = apply_rope(dummy_q, dummy_k, cos, sin, debug=True)  # unused placeholder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, block_size, dropout, rope_base=10000):\n",
    "        super().__init__()\n",
    "        if d_model % n_heads != 0:\n",
    "            raise ValueError(\"d_model must be divisible by n_heads\")\n",
    "        head_dim = d_model // n_heads\n",
    "        if head_dim % 2 != 0:\n",
    "            raise ValueError(\"head_dim must be even for RoPE\")\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Causal mask keeps attention from looking ahead\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        cos, sin = build_rope_cache(block_size, head_dim, device=torch.device(\"cpu\"), base=rope_base)\n",
    "        self.register_buffer(\"cos_cached\", cos)\n",
    "        self.register_buffer(\"sin_cached\", sin)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        # Split into q, k, v and then into heads\n",
    "        qkv = qkv.view(B, T, 3, self.n_heads, self.head_dim)\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "        q = q.transpose(1, 2)  # (B, H, T, D)\n",
    "        k = k.transpose(1, 2)  # (B, H, T, D)\n",
    "        v = v.transpose(1, 2)  # (B, H, T, D)\n",
    "\n",
    "        cos = self.cos_cached[:, :, :T, :]\n",
    "        sin = self.sin_cached[:, :, :T, :]\n",
    "        q, k = apply_rope(q, k, cos, sin)\n",
    "\n",
    "        # Attention scores\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        att = att.masked_fill(self.mask[:T, :T] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        out = att @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.proj(out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, block_size, ffn_mult, dropout, rope_base=10000):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(d_model, eps=NORM_EPS)\n",
    "        self.norm2 = RMSNorm(d_model, eps=NORM_EPS)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, block_size, dropout, rope_base=rope_base)\n",
    "        self.ffn = SwiGLU(d_model, ffn_mult * d_model, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-norm residual blocks are stable for deep Transformers\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        block_size,\n",
    "        d_model,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        ffn_mult,\n",
    "        dropout,\n",
    "        rope_base=10000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        blocks = []\n",
    "        for _ in range(n_layers):\n",
    "            blocks.append(\n",
    "                TransformerBlock(\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    block_size,\n",
    "                    ffn_mult,\n",
    "                    dropout,\n",
    "                    rope_base=rope_base,\n",
    "                )\n",
    "            )\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.norm_f = RMSNorm(d_model, eps=NORM_EPS)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        if T > self.block_size:\n",
    "            raise ValueError(\"Sequence length exceeds block size\")\n",
    "\n",
    "        tok = self.token_emb(idx)\n",
    "        x = self.drop(tok)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            logits_flat = logits.view(-1, logits.size(-1))\n",
    "            targets_flat = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        was_training = self.training\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size :]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "\n",
    "            if top_k is not None:\n",
    "                k = min(top_k, logits.size(-1))\n",
    "                v, _ = torch.topk(logits, k)\n",
    "                cutoff = v[:, [-1]]\n",
    "                logits = logits.masked_fill(logits < cutoff, float(\"-inf\"))\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "        if was_training:\n",
    "            self.train()\n",
    "        return idx\n",
    "\n",
    "\n",
    "vocab_size = tok[\"vocab_size\"]  # vocabulary size\n",
    "model = GPT(  # model instance\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    n_layers=N_LAYERS,\n",
    "    ffn_mult=FFN_MULT,\n",
    "    dropout=DROPOUT,\n",
    "    rope_base=ROPE_BASE,\n",
    ").to(device)\n",
    "\n",
    "param_count = sum(p.numel() for p in model.parameters())  # parameter count\n",
    "print(f\"Model parameters: {param_count/1e6:.2f}M\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)  # optimizer instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(EVAL_ITERS):\n",
    "            xb, yb = get_batch(split)\n",
    "            _, loss = model(xb, yb)\n",
    "            losses.append(loss.item())\n",
    "        avg = sum(losses) / len(losses)\n",
    "        out[split] = {\"loss\": avg, \"ppl\": math.exp(avg)}\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "def sample_generation(prompt, max_new_tokens=80):\n",
    "    idx = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "    out = model.generate(idx, max_new_tokens=max_new_tokens, temperature=0.9, top_k=40)\n",
    "    return decode(out[0].tolist())\n",
    "\n",
    "\n",
    "print(\"Initial eval:\")\n",
    "metrics = estimate_loss()  # evaluation metrics dict\n",
    "print(\n",
    "    f\"train loss {metrics['train']['loss']:.3f}, train ppl {metrics['train']['ppl']:.2f} | \"\n",
    "    f\"val loss {metrics['val']['loss']:.3f}, val ppl {metrics['val']['ppl']:.2f}\"\n",
    ")\n",
    "\n",
    "# optimization steps\n",
    "for step in range(1, MAX_STEPS + 1):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    _, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    if CLIP_GRAD_NORM is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD_NORM)\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % EVAL_EVERY == 0:\n",
    "        metrics = estimate_loss()\n",
    "        print(\n",
    "            f\"step {step}: \"\n",
    "            f\"train loss {metrics['train']['loss']:.3f}, train ppl {metrics['train']['ppl']:.2f} | \"\n",
    "            f\"val loss {metrics['val']['loss']:.3f}, val ppl {metrics['val']['ppl']:.2f}\"\n",
    "        )\n",
    "\n",
    "    if step % SAMPLE_EVERY == 0:\n",
    "        print(\"Sample generation:\")\n",
    "        print(sample_generation(\"The meaning of life is\"))\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: prompted generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [  # list of prompts\n",
    "    \"The meaning of life is\",\n",
    "    \"Once upon a time\",\n",
    "    \"In the middle of the night\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    idx = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "    out = model.generate(idx, max_new_tokens=80, temperature=0.9, top_k=40)\n",
    "    print(\"PROMPT:\", prompt)\n",
    "    print(decode(out[0].tolist()))\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling notes\n",
    "\n",
    "Toy defaults in this notebook:\n",
    "- D_MODEL=256, N_HEADS=4, N_LAYERS=4, BLOCK_SIZE=128, BATCH_SIZE=32\n",
    "- LR=3e-4, MAX_STEPS around 3000\n",
    "\n",
    "Production-ish suggestions (much slower):\n",
    "- D_MODEL=768 or 1024, N_HEADS=12 or 16, N_LAYERS=12 or 24\n",
    "- BLOCK_SIZE=512 or 1024, BATCH_SIZE as large as memory allows\n",
    "- More training steps and a learning rate schedule\n",
    "\n",
    "When scaling up, consider gradient accumulation, mixed precision, and\n",
    "periodic evaluation on a held-out validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1) Replace RoPE with learned absolute positional embeddings and compare results.\n",
    "2) Try GELU instead of SwiGLU and measure training speed and loss.\n",
    "3) Vary the RoPE base (ROPE_BASE) and see how it affects long-range behavior.\n",
    "4) Add dropout to attention weights and compare with/without.\n",
    "5) Implement weight tying between token embeddings and the LM head.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}