{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 06: KV Cache for Fast Generation\n",
        "\n",
        "When a GPT-style model generates one token at a time, it repeatedly re-computes attention over the whole context. A KV cache saves the **keys** and **values** from previous steps so each new token only needs attention against the cached history.\n",
        "\n",
        "You will learn:\n",
        "- how a decoder Transformer produces queries, keys, values\n",
        "- how to reuse cached K/V tensors during generation\n",
        "- why caching turns O(T^2) per step into O(T) per step\n",
        "\n",
        "We'll build a small model, train briefly, and compare naive vs cached generation speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Setup and config\nimport math\nimport os\nimport random\nimport json\nimport time\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Top-level configuration (small defaults)\nBLOCK_SIZE = 128  # context length (tokens)\nBATCH_SIZE = 32  # batch size\nD_MODEL = 256  # model width\nN_HEADS = 4  # attention heads\nN_LAYERS = 4  # transformer layers\nFFN_MULT = 4  # FFN expansion multiplier\nDROPOUT = 0.1  # dropout prob\n\nLR = 3e-4  # learning rate\nMAX_STEPS = 400  # training steps\nEVAL_EVERY = 100  # eval interval (steps)\nEVAL_ITERS = 30  # eval batches\n\nTOKENIZER_VOCAB_SIZE = 2000  # target tokenizer vocab size\nBPE_TRAIN_CHARS = 200_000  # max chars for BPE training\n\nGEN_TOKENS = 120  # generated tokens\n\n# Reproducibility\nseed = 42\nrandom.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Dataset: Wikitext-2 (with fallback)\n\nWe prefer `wikitext-2-raw-v1` from Hugging Face. If that fails (offline), we fall back to wikitext-2-raw-v1, and then a tiny local string so the notebook always runs.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from datasets import load_dataset\n\n\ndef load_text_dataset():\n    from datasets import load_dataset\n    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n    name = \"wikitext-2-raw-v1\"\n    train_text = \"\\n\".join(ds[\"train\"][\"text\"])\n    val_text = \"\\n\".join(ds[\"validation\"][\"text\"]) if \"validation\" in ds else \"\"\n    return train_text, val_text, name\n\ntrain_text, val_text, dataset_name = load_text_dataset()\nprint(\"Dataset:\", dataset_name)\nprint(\"Train chars:\", len(train_text), \"Val chars:\", len(val_text))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer (small BPE)\n",
        "\n",
        "We either load `tokenizer_bpe.json` or train a tiny BPE tokenizer on the dataset. This keeps the notebook self-contained.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "SPECIAL_TOKENS = [\"<pad>\", \"<unk>\"]  # special token strings\nTOKENIZER_PATH = \"tokenizer_bpe.json\"  # tokenizer file path\n\n\ndef _get_stats(vocab):\n    # Count symbol pairs inside the current vocabulary\n    pairs = defaultdict(int)\n    for word, freq in vocab.items():\n        symbols = word.split()\n        for i in range(len(symbols) - 1):\n            pairs[(symbols[i], symbols[i + 1])] += freq\n    return pairs\n\n\ndef _merge_vocab(pair, vocab):\n    bigram = \" \".join(pair)\n    replacement = \"\".join(pair)\n    new_vocab = {}\n    for word, freq in vocab.items():\n        new_word = word.replace(bigram, replacement)\n        new_vocab[new_word] = freq\n    return new_vocab\n\n\ndef _build_simple_bpe_helpers(model):\n    merges = [tuple(p) for p in model[\"merges\"]]\n    token_to_id = model[\"token_to_id\"]\n    id_to_token = {i: t for t, i in token_to_id.items()}\n\n    def bpe_encode_word(word):\n        # Start from characters, then apply merges greedily\n        symbols = list(word) + [\"</w>\"]\n        for a, b in merges:\n            i = 0\n            new_symbols = []\n            while i < len(symbols):\n                if i < len(symbols) - 1 and symbols[i] == a and symbols[i + 1] == b:\n                    new_symbols.append(a + b)\n                    i += 2\n                else:\n                    new_symbols.append(symbols[i])\n                    i += 1\n            symbols = new_symbols\n        return symbols\n\n    def encode_fn(text_in):\n        ids = []\n        for w in text_in.split():\n            for sym in bpe_encode_word(w):\n                ids.append(token_to_id.get(sym, token_to_id[\"<unk>\"]))\n        return ids\n\n    def decode_fn(ids_in):\n        tokens = [id_to_token[i] for i in ids_in]\n        text_out = \"\".join([\" \" if t == \"</w>\" else t for t in tokens])\n        return \" \".join(text_out.split())\n\n    return encode_fn, decode_fn\n\n\ndef _train_simple_bpe(text, vocab_size=2000, max_merges=200):\n    # Build initial vocab of character sequences\n    words = [w for w in text.split() if w]\n    vocab = defaultdict(int)\n    for w in words:\n        vocab[\" \".join(list(w)) + \" </w>\"] += 1\n\n    symbols = set()\n    for word in vocab:\n        symbols.update(word.split())\n    base_vocab = len(symbols) + len(SPECIAL_TOKENS)\n    num_merges = max(0, min(max_merges, vocab_size - base_vocab))\n\n    merges = []\n    for _ in range(num_merges):\n        pairs = _get_stats(vocab)\n        if not pairs:\n            break\n        best = max(pairs, key=pairs.get)\n        vocab = _merge_vocab(best, vocab)\n        merges.append(best)\n\n    symbols = set()\n    for word in vocab:\n        symbols.update(word.split())\n\n    tokens = list(SPECIAL_TOKENS) + sorted(symbols)\n    token_to_id = {t: i for i, t in enumerate(tokens)}\n\n    model = {\n        \"merges\": merges,\n        \"token_to_id\": token_to_id,\n    }\n\n    encode_fn, decode_fn = _build_simple_bpe_helpers(model)\n    pad_id = token_to_id[\"<pad>\"]\n    return model, encode_fn, decode_fn, len(tokens), pad_id\n\n\ndef load_tokenizer(path):\n    if not os.path.exists(path):\n        return None\n    # Try Hugging Face tokenizers first\n    try:\n        from tokenizers import Tokenizer\n\n        tokenizer = Tokenizer.from_file(path)\n\n        def encode_fn(s):\n            return tokenizer.encode(s).ids\n\n        def decode_fn(ids):\n            return tokenizer.decode(ids)\n\n        vocab_size = len(tokenizer.get_vocab())\n        pad_id = tokenizer.token_to_id(\"<pad>\")\n        if pad_id is None:\n            pad_id = 0\n        return {\n            \"type\": \"tokenizers\",\n            \"encode\": encode_fn,\n            \"decode\": decode_fn,\n            \"vocab_size\": vocab_size,\n            \"pad_id\": pad_id,\n        }\n    except Exception:\n        pass\n\n    # Fallback to simple BPE json\n    try:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            model = json.load(f)\n        if \"merges\" in model and \"token_to_id\" in model:\n            encode_fn, decode_fn = _build_simple_bpe_helpers(model)\n            pad_id = model[\"token_to_id\"].get(\"<pad>\", 0)\n            return {\n                \"type\": \"simple_bpe\",\n                \"encode\": encode_fn,\n                \"decode\": decode_fn,\n                \"vocab_size\": len(model[\"token_to_id\"]),\n                \"pad_id\": pad_id,\n            }\n    except Exception:\n        pass\n\n    return None\n\n\ndef train_tokenizer(text, vocab_size=2000, save_path=\"tokenizer_bpe.json\"):\n    text = text[:BPE_TRAIN_CHARS]\n    try:\n        from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n\n        tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n        tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC()])\n        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n        trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=SPECIAL_TOKENS)\n\n        tokenizer.train_from_iterator([text], trainer=trainer)\n        tokenizer.save(save_path)\n\n        def encode_fn(s):\n            return tokenizer.encode(s).ids\n\n        def decode_fn(ids):\n            return tokenizer.decode(ids)\n\n        vocab_size_out = len(tokenizer.get_vocab())\n        pad_id = tokenizer.token_to_id(\"<pad>\")\n        if pad_id is None:\n            pad_id = 0\n        return {\n            \"type\": \"tokenizers\",\n            \"encode\": encode_fn,\n            \"decode\": decode_fn,\n            \"vocab_size\": vocab_size_out,\n            \"pad_id\": pad_id,\n        }\n    except Exception as e:\n        print(\"tokenizers not available or failed. Using simple BPE fallback.\")\n        print(\"Reason:\", repr(e))\n        model, encode_fn, decode_fn, vocab_size_out, pad_id = _train_simple_bpe(\n            text,\n            vocab_size=vocab_size,\n        )\n        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(model, f)\n        return {\n            \"type\": \"simple_bpe\",\n            \"encode\": encode_fn,\n            \"decode\": decode_fn,\n            \"vocab_size\": vocab_size_out,\n            \"pad_id\": pad_id,\n        }"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "tok = load_tokenizer(TOKENIZER_PATH)\nif tok is None:\n    print(\"No tokenizer found. Training a small BPE tokenizer...\")\n    tok = train_tokenizer(train_text + \"\\n\" + val_text, vocab_size=TOKENIZER_VOCAB_SIZE)\nelse:\n    print(\"Loaded tokenizer from\", TOKENIZER_PATH)\n\nencode = tok[\"encode\"]\ndecode = tok[\"decode\"]\n\nprint(\"Tokenizer type:\", tok[\"type\"])\nprint(\"Vocab size:\", tok[\"vocab_size\"])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Numericalize the dataset\n",
        "\n",
        "We convert text to token ids once, then sample contiguous blocks for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "train_ids = torch.tensor(encode(train_text), dtype=torch.long)\nval_ids = torch.tensor(encode(val_text), dtype=torch.long)\n\nprint(\"Train tokens:\", train_ids.numel())\nprint(\"Val tokens:\", val_ids.numel())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch sampling for autoregressive language modeling\n",
        "\n",
        "We sample random contiguous windows of length `BLOCK_SIZE` and predict the next token.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def get_batch(split):\n    data = train_ids if split == \"train\" else val_ids\n    # Random starting indices for each sequence in the batch\n    idx = torch.randint(0, len(data) - BLOCK_SIZE - 1, (BATCH_SIZE,))\n    x = torch.stack([data[i : i + BLOCK_SIZE] for i in idx])\n    y = torch.stack([data[i + 1 : i + BLOCK_SIZE + 1] for i in idx])\n    return x.to(device), y.to(device)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPT-style decoder with KV cache\n",
        "\n",
        "Below is a minimal decoder-only Transformer. The key change is in the attention module, which can accept a `past_kv` cache and return updated keys/values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class FeedForward(nn.Module):\n    def __init__(self, d_model, ffn_mult=4, dropout=0.1):\n        super().__init__()\n        hidden = ffn_mult * d_model\n        self.net = nn.Sequential(\n            nn.Linear(d_model, hidden),\n            nn.GELU(),\n            nn.Linear(hidden, d_model),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(self, d_model, n_heads, block_size, dropout=0.1, print_shapes=False):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.n_heads = n_heads\n        self.head_dim = d_model // n_heads\n        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n        self.dropout = nn.Dropout(dropout)\n        # Causal mask keeps attention from looking ahead\n        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)))\n        self.print_shapes = print_shapes\n        self._printed = False\n\n    def forward(self, x, past_kv=None, use_cache=False):\n        B, T, C = x.shape\n\n        # Project to queries, keys, values (still in full D_MODEL space)\n        q = self.q_proj(x)\n        k = self.k_proj(x)\n        v = self.v_proj(x)\n\n        # Reshape to separate heads: (B, T, C) -> (B, n_heads, T, head_dim)\n        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n\n        if past_kv is not None:\n            past_k, past_v = past_kv\n            # Concatenate cached keys/values with the current step\n            k = torch.cat([past_k, k], dim=2)\n            v = torch.cat([past_v, v], dim=2)\n            past_len = past_k.size(2)\n        else:\n            past_len = 0\n\n        total_len = k.size(2)\n        if total_len > self.mask.size(0):\n            raise ValueError(\"KV cache is longer than block size.\")\n\n        # Attention scores: (B, n_heads, T, total_len)\n        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n\n        if past_len == 0:\n            # Standard causal mask for training on full sequences\n            att = att.masked_fill(self.mask[:T, :T] == 0, float(\"-inf\"))\n        else:\n            # Mask so new tokens cannot attend to \"future\" positions in the new chunk\n            full_mask = self.mask[:total_len, :total_len]\n            att = att.masked_fill(full_mask[past_len:total_len, :total_len] == 0, float(\"-inf\"))\n\n        att = F.softmax(att, dim=-1)\n        att = self.dropout(att)\n\n        # Weighted sum of values -> (B, n_heads, T, head_dim)\n        out = att @ v\n        out = out.transpose(1, 2).contiguous().view(B, T, C)\n        out = self.out_proj(out)\n        out = self.dropout(out)\n\n        # Print shapes once so you can see exactly what flows through attention\n        if self.print_shapes and not self._printed:\n            print(\"x:\", x.shape)\n            print(\"q:\", q.shape, \"k:\", k.shape, \"v:\", v.shape)\n            print(\"att:\", att.shape, \"out:\", out.shape, \"past_len:\", past_len)\n            self._printed = True\n\n        new_kv = (k, v) if use_cache else None\n        return out, new_kv"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class TransformerBlock(nn.Module):\n    def __init__(self, d_model, n_heads, block_size, ffn_mult=4, dropout=0.1, print_shapes=False):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n        self.attn = MultiHeadSelfAttention(\n            d_model,\n            n_heads,\n            block_size,\n            dropout=dropout,\n            print_shapes=print_shapes,\n        )\n        self.ffn = FeedForward(d_model, ffn_mult=ffn_mult, dropout=dropout)\n\n    def forward(self, x, past_kv=None, use_cache=False):\n        attn_out, new_kv = self.attn(self.ln1(x), past_kv=past_kv, use_cache=use_cache)\n        x = x + attn_out\n        x = x + self.ffn(self.ln2(x))\n        return x, new_kv\n\n\nclass GPT(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        block_size,\n        d_model,\n        n_heads,\n        n_layers,\n        ffn_mult,\n        dropout,\n    ):\n        super().__init__()\n        self.block_size = block_size\n        self.token_emb = nn.Embedding(vocab_size, d_model)\n        self.pos_emb = nn.Embedding(block_size, d_model)\n        self.drop = nn.Dropout(dropout)\n\n        blocks = []\n        for i in range(n_layers):\n            blocks.append(\n                TransformerBlock(\n                    d_model,\n                    n_heads,\n                    block_size,\n                    ffn_mult=ffn_mult,\n                    dropout=dropout,\n                    print_shapes=(i == 0),\n                )\n            )\n        self.blocks = nn.ModuleList(blocks)\n        self.ln_f = nn.LayerNorm(d_model)\n        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n\n    def forward(self, idx, targets=None, past_kv=None, use_cache=False):\n        B, T = idx.shape\n        if past_kv is None:\n            past_kv = [None] * len(self.blocks)\n            past_len = 0\n        else:\n            past_len = past_kv[0][0].size(2) if past_kv[0] is not None else 0\n\n        if past_len + T > self.block_size:\n            raise ValueError(\"Sequence length exceeds block size\")\n\n        # Token and positional embeddings\n        tok = self.token_emb(idx)\n        pos = self.pos_emb(torch.arange(past_len, past_len + T, device=idx.device))\n        x = tok + pos  # broadcast over batch\n        x = self.drop(x)\n\n        new_kv = [] if use_cache else None\n        for block, layer_past in zip(self.blocks, past_kv):\n            x, layer_kv = block(x, past_kv=layer_past, use_cache=use_cache)\n            if use_cache:\n                new_kv.append(layer_kv)\n\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n\n        loss = None\n        if targets is not None:\n            logits_flat = logits.view(-1, logits.size(-1))\n            targets_flat = targets.view(-1)\n            loss = F.cross_entropy(logits_flat, targets_flat)\n\n        return logits, loss, new_kv"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop\n",
        "\n",
        "We keep training short (just enough to make generation non-random-ish).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "vocab_size = tok[\"vocab_size\"]\nmodel = GPT(\n    vocab_size=vocab_size,\n    block_size=BLOCK_SIZE,\n    d_model=D_MODEL,\n    n_heads=N_HEADS,\n    n_layers=N_LAYERS,\n    ffn_mult=FFN_MULT,\n    dropout=DROPOUT,\n).to(device)\n\n# Parameter count is a quick sanity check on model size\nparam_count = sum(p.numel() for p in model.parameters())\nprint(f\"Model parameters: {param_count/1e6:.2f}M\")\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n\n\n@torch.no_grad()\ndef estimate_loss():\n    model.eval()\n    out = {}\n    for split in [\"train\", \"val\"]:\n        losses = []\n        for _ in range(EVAL_ITERS):\n            xb, yb = get_batch(split)\n            _, loss, _ = model(xb, yb)\n            losses.append(loss.item())\n        avg = sum(losses) / len(losses)\n        # Perplexity is exp(cross-entropy)\n        out[split] = {\"loss\": avg, \"ppl\": math.exp(avg)}\n    model.train()\n    return out\n\n\nprint(\"Initial eval:\")\nmetrics = estimate_loss()\nprint(\n    f\"train loss {metrics['train']['loss']:.3f}, train ppl {metrics['train']['ppl']:.2f} | \"\n    f\"val loss {metrics['val']['loss']:.3f}, val ppl {metrics['val']['ppl']:.2f}\"\n)\n\nfor step in range(1, MAX_STEPS + 1):\n    xb, yb = get_batch(\"train\")\n    # Forward pass returns logits and loss\n    _, loss, _ = model(xb, yb)\n\n    optimizer.zero_grad(set_to_none=True)\n    # Backprop + parameter update\n    loss.backward()\n    optimizer.step()\n\n    if step % EVAL_EVERY == 0:\n        metrics = estimate_loss()\n        print(\n            f\"step {step}: \"\n            f\"train loss {metrics['train']['loss']:.3f}, train ppl {metrics['train']['ppl']:.2f} | \"\n            f\"val loss {metrics['val']['loss']:.3f}, val ppl {metrics['val']['ppl']:.2f}\"\n        )"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference: naive vs cached generation\n",
        "\n",
        "We compare two approaches:\n",
        "- **naive**: recompute attention over the full context each step\n",
        "- **cached**: reuse keys/values from previous steps\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "@torch.no_grad()\ndef sample_from_logits(logits, temperature=1.0, top_k=None):\n    # Apply temperature and (optional) top-k filtering\n    logits = logits / max(temperature, 1e-6)\n    if top_k is not None:\n        k = min(top_k, logits.size(-1))\n        v, _ = torch.topk(logits, k)\n        cutoff = v[:, [-1]]\n        logits = logits.masked_fill(logits < cutoff, float(\"-inf\"))\n    probs = F.softmax(logits, dim=-1)\n    return torch.multinomial(probs, num_samples=1)\n\n\n@torch.no_grad()\ndef generate_naive(model, idx, max_new_tokens, temperature=1.0, top_k=None):\n    was_training = model.training\n    model.eval()\n\n    for _ in range(max_new_tokens):\n        # Crop to model context window\n        idx_cond = idx[:, -model.block_size :]\n        logits, _, _ = model(idx_cond)\n        next_id = sample_from_logits(logits[:, -1, :], temperature=temperature, top_k=top_k)\n        idx = torch.cat([idx, next_id], dim=1)\n\n    if was_training:\n        model.train()\n    return idx\n\n\ndef crop_kv_cache(past_kv, max_len):\n    if past_kv is None:\n        return None\n    cropped = []\n    for k, v in past_kv:\n        if k.size(2) > max_len:\n            k = k[:, :, -max_len:, :]\n            v = v[:, :, -max_len:, :]\n        cropped.append((k, v))\n    return cropped\n\n\n@torch.no_grad()\ndef generate_cached(model, idx, max_new_tokens, temperature=1.0, top_k=None):\n    was_training = model.training\n    model.eval()\n\n    # Prime the cache on the prompt (crop to block size)\n    idx_cond = idx[:, -model.block_size :]\n    logits, _, past_kv = model(idx_cond, use_cache=True)\n\n    for _ in range(max_new_tokens):\n        next_id = sample_from_logits(logits[:, -1, :], temperature=temperature, top_k=top_k)\n        idx = torch.cat([idx, next_id], dim=1)\n\n        # Keep only the most recent context window in the cache\n        past_kv = crop_kv_cache(past_kv, max_len=model.block_size - 1)\n        logits, _, past_kv = model(next_id, past_kv=past_kv, use_cache=True)\n\n    if was_training:\n        model.train()\n    return idx\n\n\ndef time_generation(label, fn, model, idx, max_new_tokens, **kwargs):\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    start = time.perf_counter()\n    out = fn(model, idx, max_new_tokens, **kwargs)\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    elapsed = time.perf_counter() - start\n    tok_per_sec = max_new_tokens / elapsed\n    print(f\"{label}: {elapsed:.3f}s, {tok_per_sec:.1f} tokens/sec\")\n    return out"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "prompt = \"The meaning of life is\"\nidx = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n\n# Use the same seed so both methods sample the same tokens\nsample_seed = 123\n\nprint(\"--- Timing comparison ---\")\ntorch.manual_seed(sample_seed)\nout_naive = time_generation(\n    \"naive\",\n    generate_naive,\n    model,\n    idx.clone(),\n    GEN_TOKENS,\n    temperature=0.9,\n    top_k=40,\n)\n\ntorch.manual_seed(sample_seed)\nout_cached = time_generation(\n    \"cached\",\n    generate_cached,\n    model,\n    idx.clone(),\n    GEN_TOKENS,\n    temperature=0.9,\n    top_k=40,\n)\n\nprint(\"\\n--- Naive output ---\")\nprint(decode(out_naive[0].tolist()))\nprint(\"\\n--- Cached output ---\")\nprint(decode(out_cached[0].tolist()))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scaling notes for production inference\n",
        "\n",
        "A few real-world considerations when serving large models:\n",
        "- **Batching**: group multiple requests so attention uses larger matrix multiplies.\n",
        "- **Paged KV cache**: store K/V in blocks so you can evict or swap old tokens efficiently.\n",
        "- **Quantized KV**: store K/V in lower precision (e.g., FP8 or INT8) to save memory bandwidth.\n",
        "- **Speculative decoding**: draft tokens with a small model, then verify with a large model.\n",
        "- **Streaming + early exit**: send tokens to users as soon as they are ready.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. Increase `BLOCK_SIZE` to 256 and see how the speed gap changes.\n",
        "2. Try greedy decoding (`torch.argmax`) instead of sampling and compare output consistency.\n",
        "3. Measure memory usage with and without KV cache (hint: `torch.cuda.memory_allocated`).\n",
        "4. Add a maximum prompt length and show how cache cropping affects output.\n",
        "5. Implement top-p (nucleus) sampling and compare with top-k.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}